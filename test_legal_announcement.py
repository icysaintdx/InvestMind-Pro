#!/usr/bin/env python3
"""
æµ‹è¯•æ³•å¾‹é£é™©å’Œå…¬å¸å…¬å‘Šçˆ¬è™«
"""

import sys
import os
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

print("=" * 80)
print("ğŸ§ª æµ‹è¯•æ³•å¾‹é£é™©å’Œå…¬å¸å…¬å‘Šçˆ¬è™«")
print("=" * 80)
print()

# æµ‹è¯•1: ä¸­å›½è£åˆ¤æ–‡ä¹¦ç½‘çˆ¬è™«
print("âš–ï¸ æµ‹è¯•1: ä¸­å›½è£åˆ¤æ–‡ä¹¦ç½‘çˆ¬è™«")
print("-" * 80)

try:
    from backend.dataflows.legal.wenshu_crawler import get_wenshu_crawler
    
    crawler = get_wenshu_crawler()
    company_name = "è´µå·èŒ…å°é…’è‚¡ä»½æœ‰é™å…¬å¸"
    
    print(f"æœç´¢å…¬å¸: {company_name}")
    print()
    
    # æœç´¢æ¡ˆä»¶
    cases = crawler.search_company_cases(company_name, days=365)
    
    if cases:
        print(f"âœ… æ‰¾åˆ° {len(cases)} ä¸ªæ¡ˆä»¶")
        print()
        
        # æ˜¾ç¤ºæ¡ˆä»¶è¯¦æƒ…
        for i, case in enumerate(cases[:3], 1):
            print(f"{i}. {case['case_name']}")
            print(f"   æ¡ˆä»¶ç±»å‹: {case['case_type']}")
            print(f"   æ³•é™¢: {case['court']}")
            print(f"   æ—¥æœŸ: {case['case_date']}")
            print(f"   é£é™©ç­‰çº§: {case['risk_level']}")
            print()
        
        # é£é™©åˆ†æ
        risk_analysis = crawler.analyze_legal_risk(cases)
        print("ğŸ“Š é£é™©åˆ†æ:")
        print(f"   æ€»æ¡ˆä»¶æ•°: {risk_analysis['total_cases']}")
        print(f"   é£é™©ç­‰çº§: {risk_analysis['risk_level']}")
        print(f"   é£é™©è¯„åˆ†: {risk_analysis['risk_score']}")
        print(f"   æ€»ç»“: {risk_analysis['summary']}")
    else:
        print("âš ï¸ æœªæ‰¾åˆ°ç›¸å…³æ¡ˆä»¶")
        
except Exception as e:
    print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
    import traceback
    traceback.print_exc()

print("\n")

# æµ‹è¯•2: å·¨æ½®èµ„è®¯ç½‘çˆ¬è™«
print("ğŸ“¢ æµ‹è¯•2: å·¨æ½®èµ„è®¯ç½‘çˆ¬è™«")
print("-" * 80)

try:
    from backend.dataflows.announcement.cninfo_crawler import get_cninfo_crawler
    
    crawler = get_cninfo_crawler()
    stock_code = "600519"
    
    print(f"è·å–è‚¡ç¥¨: {stock_code} çš„å…¬å‘Š")
    print()
    
    # è·å–å…¬å‘Š
    announcements = crawler.get_company_announcements(stock_code, days=30)
    
    if announcements:
        print(f"âœ… è·å–åˆ° {len(announcements)} æ¡å…¬å‘Š")
        print()
        
        # æ˜¾ç¤ºå…¬å‘Šè¯¦æƒ…
        for i, ann in enumerate(announcements[:3], 1):
            print(f"{i}. {ann['title']}")
            print(f"   ç±»å‹: {ann['type']}")
            print(f"   æ—¥æœŸ: {ann['publish_date']}")
            print(f"   é‡è¦æ€§: {ann['importance']}")
            print()
        
        # å…¬å‘Šåˆ†æ
        analysis = crawler.analyze_announcements(announcements)
        print("ğŸ“Š å…¬å‘Šåˆ†æ:")
        print(f"   æ€»å…¬å‘Šæ•°: {analysis['total']}")
        print(f"   é‡è¦å…¬å‘Š: {analysis['important_count']}")
        print(f"   å…¬å‘Šç±»å‹: {analysis['types']}")
        print(f"   æ€»ç»“: {analysis['summary']}")
    else:
        print("âš ï¸ æœªè·å–åˆ°å…¬å‘Š")
        
except Exception as e:
    print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
    import traceback
    traceback.print_exc()

print("\n")
print("=" * 80)
print("ğŸ“‹ æµ‹è¯•æ€»ç»“")
print("=" * 80)
print()
print("âœ… = æµ‹è¯•é€šè¿‡")
print("âš ï¸ = æ— æ•°æ®ï¼ˆä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®ï¼‰")
print("âŒ = æµ‹è¯•å¤±è´¥")
print()
print("æ³¨æ„:")
print("1. å½“å‰ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®è¿›è¡Œæµ‹è¯•")
print("2. å®é™…ä½¿ç”¨éœ€è¦å®ç°çœŸå®çš„APIè°ƒç”¨")
print("3. ä¸­å›½è£åˆ¤æ–‡ä¹¦ç½‘éœ€è¦å¤æ‚çš„è®¤è¯å’Œåçˆ¬è™«å¤„ç†")
print("4. å·¨æ½®èµ„è®¯ç½‘APIéœ€è¦æ ¹æ®å®˜æ–¹æ–‡æ¡£è°ƒæ•´")
print()
print("ä¸‹ä¸€æ­¥:")
print("1. å®ç°çœŸå®çš„APIè°ƒç”¨")
print("2. å¤„ç†è®¤è¯å’Œåçˆ¬è™«")
print("3. é›†æˆåˆ°ç»Ÿä¸€API")
print()
