# 爬虫开发任务清单

> **创建时间**: 2025-12-05  
> **优先级**: 高  
> **目标**: 完成中国裁判文书网和巨潮资讯网两个爬虫

---

## 📋 任务概览

### 🎯 目标
1. **中国裁判文书网爬虫** - 获取上市公司法律诉讼信息
2. **巨潮资讯网爬虫** - 获取上市公司公告信息

### 📊 当前状态
- ✅ 基础框架已完成
- ✅ 模拟数据可用
- ⏳ 真实API调用待实现
- ⏳ 加密算法待移植

---

## 🔧 任务一：中国裁判文书网爬虫

### 📁 文件位置
- `backend/dataflows/legal/wenshu_crawler.py`
- 参考文档：`docs/中国裁判文书网.cpws.js.md`

### ✅ 已完成
1. 基础类结构
2. 模拟数据接口
3. 风险分析逻辑
4. 3DES加密框架

### 🔨 待完成任务

#### 1. 移植JavaScript加密算法 ⭐⭐⭐
**优先级**: 最高  
**预计时间**: 2-3小时

**任务细节**:
- [ ] 分析 `中国裁判文书网.cpws.js.md` 中的加密逻辑
- [ ] 实现 `cipher()` 函数（3DES加密 + 二进制转换）
- [ ] 实现 `uuid()` 函数（32位GUID生成）
- [ ] 实现 `__RequestVerificationToken()` 函数（24位随机字符串）
- [ ] 实现 `getcfg()` 函数（返回固定配置字符串）
- [ ] 测试加密结果与JS版本一致性

**关键代码位置**:
```python
# wenshu_crawler.py
def _generate_cipher(self) -> str:
    # 需要完善此函数
    pass

def _des3_encrypt(self, plaintext: str, key: str, iv: str) -> str:
    # 已有框架，需要验证
    pass
```

#### 2. 实现真实API调用 ⭐⭐
**优先级**: 高  
**预计时间**: 3-4小时

**任务细节**:
- [ ] 分析裁判文书网真实API端点
- [ ] 构建正确的请求头和参数
- [ ] 处理反爬虫机制（User-Agent、Referer、Cookie等）
- [ ] 实现请求重试和错误处理
- [ ] 解析API响应数据

**API端点**（需验证）:
```python
base_url = "https://wenshu.court.gov.cn"
search_api = "/website/wenshu/181217BMTKHNT2W0/search"
detail_api = "/website/wenshu/181217BMTKHNT2W0/detail"
```

#### 3. 数据解析和清洗 ⭐
**优先级**: 中  
**预计时间**: 2小时

**任务细节**:
- [ ] 解析案件列表响应
- [ ] 提取关键字段（案号、案由、当事人、日期等）
- [ ] 数据标准化处理
- [ ] 异常数据过滤

#### 4. 测试和优化 ⭐
**优先级**: 中  
**预计时间**: 1-2小时

**任务细节**:
- [ ] 创建测试脚本 `test_wenshu_crawler.py`
- [ ] 测试不同公司的案件查询
- [ ] 测试异常情况处理
- [ ] 性能优化（缓存、并发等）

---

## 🔧 任务二：巨潮资讯网爬虫

### 📁 文件位置
- `backend/dataflows/announcement/cninfo_crawler.py`
- 参考文档：`docs/财联社.js.md`（加密算法参考）

### ✅ 已完成
1. 基础类结构
2. API请求框架
3. 公告解析逻辑
4. 重要性判断逻辑

### 🔨 待完成任务

#### 1. 验证和修复API调用 ⭐⭐⭐
**优先级**: 最高  
**预计时间**: 2-3小时

**任务细节**:
- [ ] 测试当前API是否可用
- [ ] 分析API响应格式变化
- [ ] 修复请求参数（如有变化）
- [ ] 处理反爬虫机制
- [ ] 验证PDF下载链接拼接

**当前API**:
```python
api_url = "http://www.cninfo.com.cn/new/hisAnnouncement/query"
static_url = "http://static.cninfo.com.cn"
```

#### 2. 增加公告类型过滤 ⭐⭐
**优先级**: 高  
**预计时间**: 1-2小时

**任务细节**:
- [ ] 支持按公告类型筛选（年报、季报、重大事项等）
- [ ] 实现公告类型枚举
- [ ] 优化重要公告判断逻辑
- [ ] 添加公告内容关键词提取

**公告类型**:
- 定期报告（年报、半年报、季报）
- 业绩预告/快报
- 重大事项（重组、收购、诉讼等）
- 股东大会/董事会决议
- 分红派息
- 股权变动

#### 3. 实现公告内容提取 ⭐
**优先级**: 中  
**预计时间**: 2-3小时

**任务细节**:
- [ ] 下载PDF公告文件
- [ ] 提取PDF文本内容
- [ ] 关键信息抽取（金额、日期、人名等）
- [ ] 生成公告摘要

**依赖库**:
```bash
pip install PyPDF2 pdfplumber
```

#### 4. 测试和优化 ⭐
**优先级**: 中  
**预计时间**: 1-2小时

**任务细节**:
- [ ] 创建测试脚本 `test_cninfo_crawler.py`
- [ ] 测试不同股票的公告查询
- [ ] 测试日期范围筛选
- [ ] 性能优化（批量查询、缓存等）

---

## 🔗 任务三：API集成

### 📁 文件位置
- `backend/api/legal_api.py`（新建）
- `backend/api/announcement_api.py`（新建）

### 🔨 待完成任务

#### 1. 创建法律风险API ⭐⭐
**优先级**: 高  
**预计时间**: 1-2小时

**任务细节**:
- [ ] 创建 `legal_api.py`
- [ ] 实现 `/api/legal/search` 端点（搜索案件）
- [ ] 实现 `/api/legal/risk` 端点（风险分析）
- [ ] 实现 `/api/legal/detail` 端点（案件详情）
- [ ] 在 `server.py` 中注册路由

**API设计**:
```python
@router.get("/api/legal/search")
async def search_cases(
    company_name: str,
    days: int = 365
):
    """搜索公司相关案件"""
    pass

@router.get("/api/legal/risk")
async def analyze_risk(
    company_name: str
):
    """分析法律风险"""
    pass
```

#### 2. 创建公告信息API ⭐⭐
**优先级**: 高  
**预计时间**: 1-2小时

**任务细节**:
- [ ] 创建 `announcement_api.py`
- [ ] 实现 `/api/announcement/list` 端点（公告列表）
- [ ] 实现 `/api/announcement/important` 端点（重要公告）
- [ ] 实现 `/api/announcement/analyze` 端点（公告分析）
- [ ] 在 `server.py` 中注册路由

**API设计**:
```python
@router.get("/api/announcement/list")
async def get_announcements(
    stock_code: str,
    days: int = 30,
    announcement_type: str = None
):
    """获取公告列表"""
    pass

@router.get("/api/announcement/important")
async def get_important_announcements(
    stock_code: str
):
    """获取重要公告"""
    pass
```

---

## 📦 依赖安装

### Python库
```bash
# 加密库
pip install pycryptodome

# PDF处理
pip install PyPDF2 pdfplumber

# HTTP请求
pip install requests httpx

# 已安装的库
# beautifulsoup4, lxml, pandas
```

---

## 🧪 测试计划

### 测试脚本
1. `test_wenshu_crawler.py` - 裁判文书网爬虫测试
2. `test_cninfo_crawler.py` - 巨潮资讯网爬虫测试
3. `test_legal_api.py` - 法律风险API测试
4. `test_announcement_api.py` - 公告信息API测试

### 测试用例
```python
# 测试公司列表
test_companies = [
    ("贵州茅台", "600519.SH"),
    ("中国平安", "601318.SH"),
    ("比亚迪", "002594.SZ"),
    ("宁德时代", "300750.SZ")
]
```

---

## 📅 时间规划

### 第1天（今天）
- [x] 创建任务清单
- [ ] 移植裁判文书网加密算法（2-3小时）
- [ ] 实现裁判文书网真实API调用（3-4小时）

### 第2天
- [ ] 验证和修复巨潮资讯网API（2-3小时）
- [ ] 实现公告类型过滤（1-2小时）
- [ ] 创建法律风险API（1-2小时）

### 第3天
- [ ] 创建公告信息API（1-2小时）
- [ ] 编写测试脚本（2-3小时）
- [ ] 性能优化和文档编写（2-3小时）

**总预计时间**: 18-24小时（3个工作日）

---

## 🎯 成功标准

### 中国裁判文书网爬虫
- ✅ 能成功搜索指定公司的案件
- ✅ 返回至少5个真实案件数据
- ✅ 加密算法与JS版本一致
- ✅ 风险评分准确

### 巨潮资讯网爬虫
- ✅ 能成功获取指定股票的公告
- ✅ 返回至少10条真实公告数据
- ✅ 公告类型分类准确
- ✅ 重要公告识别准确

### API集成
- ✅ 所有API端点正常工作
- ✅ 响应时间 < 5秒
- ✅ 错误处理完善
- ✅ 文档完整

---

## 🚀 开始工作

### 立即开始
1. **安装依赖**:
   ```bash
   pip install pycryptodome PyPDF2 pdfplumber
   ```

2. **创建测试脚本**:
   ```bash
   cd d:\AlphaCouncil
   touch test_wenshu_crawler.py
   touch test_cninfo_crawler.py
   ```

3. **开始第一个任务**: 移植裁判文书网加密算法

---

## 📝 注意事项

### 反爬虫策略
1. **请求频率控制**: 每次请求间隔1-2秒
2. **User-Agent轮换**: 使用多个真实浏览器UA
3. **Cookie管理**: 保持会话状态
4. **代理IP**: 如需要，使用代理池

### 法律合规
1. **遵守robots.txt**: 检查网站爬虫协议
2. **数据使用**: 仅用于投资分析，不商业化
3. **频率限制**: 避免对服务器造成压力
4. **数据保护**: 不泄露敏感信息

---

## 📚 参考资源

### 官方文档
- 中国裁判文书网: https://wenshu.court.gov.cn/
- 巨潮资讯网: http://www.cninfo.com.cn/

### 技术文档
- `docs/中国裁判文书网.cpws.js.md` - 加密算法
- `docs/财联社.js.md` - 加密算法参考
- `docs/爬虫集成实施计划.md` - 整体规划

---

现在开始第一个任务吧！🚀
