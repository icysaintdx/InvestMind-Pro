# 爬虫集成实施计划

> 开始时间: 2025-12-04 04:40  
> 状态: 🚀 进行中

---

## 📋 已有爬虫清单

### 1. china_market_crawler.py ✅
**包含的爬虫**:
- ✅ 东方财富新闻 (`get_eastmoney_news`)
- ✅ 新浪财经新闻 (`get_sina_news`)
- ✅ 雪球评论 (`get_xueqiu_comments`)
- ✅ 财联社快讯 (`get_cls_news`)
- ✅ AKShare新闻 (`get_akshare_news`)
- ✅ Tushare新闻 (`get_tushare_news`)

### 2. social_media_crawler.py ✅
**包含的爬虫**:
- ✅ 微博爬虫（基类）
- ✅ 抖音爬虫（基类）

---

## 🧪 测试计划

### 阶段1: 测试现有爬虫（今天完成）

#### 步骤1: 运行测试脚本
```bash
cd d:\AlphaCouncil
python test_crawlers.py
```

#### 步骤2: 检查测试结果
- [ ] 东方财富新闻 - 是否成功？
- [ ] 新浪财经新闻 - 是否成功？
- [ ] 雪球评论 - 是否成功？
- [ ] 财联社快讯 - 是否成功？
- [ ] AKShare新闻 - 是否成功？
- [ ] Tushare新闻 - 是否成功？

#### 步骤3: 修复失败的爬虫
根据测试结果，逐个修复失败的爬虫

---

## 🔧 待实现的爬虫

### 高优先级（本周完成）

#### 1. 东方财富股吧爬虫 🔥
**目标**: 获取散户讨论、热点题材  
**数据内容**:
- 帖子标题、内容
- 发帖时间、作者
- 阅读数、评论数、点赞数
- 情绪标签

**技术方案**:
```python
class EastmoneyGubaSpider:
    """东方财富股吧爬虫"""
    
    def get_stock_posts(self, stock_code: str, limit: int = 50):
        """获取股票吧帖子"""
        # API: https://guba.eastmoney.com/list,{stock_code}.html
        pass
    
    def get_post_detail(self, post_id: str):
        """获取帖子详情"""
        pass
    
    def analyze_sentiment(self, posts: List[Dict]):
        """分析讨论情绪"""
        pass
```

**预期输出**:
```json
{
    "stock_code": "600519",
    "posts": [
        {
            "title": "茅台今天又涨了",
            "content": "看好后市...",
            "author": "用户123",
            "publish_time": "2025-12-04 10:30",
            "read_count": 1234,
            "comment_count": 56,
            "like_count": 89,
            "sentiment": "positive"
        }
    ],
    "overall_sentiment": {
        "score": 0.65,
        "label": "积极",
        "positive_ratio": 0.70,
        "negative_ratio": 0.15
    }
}
```

#### 2. 巨潮资讯网公告爬虫 📄
**目标**: 获取公司公告、财报  
**数据内容**:
- 公告标题、类型
- 发布时间
- PDF链接
- 关键信息提取

**技术方案**:
```python
class CnInfoSpider:
    """巨潮资讯网爬虫"""
    
    def get_announcements(self, stock_code: str, start_date: str, end_date: str):
        """获取公司公告"""
        # API: http://www.cninfo.com.cn/new/disclosure
        pass
    
    def download_pdf(self, announcement_id: str):
        """下载公告PDF"""
        pass
    
    def extract_key_info(self, pdf_path: str):
        """提取关键信息"""
        pass
```

#### 3. 证券时报爬虫 📰
**目标**: 获取权威财经新闻  
**数据内容**:
- 新闻标题、内容
- 发布时间、作者
- 相关股票
- 新闻分类

**技术方案**:
```python
class StcnSpider:
    """证券时报爬虫"""
    
    def get_latest_news(self, limit: int = 20):
        """获取最新新闻"""
        # URL: http://www.stcn.com/
        pass
    
    def search_stock_news(self, stock_code: str):
        """搜索股票相关新闻"""
        pass
```

### 中优先级（下周完成）

#### 4. 中国裁判文书网爬虫 ⚖️
**目标**: 获取公司涉诉信息  
**数据内容**:
- 案件名称、类型
- 涉案金额
- 判决结果
- 案件状态

**技术方案**:
```python
class WenshuSpider:
    """中国裁判文书网爬虫"""
    
    def search_company_cases(self, company_name: str):
        """搜索公司涉诉案件"""
        # URL: https://wenshu.court.gov.cn/
        pass
    
    def get_case_detail(self, case_id: str):
        """获取案件详情"""
        pass
```

---

## 🌐 代理功能实现

### 需求分析
- Google News 需要代理
- Reddit 需要代理
- 海外大模型需要代理
- 国内网站不使用代理

### 实现方案

#### 步骤1: 创建代理管理器
```python
# backend/utils/proxy_manager.py

import os
import requests
from typing import Optional, Dict

class ProxyManager:
    """代理管理器"""
    
    def __init__(self):
        self.http_proxy = os.getenv('HTTP_PROXY', '')
        self.https_proxy = os.getenv('HTTPS_PROXY', '')
        
        # 需要代理的域名
        self.proxy_domains = [
            'google.com',
            'googleapis.com',
            'reddit.com',
            'openai.com',
            'anthropic.com',
            'api.openai.com',
            'api.anthropic.com'
        ]
    
    def should_use_proxy(self, url: str) -> bool:
        """判断URL是否需要代理"""
        return any(domain in url for domain in self.proxy_domains)
    
    def get_proxies(self, url: str) -> Optional[Dict[str, str]]:
        """获取代理配置"""
        if not self.should_use_proxy(url):
            return None
        
        if not self.http_proxy and not self.https_proxy:
            return None
        
        return {
            'http': self.http_proxy,
            'https': self.https_proxy
        }
    
    def make_request(self, url: str, **kwargs):
        """带代理的请求"""
        proxies = self.get_proxies(url)
        if proxies:
            kwargs['proxies'] = proxies
        return requests.get(url, **kwargs)

# 全局实例
_proxy_manager = None

def get_proxy_manager():
    """获取代理管理器实例"""
    global _proxy_manager
    if _proxy_manager is None:
        _proxy_manager = ProxyManager()
    return _proxy_manager
```

#### 步骤2: 集成到爬虫中
```python
from backend.utils.proxy_manager import get_proxy_manager

class GoogleNewsSpider:
    """Google新闻爬虫"""
    
    def __init__(self):
        self.proxy_manager = get_proxy_manager()
    
    def get_news(self, query: str):
        """获取新闻"""
        url = f"https://news.google.com/search?q={query}"
        
        # 使用代理管理器
        response = self.proxy_manager.make_request(url)
        return response.json()
```

#### 步骤3: 配置代理
在 `.env` 文件中添加：
```bash
# 代理配置（可选）
HTTP_PROXY=http://127.0.0.1:7890
HTTPS_PROXY=http://127.0.0.1:7890

# 或者使用 socks5
# HTTP_PROXY=socks5://127.0.0.1:1080
# HTTPS_PROXY=socks5://127.0.0.1:1080
```

---

## 📅 时间表

### 第1天（今天）
- [x] 创建测试脚本
- [ ] 运行测试
- [ ] 修复失败的爬虫
- [ ] 文档整理

### 第2-3天
- [ ] 实现东方财富股吧爬虫
- [ ] 测试股吧爬虫
- [ ] 集成情绪分析

### 第4-5天
- [ ] 实现巨潮资讯网爬虫
- [ ] 实现证券时报爬虫
- [ ] 测试新爬虫

### 第6-7天
- [ ] 实现代理功能
- [ ] 集成Google News
- [ ] 集成Reddit

### 第8-10天
- [ ] 实现中国裁判文书网爬虫
- [ ] 统一数据接口
- [ ] 前端集成

---

## 🎯 成功标准

### 爬虫质量
- ✅ 成功率 > 90%
- ✅ 响应时间 < 5秒
- ✅ 数据完整性 > 95%

### 数据质量
- ✅ 新闻相关性 > 80%
- ✅ 情绪分析准确率 > 70%
- ✅ 去重率 > 90%

### 系统稳定性
- ✅ 反爬虫机制应对
- ✅ 错误处理完善
- ✅ 日志记录详细

---

## 📝 注意事项

### 反爬虫策略
1. **User-Agent 轮换** - 模拟不同浏览器
2. **请求频率控制** - 避免被封IP
3. **Cookie 管理** - 维持会话
4. **IP代理池** - 如果需要
5. **验证码识别** - 如果遇到

### 法律合规
1. **遵守 robots.txt** - 尊重网站规则
2. **合理使用频率** - 不影响网站正常运行
3. **数据用途** - 仅用于个人学习研究
4. **隐私保护** - 不泄露用户信息

### 数据存储
1. **结构化存储** - 便于查询分析
2. **定期清理** - 避免数据冗余
3. **备份机制** - 防止数据丢失

---

## 🚀 立即开始

### 第一步：运行测试
```bash
cd d:\AlphaCouncil
python test_crawlers.py
```

### 第二步：查看结果
根据测试结果，确定哪些爬虫需要修复

### 第三步：修复问题
逐个修复失败的爬虫

---

**现在请运行测试脚本，把结果发给我！** 🎯
