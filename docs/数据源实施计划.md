# 数据源实施计划

> 创建时间: 2025-12-04 06:23  
> 状态: 📋 计划中

---

## 🎯 总体策略

### 数据分类方式: LLM自行处理 ✅

**原因**:
1. LLM可以根据上下文理解数据
2. 更灵活，不受固定分类限制
3. 可以发现隐藏的关联
4. 维护成本低

**实施方式**:
- 为每个智能体提供完整的数据源
- 让LLM根据角色自行筛选和分析
- 在prompt中明确智能体的关注点

**示例**:
```javascript
// 风险评估智能体
const instruction = `
你是系统性风险评估专家。
请从以下数据中识别：
1. 法律诉讼风险
2. 监管处罚风险
3. 合规违规风险
4. 其他系统性风险

数据源：
${newsData}  // 包含所有新闻
${legalData}  // 包含法律文书
${announcementData}  // 包含公司公告
`
```

---

## 📊 当前状态详解

### 1. 风险评估 (risk_system)

**状态**: ⏳ 框架完成

**已完成**:
- ✅ 爬虫类定义 (`wenshu_crawler.py`)
- ✅ 数据获取方法
- ✅ 数据解析逻辑
- ✅ 错误处理

**待完成**:
- ❌ 3DES加密实现
- ❌ 真实API调用
- ❌ 反爬虫处理（curl_cffi）

**文件位置**:
```
backend/dataflows/legal/wenshu_crawler.py
```

**当前代码**:
```python
def search_company_cases(self, company_name: str, limit: int = 10) -> List[Dict]:
    """搜索公司相关案件（当前返回模拟数据）"""
    # TODO: 实现真实API调用
    return self._generate_mock_cases(company_name, limit)
```

**需要改为**:
```python
def search_company_cases(self, company_name: str, limit: int = 10) -> List[Dict]:
    """搜索公司相关案件"""
    # 1. 生成3DES加密参数
    encrypted_params = self._encrypt_params(company_name)
    
    # 2. 使用curl_cffi发送请求
    response = self._send_request(encrypted_params)
    
    # 3. 解析真实数据
    return self._parse_response(response)
```

---

### 2. 交易员/合规运营 (trader)

**状态**: ⏳ 框架完成

**已完成**:
- ✅ 爬虫类定义 (`cninfo_crawler.py`)
- ✅ 数据获取方法
- ✅ 数据解析逻辑
- ✅ 错误处理

**待完成**:
- ❌ 真实API调用
- ❌ 反爬虫处理

**文件位置**:
```
backend/dataflows/announcement/cninfo_crawler.py
```

**当前代码**:
```python
def get_company_announcements(self, stock_code: str, limit: int = 20) -> List[Dict]:
    """获取公司公告（当前返回模拟数据）"""
    # TODO: 实现真实API调用
    return self._generate_mock_announcements(stock_code, limit)
```

**需要改为**:
```python
def get_company_announcements(self, stock_code: str, limit: int = 20) -> List[Dict]:
    """获取公司公告"""
    # 1. 构建请求参数
    params = self._build_params(stock_code, limit)
    
    # 2. 发送请求
    response = self._send_request(params)
    
    # 3. 解析真实数据
    return self._parse_announcements(response)
```

---

### 3. 其他风险类智能体

**状态**: 📝 待开发

**包含**:
- risk_manager (风险经理)
- risk_conservative (保守型)
- risk_aggressive (激进型)
- risk_neutral (中性型)
- risk_portfolio (组合风险)

**策略**: 复用风险评估的数据源

**实施方式**:
```javascript
// 所有风险类智能体共享数据源
if (['risk_system', 'risk_manager', 'risk_conservative', 
     'risk_aggressive', 'risk_neutral', 'risk_portfolio'].includes(agent.id)) {
  
  // 获取法律风险数据
  const legalData = await fetchLegalData(data.symbol)
  
  // 根据不同角色提供不同的prompt
  const instruction = getRiskInstruction(agent.id, legalData)
}
```

---

## 🚀 实施步骤

### Phase 1: 实现真实API调用（本周）

#### 1.1 中国裁判文书网 (2-3天)

**步骤**:
1. ✅ 研究3DES加密算法（已完成，参考 `docs/中国裁判文书网.cpws.js.md`）
2. ⏳ 实现3DES加密
3. ⏳ 实现curl_cffi请求
4. ⏳ 测试真实API
5. ⏳ 集成到统一API

**代码位置**:
```python
# backend/dataflows/legal/wenshu_crawler.py

def _encrypt_params(self, company_name: str) -> str:
    """3DES加密参数"""
    from Crypto.Cipher import DES3
    # 实现加密逻辑
    pass

def _send_request(self, encrypted_params: str) -> dict:
    """使用curl_cffi发送请求"""
    from curl_cffi import requests
    # 实现请求逻辑
    pass
```

**参考文档**:
- `docs/真实API实现指南.md`
- `docs/中国裁判文书网.cpws.js.md`

**GitHub参考**:
- https://github.com/nixinxin/WenShu
- https://github.com/sixs/wenshu_spider

---

#### 1.2 巨潮资讯网 (1-2天)

**步骤**:
1. ⏳ 分析API接口
2. ⏳ 实现请求逻辑
3. ⏳ 测试真实API
4. ⏳ 集成到统一API

**代码位置**:
```python
# backend/dataflows/announcement/cninfo_crawler.py

def _build_params(self, stock_code: str, limit: int) -> dict:
    """构建请求参数"""
    # 实现参数构建
    pass

def _send_request(self, params: dict) -> dict:
    """发送请求"""
    import httpx
    # 实现请求逻辑
    pass
```

---

### Phase 2: 集成到前端（1天）

#### 2.1 创建统一API端点

**文件**: `backend/api/legal_announcement_api.py`

```python
from fastapi import APIRouter, HTTPException
from backend.dataflows.legal.wenshu_crawler import get_wenshu_crawler
from backend.dataflows.announcement.cninfo_crawler import get_cninfo_crawler

router = APIRouter(prefix="/api/legal-announcement", tags=["Legal & Announcement"])

@router.get("/legal/{company_name}")
async def get_legal_data(company_name: str):
    """获取法律风险数据"""
    crawler = get_wenshu_crawler()
    cases = crawler.search_company_cases(company_name)
    risk = crawler.analyze_legal_risk(cases)
    return {
        "success": True,
        "company": company_name,
        "cases": cases,
        "risk": risk
    }

@router.get("/announcement/{stock_code}")
async def get_announcement_data(stock_code: str):
    """获取公司公告数据"""
    crawler = get_cninfo_crawler()
    announcements = crawler.get_company_announcements(stock_code)
    important = crawler.filter_important_announcements(announcements)
    return {
        "success": True,
        "stock_code": stock_code,
        "announcements": announcements,
        "important": important
    }
```

#### 2.2 前端调用

**文件**: `alpha-council-vue/src/views/AnalysisView.vue`

```javascript
// 获取法律风险数据
const fetchLegalData = async (companyName) => {
  try {
    const response = await fetch(`http://localhost:8000/api/legal-announcement/legal/${companyName}`)
    const result = await response.json()
    return result
  } catch (e) {
    console.error('获取法律数据失败:', e)
    return null
  }
}

// 获取公司公告数据
const fetchAnnouncementData = async (stockCode) => {
  try {
    const response = await fetch(`http://localhost:8000/api/legal-announcement/announcement/${stockCode}`)
    const result = await response.json()
    return result
  } catch (e) {
    console.error('获取公告数据失败:', e)
    return null
  }
}

// 在智能体分析时使用
if (agent.id === 'risk_system') {
  const legalData = await fetchLegalData(data.name)
  if (legalData && legalData.success) {
    agentDataSources.value[agent.id] = [
      {
        source: '中国裁判文书网',
        count: legalData.cases.length,
        title: `${legalData.cases.length}条案件`
      }
    ]
  }
}
```

---

### Phase 3: 优化和完善（1-2天）

#### 3.1 添加缓存
```python
from functools import lru_cache
from datetime import datetime, timedelta

@lru_cache(maxsize=100)
def get_cached_legal_data(company_name: str, date: str):
    """缓存法律数据（按天缓存）"""
    crawler = get_wenshu_crawler()
    return crawler.search_company_cases(company_name)
```

#### 3.2 添加错误重试
```python
from tenacity import retry, stop_after_attempt, wait_exponential

@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))
def fetch_with_retry(url: str):
    """带重试的请求"""
    response = requests.get(url)
    response.raise_for_status()
    return response.json()
```

#### 3.3 添加代理池
```python
class ProxyPool:
    def __init__(self):
        self.proxies = self._load_proxies()
    
    def get_proxy(self):
        """获取可用代理"""
        # 实现代理轮换
        pass
```

---

## 📋 详细任务清单

### 本周任务（高优先级）

- [ ] **Day 1-2: 实现3DES加密**
  - [ ] 安装pycryptodome: `pip install pycryptodome`
  - [ ] 实现加密函数
  - [ ] 测试加密结果
  - [ ] 文档: `docs/3DES加密实现.md`

- [ ] **Day 2-3: 实现中国裁判文书网API**
  - [ ] 安装curl_cffi: `pip install curl_cffi`
  - [ ] 实现请求函数
  - [ ] 测试真实API
  - [ ] 文档: `docs/裁判文书网API测试.md`

- [ ] **Day 3-4: 实现巨潮资讯网API**
  - [ ] 分析API接口
  - [ ] 实现请求函数
  - [ ] 测试真实API
  - [ ] 文档: `docs/巨潮资讯网API测试.md`

- [ ] **Day 4-5: 集成到前端**
  - [ ] 创建API端点
  - [ ] 前端调用
  - [ ] 测试完整流程
  - [ ] 文档: `docs/法律公告集成完成.md`

---

## 🧪 测试计划

### 单元测试
```python
# test_legal_crawler.py
def test_wenshu_encryption():
    """测试3DES加密"""
    crawler = get_wenshu_crawler()
    encrypted = crawler._encrypt_params("贵州茅台")
    assert encrypted is not None

def test_wenshu_api():
    """测试真实API调用"""
    crawler = get_wenshu_crawler()
    cases = crawler.search_company_cases("贵州茅台")
    assert len(cases) > 0
```

### 集成测试
```bash
# 测试完整流程
python test_legal_announcement.py

# 预期输出
✅ 中国裁判文书网: 获取到5条案件
✅ 巨潮资讯网: 获取到10条公告
✅ 风险评估: 风险等级 - 低
```

---

## 💰 成本估算

### 开发时间
- 3DES加密: 0.5天
- 裁判文书网API: 1.5天
- 巨潮资讯网API: 1天
- 前端集成: 0.5天
- 测试优化: 0.5天
- **总计**: 4天

### 技术难度
- 3DES加密: ⭐⭐⭐
- 反爬虫处理: ⭐⭐⭐⭐
- API集成: ⭐⭐

---

## 🎯 成功标准

### 最小可行产品（MVP）
- ✅ 能获取真实的法律案件数据
- ✅ 能获取真实的公司公告数据
- ✅ 前端能正确显示数据源和数量
- ✅ 智能体能使用真实数据进行分析

### 完整版本
- ✅ MVP所有功能
- ✅ 添加缓存机制
- ✅ 添加错误重试
- ✅ 添加代理池
- ✅ 性能优化
- ✅ 完整的测试覆盖

---

## 🚀 立即开始

### 第一步: 安装依赖
```bash
pip install pycryptodome curl_cffi
```

### 第二步: 实现3DES加密
参考 `docs/真实API实现指南.md` 中的加密代码

### 第三步: 测试
```bash
python test_legal_crawler.py
```

---

**准备好开始实施了吗？** 🎯

建议从最简单的开始：
1. 先实现巨潮资讯网（相对简单）
2. 再实现裁判文书网（需要加密）
3. 最后优化和完善
