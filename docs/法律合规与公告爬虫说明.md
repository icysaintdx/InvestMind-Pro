# æ³•å¾‹åˆè§„ä¸å…¬å‘Šçˆ¬è™«è¯´æ˜

> åˆ›å»ºæ—¶é—´: 2025-12-04 05:38  
> çŠ¶æ€: âœ… æ¡†æ¶å·²åˆ›å»ºï¼Œå¾…å®ç°çœŸå®API

---

## ğŸ¯ ç›®æ ‡

### æ ¸å¿ƒæ•°æ®æº
1. **ä¸­å›½è£åˆ¤æ–‡ä¹¦ç½‘** - æ³•å¾‹é£é™©æ•°æ®
2. **å·¨æ½®èµ„è®¯ç½‘** - å…¬å¸å…¬å‘Šæ•°æ®

### é‡è¦æ€§
- â­â­â­â­â­ æ³•å¾‹åˆè§„æ˜¯æŠ•èµ„å†³ç­–çš„æ ¸å¿ƒå› ç´ 
- â­â­â­â­â­ å…¬å¸å…¬å‘ŠåŒ…å«é‡å¤§ä¿¡æ¯

---

## ğŸ“¦ å·²åˆ›å»ºçš„æ¨¡å—

### 1. ä¸­å›½è£åˆ¤æ–‡ä¹¦ç½‘çˆ¬è™«

**æ–‡ä»¶**: `backend/dataflows/legal/wenshu_crawler.py`

**åŠŸèƒ½**:
- æœç´¢å…¬å¸ç›¸å…³æ¡ˆä»¶
- åˆ†ææ³•å¾‹é£é™©
- é£é™©ç­‰çº§è¯„ä¼°

**æ–¹æ³•**:
```python
from backend.dataflows.legal.wenshu_crawler import get_wenshu_crawler

crawler = get_wenshu_crawler()

# æœç´¢æ¡ˆä»¶
cases = crawler.search_company_cases("å…¬å¸åç§°", days=365)

# é£é™©åˆ†æ
risk = crawler.analyze_legal_risk(cases)
```

**è¿”å›æ•°æ®**:
```python
{
    'case_id': 'xxx',
    'case_name': 'æ¡ˆä»¶åç§°',
    'case_type': 'æ°‘äº‹æ¡ˆä»¶',
    'court': 'æ³•é™¢åç§°',
    'case_date': '2024-11-15',
    'parties': ['åŸå‘Š', 'è¢«å‘Š'],
    'case_reason': 'åˆåŒçº çº·',
    'risk_level': 'medium',
    'summary': 'æ¡ˆä»¶æ‘˜è¦'
}
```

---

### 2. å·¨æ½®èµ„è®¯ç½‘çˆ¬è™«

**æ–‡ä»¶**: `backend/dataflows/announcement/cninfo_crawler.py`

**åŠŸèƒ½**:
- è·å–å…¬å¸å…¬å‘Š
- è¿‡æ»¤é‡è¦å…¬å‘Š
- å…¬å‘Šåˆ†æ

**æ–¹æ³•**:
```python
from backend.dataflows.announcement.cninfo_crawler import get_cninfo_crawler

crawler = get_cninfo_crawler()

# è·å–å…¬å‘Š
announcements = crawler.get_company_announcements("600519", days=30)

# è¿‡æ»¤é‡è¦å…¬å‘Š
important = crawler.filter_important_announcements(announcements)

# å…¬å‘Šåˆ†æ
analysis = crawler.analyze_announcements(announcements)
```

**è¿”å›æ•°æ®**:
```python
{
    'announcement_id': 'xxx',
    'stock_code': '600519',
    'title': 'å…¬å‘Šæ ‡é¢˜',
    'type': 'å®šæœŸæŠ¥å‘Š',
    'publish_date': '2024-10-30',
    'url': 'http://...',
    'summary': 'å…¬å‘Šæ‘˜è¦',
    'importance': 'high'
}
```

---

## ğŸ”§ å½“å‰çŠ¶æ€

### âœ… å·²å®Œæˆ
1. åˆ›å»ºçˆ¬è™«æ¡†æ¶
2. å®šä¹‰æ•°æ®ç»“æ„
3. å®ç°åŸºæœ¬æ–¹æ³•
4. åˆ›å»ºæµ‹è¯•è„šæœ¬

### â³ å¾…å®ç°
1. **çœŸå®APIè°ƒç”¨** - å½“å‰ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®
2. **è®¤è¯å¤„ç†** - ä¸­å›½è£åˆ¤æ–‡ä¹¦ç½‘éœ€è¦è®¤è¯
3. **åçˆ¬è™«å¤„ç†** - éœ€è¦å¤„ç†éªŒè¯ç ã€IPé™åˆ¶ç­‰
4. **æ•°æ®è§£æ** - è§£æå®é™…è¿”å›çš„HTML/JSON
5. **é”™è¯¯å¤„ç†** - å®Œå–„å¼‚å¸¸å¤„ç†

---

## ğŸš§ å®ç°éš¾ç‚¹

### 1. ä¸­å›½è£åˆ¤æ–‡ä¹¦ç½‘

**éš¾ç‚¹**:
- éœ€è¦ç™»å½•è®¤è¯
- å¤æ‚çš„åçˆ¬è™«æœºåˆ¶
- éªŒè¯ç è¯†åˆ«
- IPé™åˆ¶

**è§£å†³æ–¹æ¡ˆ**:
- ä½¿ç”¨ `selenium` æ¨¡æ‹Ÿæµè§ˆå™¨
- ä½¿ç”¨ `curl_cffi` æ¨¡æ‹ŸçœŸå®æµè§ˆå™¨æŒ‡çº¹
- ä½¿ç”¨ä»£ç†IPæ± 
- OCRè¯†åˆ«éªŒè¯ç 

**å‚è€ƒä»£ç **:
```python
from curl_cffi import requests as curl_requests

session = curl_requests.Session(impersonate="chrome120")
response = session.get(url, headers=headers)
```

---

### 2. å·¨æ½®èµ„è®¯ç½‘

**éš¾ç‚¹**:
- APIå‚æ•°å¤æ‚
- éœ€è¦ç‰¹å®šçš„Referer
- è¿”å›æ ¼å¼ä¸ç»Ÿä¸€

**è§£å†³æ–¹æ¡ˆ**:
- åˆ†æç½‘ç«™è¯·æ±‚
- ä½¿ç”¨æ­£ç¡®çš„è¯·æ±‚å¤´
- å¤„ç†å¤šç§è¿”å›æ ¼å¼

**å‚è€ƒä»£ç **:
```python
headers = {
    'User-Agent': 'Mozilla/5.0...',
    'Referer': 'http://www.cninfo.com.cn',
    'Content-Type': 'application/x-www-form-urlencoded'
}

params = {
    'stock': stock_code,
    'seDate': f'{start_date}~{end_date}',
    'pageNum': 1,
    'pageSize': 30
}

response = requests.post(api_url, data=params, headers=headers)
```

---

## ğŸ“Š é‡è¦å…¬å‘Šç±»å‹

### å®šæœŸæŠ¥å‘Š
- å¹´åº¦æŠ¥å‘Š
- åŠå¹´åº¦æŠ¥å‘Š
- å­£åº¦æŠ¥å‘Š

### é‡å¤§äº‹é¡¹
- é‡å¤§èµ„äº§é‡ç»„
- æ”¶è´­å…¼å¹¶
- å…³è”äº¤æ˜“
- å¯¹å¤–æ‹…ä¿
- è¯‰è®¼ä»²è£

### è‚¡æƒå˜åŠ¨
- è‚¡ä¸œå¤§ä¼šå†³è®®
- è‘£äº‹ä¼šå†³è®®
- è‚¡æƒè´¨æŠ¼
- è‚¡æƒè½¬è®©
- å¢å‘é…è‚¡

### é£é™©æç¤º
- ä¸šç»©é¢„å‘Š
- ä¸šç»©å¿«æŠ¥
- é£é™©è­¦ç¤º
- ç«‹æ¡ˆè°ƒæŸ¥
- è¡Œæ”¿å¤„ç½š

---

## ğŸ§ª æµ‹è¯•

### è¿è¡Œæµ‹è¯•
```bash
python test_legal_announcement.py
```

### é¢„æœŸç»“æœ
```
âœ… ä¸­å›½è£åˆ¤æ–‡ä¹¦ç½‘: æ‰¾åˆ°æ¡ˆä»¶ï¼ˆæ¨¡æ‹Ÿæ•°æ®ï¼‰
âœ… å·¨æ½®èµ„è®¯ç½‘: è·å–å…¬å‘Šï¼ˆæ¨¡æ‹Ÿæ•°æ®ï¼‰
```

---

## ğŸ”„ é›†æˆè®¡åˆ’

### é˜¶æ®µ1: åŸºç¡€å®ç°ï¼ˆå½“å‰ï¼‰
- âœ… åˆ›å»ºçˆ¬è™«æ¡†æ¶
- âœ… ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®æµ‹è¯•

### é˜¶æ®µ2: çœŸå®APIï¼ˆä¸‹ä¸€æ­¥ï¼‰
- â³ å®ç°ä¸­å›½è£åˆ¤æ–‡ä¹¦ç½‘çœŸå®API
- â³ å®ç°å·¨æ½®èµ„è®¯ç½‘çœŸå®API
- â³ å¤„ç†è®¤è¯å’Œåçˆ¬è™«

### é˜¶æ®µ3: é›†æˆåˆ°ç»Ÿä¸€API
- â³ åˆ›å»ºAPIç«¯ç‚¹
- â³ é›†æˆåˆ° `unified_news_api.py`
- â³ å‰ç«¯å±•ç¤º

---

## ğŸ’¡ ä½¿ç”¨å»ºè®®

### 1. æ³•å¾‹é£é™©è¯„ä¼°
```python
# è·å–å…¬å¸æ³•å¾‹æ¡ˆä»¶
cases = wenshu_crawler.search_company_cases("å…¬å¸åç§°")

# åˆ†æé£é™©
risk = wenshu_crawler.analyze_legal_risk(cases)

if risk['risk_level'] == 'high':
    print("âš ï¸ é«˜æ³•å¾‹é£é™©ï¼Œå»ºè®®è°¨æ…æŠ•èµ„")
```

### 2. å…¬å‘Šç›‘æ§
```python
# è·å–æœ€è¿‘å…¬å‘Š
announcements = cninfo_crawler.get_company_announcements("600519", days=7)

# è¿‡æ»¤é‡è¦å…¬å‘Š
important = cninfo_crawler.filter_important_announcements(announcements)

for ann in important:
    if ann['importance'] == 'high':
        print(f"ğŸ”” é‡è¦å…¬å‘Š: {ann['title']}")
```

---

## ğŸ“‹ ä¸‹ä¸€æ­¥

### ç«‹å³æµ‹è¯•
```bash
python test_legal_announcement.py
```

### ç»§ç»­å¼€å‘
1. å®ç°çœŸå®APIè°ƒç”¨
2. å¤„ç†è®¤è¯å’Œåçˆ¬è™«
3. é›†æˆåˆ°ç»Ÿä¸€API
4. å‰ç«¯å±•ç¤º

---

**æ³¨æ„**: å½“å‰ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®ï¼Œå®é™…ä½¿ç”¨éœ€è¦å®ç°çœŸå®çš„APIè°ƒç”¨ï¼
