股票分析项目（非技术指标层面）核心范畴与关注维度汇总
一、公司基本面核心维度（运营 + 财务 + 竞争力）
核心经营数据：营收 / 净利润增速（同比 / 环比）、毛利率 / 净利率波动、营收构成（核心业务占比、新业务增长潜力）、产能利用率、库存周转天数、应收账款回收率
财务健康度：资产负债率、流动比率 / 速动比率、现金流结构（经营 / 投资 / 筹资现金流匹配度）、商誉规模及减值风险、研发投入占比及转化效率（专利 / 新产品落地）
核心竞争力：行业地位（市场占有率）、护城河（技术壁垒 / 品牌优势 / 政策许可 / 供应链掌控力）、核心团队稳定性（高管持股 / 离职率）、客户集中度（前 5 大客户占比）、供应商依赖度
发展规划与落地：战略布局（新市场 / 新赛道拓展）、产能扩张计划、并购重组进展、股权激励 / 员工持股计划（绑定核心利益）、分红政策（股息率 / 分红比例）
二、风险管控核心维度（法律 + 合规 + 经营）
法律风险：诉讼 / 仲裁案件（涉案金额、影响范围）、行政处罚记录（环保 / 税务 / 市场监管）、知识产权纠纷（侵权 / 被侵权）、劳动纠纷集中爆发情况
合规风险：信息披露违规（虚假陈述 / 延迟披露）、关联交易不规范（利益输送嫌疑）、内控缺陷（审计报告非标意见）、行业监管政策变动适配性（如医药 / 新能源 / 教培等强监管行业）
经营隐性风险：单一业务 / 单一市场依赖风险、原材料价格波动风险、汇率风险（出口型企业）、政策风险（行业扶持退坡 / 限制政策）、自然灾害 / 地缘政治影响
信用风险：债券违约记录、评级下调风险、银行授信额度紧张、逾期债务规模
三、市场与舆情维度（新闻 + 资金 + 情绪）
核心新闻舆情：公司公告（业绩预告 / 重大合同 / 减持 / 增持 / 回购）、行业新闻（政策导向 / 技术突破 / 需求变化）、权威媒体报道（正面 / 负面舆情）、社交媒体舆情（投资者讨论热度 / 情绪倾向）
资金流向关联：机构持仓变动（公募 / 私募 / 社保 / 北向资金增减持）、股东人数变化（集中 / 分散）、大宗交易频次及折价率、融资融券余额变动（杠杆资金情绪）
产业链舆情：上游原材料供应稳定性、下游需求景气度、竞争对手动态（价格战 / 新品发布 / 市场扩张）、行业景气度拐点信号（如库存周期 / PMI 数据关联）
四、技术面辅助维度（非指标类，侧重趋势与支撑）
关键价位与筹码：历史支撑位 / 压力位（前期高低点、成交密集区）、筹码分布结构（套牢盘 / 获利盘集中度）、破位 / 突破后的量能配合情况
市场结构关联：板块联动性（所属行业 / 概念板块涨幅贡献度）、大盘环境适配性（牛市 / 熊市 / 震荡市中的表现逻辑）、行业指数趋势对个股的牵引作用
五、智能体 / AI 大数据落地核心方向
舆情智能监测：实时抓取公司 / 行业新闻、社交媒体、研报观点，通过 NLP 进行情绪打分、风险预警（如负面舆情发酵速度、关键词触发）
数据自动聚合：对接企业工商信息、财报数据库、法律文书数据库、监管处罚公示平台，自动提取核心数据并更新（如新增诉讼、营收变动）
风险量化评估：基于历史风险事件与股价波动的相关性，建立风险评分模型（如法律风险评分、合规风险评分），输出风险等级与影响权重
基本面异动预警：设置核心指标阈值（如净利润增速骤降、毛利率下滑超 5%、新增大额诉讼），触发自动提醒并关联历史案例分析
关联数据挖掘：挖掘产业链上下游数据（如上游原材料库存、下游终端需求数据）与个股业绩的关联性，提前预判业绩拐点
六、需重点注意的核心原则
数据优先级：官方数据（公司公告、监管公示）＞权威第三方数据＞非官方舆情，避免虚假信息误导
风险权重适配：高风险行业（如 ST 股、强监管行业）需提高法律 / 合规风险权重，成长型企业需提高研发转化 / 团队稳定性权重
动态更新机制：核心数据（舆情、资金、风险事件）需实时更新，静态数据（财务数据、战略规划）需跟踪落地进度，避免 “静态分析” 脱离实际
跨维度关联验证：单一维度信号需交叉验证（如业绩预增需配合订单数据、舆情正面需排除关联方炒作），避免孤立判断


股票非技术面分析 AI 智能体核心架构设计（含细节落地）
一、整体架构总览（分层设计，可落地、可扩展）
层级	核心功能	技术选型（适配本地化部署）	数据流转逻辑
数据接入层	多源数据抓取、清洗、标准化	Python+Scrapy/Requests、FlinkCDC	数据源→清洗引擎→标准化数据集→数据仓库
核心引擎层	舆情分析、风险量化、基本面挖掘	LLM（本地化部署）、NLP、机器学习	标准化数据→各专项引擎→结构化分析结果
业务应用层	风险预警、异动监测、综合评分	自定义业务逻辑 + 可视化组件	分析结果→业务规则校验→输出决策支撑信息
交互展示层	可视化 dashboard、告警推送、接口输出	Vue3+ECharts、FastAPI	决策支撑信息→可视化展示 / 接口调用 / 告警
数据存储层	原始数据、中间结果、最终结论存储	PostgreSQL、MinIO、Redis	全链路数据分层存储，支持快速查询回溯
二、各层级详细设计（含模块、字段、规则）
（一）数据接入层（核心：全量、实时、精准，避免垃圾数据）
1. 数据源分类与抓取细节
数据类型	具体数据源	抓取频率	核心字段（标准化后）	抓取工具与策略
公司基本面数据	巨潮资讯网、东方财富 Choice（API 对接）、同花顺 iFinD	财务数据：财报披露日同步；经营数据：每日更新	公司全称 / 代码、营收、净利润、毛利率、净利率、资产负债率、研发投入、核心业务占比、高管信息、股东结构、并购重组进展	Requests+API 对接（付费数据源）；Scrapy（免费公告）；数据去重：基于公告编号 + 字段哈希
法律合规数据	中国裁判文书网、中国执行信息公开网、监管处罚公示平台	实时抓取（每 30 分钟）	涉案公司、案件类型（诉讼 / 仲裁 / 处罚）、涉案金额、影响范围、处理结果、公示日期、处罚机关	Scrapy + 反爬策略（User-Agent 轮换、IP 代理池）；OCR 识别非结构化文书
舆情数据	公司公告、证券时报 / 中国证券报、同花顺 / 东方财富股吧、微博 / 雪球	实时抓取（每 10 分钟）	舆情主体、发布时间、舆情内容、来源渠道、关键词、情绪倾向（正面 / 负面 / 中性）、传播热度	多线程爬虫 + NLP 预处理（去停用词、分词）；股吧 / 社交媒体：模拟登录 + 增量抓取
资金与市场数据	北向资金流向、机构持仓报告、大宗交易平台、融资融券数据	实时（资金流向）；每日（持仓 / 大宗）	股票代码、机构类型、增减持数量、持仓占比、大宗交易价格、折价率、融资余额、融券余额	API 对接（券商 / 数据商）；Redis 缓存实时数据，避免重复请求
产业链数据	行业协会报告、PMI / 库存周期数据、上下游企业公告	每日更新	行业名称、核心指标（如原材料价格、库存天数、需求增速）、关联上市公司、影响逻辑	第三方数据接口 + 网页爬取；数据关联：基于产业链图谱匹配
2. 数据清洗与标准化规则
去重规则：基于 “数据来源 + 唯一标识（如公告编号、案件编号）+ 核心字段” 三重去重，避免重复录入
格式标准化：日期统一为 “YYYY-MM-DD HH:MM:SS”，金额统一为 “万元”，百分比保留 2 位小数，公司名称统一为证监会登记全称
异常值处理：财务数据中明显异常值（如净利润骤增 10 倍以上）标注 “待验证”，关联公告原文交叉校验；舆情数据过滤广告、无意义灌水内容
数据补全：缺失的核心字段（如某季度毛利率），通过历史数据趋势预测 + 行业均值补全，标注 “补全数据” 标识
（二）核心引擎层（核心：AI 驱动，量化分析，精准建模）
1. 舆情智能分析引擎（LLM+NLP 双驱动）
核心功能：情绪量化、关键词提取、舆情发酵预警
技术细节：
基于本地化部署的 LLM（如 Llama 3 70B / 通义千问开源版），微调训练金融领域语料（公司公告、券商研报、法律文书），提升专业术语理解能力
情绪评分模型：采用 TextCNN+BiLSTM 融合模型，输出情绪得分（0-10 分，0 为极度负面，10 为极度正面），阈值设置：≤3 分（负面）、4-7 分（中性）、≥8 分（正面）
关键词提取：采用 TF-IDF+TextRank 算法，提取核心关键词（如 “诉讼”“重大合同”“减持”“技术突破”），关联预设风险 / 利好标签库
发酵预警规则：负面舆情 1 小时内传播量超 1 万次、互动量超 5000 次，或连续 3 条以上负面舆情关联同一事件，触发一级预警；正面舆情关联 “重大合同”“业绩暴增”，且传播量超 5 万次，标记为 “利好异动”
2. 风险量化评估引擎（机器学习 + 规则引擎）
核心功能：多维度风险评分、风险等级判定、风险传导分析
技术细节：
风险维度拆解（权重占比）：
法律风险（30%）：涉案金额 / 公司净资产占比（50%）、案件胜诉概率（30%）、是否涉及核心业务（20%）
合规风险（25%）：违规次数（40%）、违规类型严重程度（30%）、整改完成度（30%）
经营风险（25%）：单一业务依赖度（30%）、现金流健康度（30%）、核心团队稳定性（20%）、政策适配性（20%）
信用风险（20%）：评级变动（40%）、逾期债务占比（30%）、融资环境（30%）
评分模型：基于逻辑回归 + 随机森林融合模型，训练历史风险事件与股价波动的关联数据，输出风险总分（0-100 分），对应等级：≤30 分（高风险）、31-60 分（中风险）、61-80 分（低风险）、≥81 分（无明显风险）
风险传导规则：若某一维度风险评分≤20 分（该维度高风险），且关联另一维度风险评分≤30 分，触发 “风险传导预警”（如法律风险→信用风险，导致融资困难）
3. 基本面异动挖掘引擎（指标阈值 + 趋势分析）
核心功能：核心指标异动监测、业绩拐点预判、产业链关联分析
技术细节：
异动监测指标库（含阈值设置）：
指标类型	具体指标	异动阈值	预警等级
盈利指标	净利润增速	同比下滑超 30% 或骤增超 100%	一级
盈利指标	毛利率	单季度下滑超 5 个百分点	二级
现金流指标	经营现金流净额	连续两个季度为负	一级
运营指标	应收账款回收率	同比下滑超 20%	二级
发展指标	研发投入占比	同比下滑超 3 个百分点	三级
业绩拐点预判：基于 LSTM 时间序列模型，输入近 8 个季度核心财务数据 + 产业链数据（如上游原材料价格、下游需求增速），预测下一季度业绩趋势（增长 / 下滑 / 持平），准确率目标≥75%
产业链关联规则：通过知识图谱构建产业链关系（如 “锂矿→锂电池→新能源汽车”），当上游原材料价格下跌 10% 以上，自动关联下游相关上市公司，标记 “成本下降利好”；当下游需求增速下滑 20%，标记 “需求疲软风险”
4. 筹码与支撑位分析引擎（非指标类，侧重结构）
核心功能：关键价位识别、筹码集中度分析、板块联动匹配
技术细节：
关键价位计算：基于近 2 年 K 线数据，识别历史成交密集区（成交量占比超 30% 的价格区间），确定支撑位（密集区下沿）、压力位（密集区上沿）；结合复权数据，修正除权除息后的价位
筹码集中度分析：计算股东人数变动率（近 3 个月）、户均持股数，结合大宗交易折价率，判断筹码集中 / 分散趋势（股东人数减少 + 大宗交易折价率≤5%，标记 “筹码集中”）
板块联动规则：基于所属行业 / 概念板块，计算个股与板块指数的相关性（近 60 天），相关性≥0.8 标记 “强联动”，当板块指数突破关键压力位，自动提醒个股联动机会；板块指数破位，提醒个股跟随风险
（三）业务应用层（核心：贴合需求，落地决策支撑）
1. 风险预警模块
预警类型：一级（高风险，需立即关注）、二级（中风险，持续跟踪）、三级（低风险，提示注意）
触发机制：风险总分≤30 分、单一维度高风险 + 风险传导、负面舆情发酵、核心指标一级异动
输出内容：预警股票代码 + 名称、预警原因（如 “涉案金额占净资产 50%，法律风险高”）、关联数据（案件详情 / 财务数据 / 舆情原文）、历史相似案例（如 “2023 年 XX 股因同类诉讼股价下跌 15%”）、应对建议（如 “规避短期持仓，等待案件进展”）
推送方式：dashboard 弹窗、邮件、企业微信 / 钉钉机器人推送（支持自定义推送频率）
2. 基本面综合评分模块
评分维度：经营能力（30%）、财务健康（25%）、竞争力（20%）、发展潜力（15%）、风险控制（10%）
评分规则：各维度满分 100 分，加权求和得综合得分（0-100 分），对应评级：≥85 分（优质）、70-84 分（良好）、60-69 分（一般）、＜60 分（较差）
输出内容：综合得分 + 评级、各维度得分明细、核心优势（如 “毛利率行业第一，研发转化效率高”）、核心短板（如 “应收账款回收慢，现金流紧张”）、改进方向建议
3. 舆情与资金联动分析模块
核心逻辑：关联舆情情绪、资金流向、股价波动，挖掘因果关系
分析维度：
利好舆情 + 北向资金增持 + 融资余额上升→标记 “资金 + 舆情共振利好”
负面舆情 + 机构减持 + 股东人数增加→标记 “资金 + 舆情共振利空”
中性舆情 + 大宗交易密集 + 筹码集中→标记 “资金潜伏信号”
输出内容：联动信号类型、关联数据（舆情原文、资金流向明细）、历史共振后的股价表现（如 “近 1 年同类信号出现 10 次，股价 30 天内平均上涨 8%”）
4. 智能选股辅助模块（非技术面，贴合已筛股票二次校验）
核心功能：对已通过技术指标筛选的股票，进行非技术面二次打分，剔除高风险标的，筛选优质标的
筛选规则：
风险总分≥60 分（低风险及以上）
基本面综合评分≥70 分（良好及以上）
无一级预警风险
近 3 个月舆情情绪得分均值≥6 分（中性偏正面）
筹码集中 + 关键支撑位稳固
输出内容：通过二次筛选的股票列表、各股票核心亮点 + 风险提示、筛选未通过的股票及淘汰原因（如 “XX 股因法律风险高，筛选未通过”）
（四）交互展示层（核心：直观、易用，支持自定义）
1. 可视化 Dashboard（核心展示界面）
核心板块：
全局概览：筛选股票池整体评分分布、风险等级分布、舆情情绪分布
个股详情：点击股票代码，展示基本面评分、风险明细、舆情动态、资金流向、关键价位图表
预警中心：实时展示一级 / 二级预警股票，按预警紧急程度排序
联动信号：展示最新资金 + 舆情共振信号，标记信号强度（强 / 中 / 弱）
交互功能：支持股票代码搜索、自定义筛选条件（如 “风险等级低 + 基本面优质”）、数据导出（Excel/PDF）、图表缩放 / 下载
2. 接口输出模块（支持对接现有系统）
接口类型：RESTful API（基于 FastAPI 开发）
核心接口：
/api/stock/score：获取个股基本面 + 风险综合评分
/api/stock/warning：获取个股最新预警信息
/api/stock/sentiment：获取个股近 30 天舆情情绪数据
/api/stock/secondary-screen：获取二次筛选后的股票列表
接口参数：股票代码、时间范围、数据类型；返回格式：JSON（支持自定义字段）
3. 自定义配置模块
支持用户自定义：
风险阈值（如调整法律风险权重）
预警推送规则（推送频率、预警等级）
评分维度权重（如侧重发展潜力，提高该维度权重）
数据源优先级（如优先使用某数据商的财务数据）
（五）数据存储层（核心：稳定、高效，支持回溯）
1. 存储架构
原始数据存储：PostgreSQL（结构化数据，如财务数据、法律案件数据）+ MinIO（非结构化数据，如公告原文、舆情截图）
实时数据缓存：Redis（存储北向资金、舆情热度、预警信息，缓存有效期 10 分钟）
中间结果存储：PostgreSQL（存储清洗后的标准化数据、情绪评分、风险得分）
最终结果存储：PostgreSQL（存储综合评分、筛选结果）+ Elasticsearch（存储舆情数据，支持全文检索）
2. 数据备份与同步
备份策略：每日凌晨全量备份，每 6 小时增量备份，备份文件存储在 NAS（如 TrueNAS），保留 30 天备份记录
同步策略：实时数据（舆情、资金）采用增量同步，静态数据（财务、法律）采用全量同步 + 增量更新，确保数据一致性
三、本地化部署适配细节（贴合 NAS+Docker 环境）
容器化拆分：将各层级拆分为独立 Docker 容器（数据抓取容器、LLM 容器、NLP 引擎容器、API 容器、前端容器），通过 Docker Compose 编排，支持单节点 / 多节点部署
资源适配：
LLM 容器：分配足够 GPU 资源（如 NVIDIA A100/A10），启用模型量化（INT4/INT8），降低显存占用
数据抓取容器：分配 2 核 4G 内存，支持多线程并发抓取
整体资源建议：CPU≥16 核，内存≥64G，GPU≥16G 显存，存储≥1TB（预留数据增长空间）
网络配置：容器内配置代理池，解决爬虫反爬问题；启用 HTTPS，保障数据传输安全；内部容器通过私有网络通信，对外仅开放 API 接口和前端端口
四、落地优先级与里程碑
阶段	核心任务	时间周期	关键产出
第一阶段	数据接入层搭建（核心数据源对接 + 清洗）、存储层部署	2-3 周	标准化数据集、存储架构落地
第二阶段	核心引擎开发（舆情分析 + 风险量化）、接口开发	3-4 周	情绪评分模型、风险评分模型、核心 API
第三阶段	业务应用层开发（预警 + 评分 + 筛选模块）	2-3 周	二次筛选功能、预警功能落地
第四阶段	交互展示层开发、本地化部署适配	1-2 周	Dashboard、Docker 部署包
第五阶段	测试优化（数据准确性、模型准确率、性能）	2 周	优化后的智能体系统、部署文档



一、LLM 微调语料准备详细开发文档
（一）微调目标
适配股票非技术面分析场景，提升对金融术语、法律文书、财报表述的理解能力，强化情绪判断、风险识别、核心信息提取的精准度（如区分 “计提减值”“重大合同” 等关键事件对股价的影响）。
（二）语料来源与分类（贴合场景，保证专业性）
语料类型	    具体来源	筛选标准	数量要求
金融专业语料	券商研报（东方财富 / 同花顺）、上市公司年报 / 半年报、行业分析报告	近 5 年数据，聚焦 A 股主板 / 创业板，剔除无实质内容研报	不少于 5000 篇（总字数≥1000 万）
法律合规语料	中国裁判文书网（金融类诉讼 / 仲裁）、监管处罚决定书、合规指南	涉及上市公司，包含 “涉案金额”“处罚原因”“影响说明”	不少于 2000 份（总字数≥300 万）
舆情标注语料	公司公告、证券时报新闻、股吧 / 雪球讨论帖	包含明确的利好 / 利空 / 中性导向，可标注核心关键词	不少于 10000 条（每条≥50 字，标注情绪标签）
结构化标注语料	自定义标注（财务异动、风险事件、舆情关联）	标注 “事件类型 + 核心数据 + 影响等级”（如 “诉讼 - 涉案金额 5000 万 - 高风险”）	不少于 5000 条（结构化标签完整）
（三）语料预处理流程（标准化，适配微调）
1. 清洗规则
去重：基于文本哈希值去重，剔除重复研报、公告原文
降噪：删除广告、无关评论、格式错乱内容（如乱码、表格图片占位符）
截断 / 补全：单条语料长度控制在 512-2048 tokens，过长截断核心信息，过短补全上下文（如补充公告标题、事件背景）
术语统一：将 “归母净利润” 统一为 “归属于母公司股东的净利润”，“计提坏账” 统一为 “计提坏账准备” 等，建立金融术语映射表
2. 标注规范（监督式微调核心）
（1）情绪标注
标签：正面（1）、负面（-1）、中性（0）
示例：
原文：“公司 2024 年一季度营收同比增长 50%，核心产品市占率提升至 30%” → 标签：1，关键词：营收增长、市占率提升
原文：“公司因信息披露违规被证监会罚款 300 万元，涉事金额达 2 亿元” → 标签：-1，关键词：信息披露违规、罚款、涉事金额
（2）风险事件标注
标签：事件类型（法律 / 合规 / 经营 / 信用）、风险等级（高 / 中 / 低）、核心数据、影响范围
示例：
原文：“公司涉及重大合同纠纷，涉案金额 8000 万元，占 2023 年净资产的 40%” → 标签：法律风险 - 高风险 - 涉案金额 8000 万 - 影响净资产
（3）核心信息提取标注
任务：从财报 / 公告中提取关键字段（如营收、净利润、研发投入、重大合同金额等）
示例：
原文：“2024 年上半年，公司实现营业收入 12.5 亿元，同比增长 25%；净利润 2.3 亿元，同比增长 30%；研发投入 1.2 亿元，占营收比例 9.6%” → 提取：营收 12.5 亿（+25%）、净利润 2.3 亿（+30%）、研发投入 1.2 亿（9.6%）
3. 格式转换（适配 LLM 微调框架）
采用 ChatGLM/LLaMA 微调标准格式（JSONL），示例如下：
json
{
  "instruction": "判断以下舆情的情绪倾向，并提取核心关键词",
  "input": "公司2024年一季度净利润同比增长60%，新增海外订单10亿元",
  "output": "情绪倾向：正面；核心关键词：净利润增长60%、新增海外订单10亿元"
}
json
{
  "instruction": "识别以下事件的风险类型、等级及核心数据",
  "input": "公司因环保违规被当地政府罚款500万元，需停产整改1个月",
  "output": "风险类型：合规风险；风险等级：中风险；核心数据：罚款500万元、停产整改1个月"
}
（四）语料质量校验
人工抽检：随机抽取 10% 语料，校验标注准确性（情绪判断准确率≥95%，风险标注准确率≥98%）
重复率校验：通过文本相似度算法（如余弦相似度），确保语料重复率≤5%
专业性校验：邀请金融 / 法律从业者审核，确保术语使用准确，无常识性错误
（五）本地化存储与管理
存储路径：NAS（TrueNAS）创建专属目录 ./llm_corpus/，按语料类型分类存储（./financial/ ./legal/ ./sentiment/ ./structured/）
版本控制：使用 Git 管理标注规则和语料更新，记录每版语料的数量、来源、标注准确率
增量更新：每月新增最新研报、公告、法律文书语料，保持语料时效性
二、API 接口详细定义文档（基于 FastAPI）
（一）接口通用规范
基础信息：
接口根路径：http://{host}:8000/api/v1
请求方式：GET（查询）/POST（提交）
数据格式：请求参数（Query/Body）、返回结果均为 JSON
编码格式：UTF-8
认证方式：API Key（请求头携带 X-API-Key: {your_api_key}）
通用返回格式：
json
{
  "code": 200,  // 200成功，400参数错误，401未认证，500服务器错误
  "msg": "success",
  "data": { ... }  // 接口核心返回数据
}
（二）核心接口详细定义
1. 个股基本面 + 风险综合评分接口
接口路径：/stock/score
请求方式：GET
功能描述：获取单只股票的基本面评分、风险评分及明细
请求参数：
|     参数名   | 类型    | 必传 |              说明              |   示例值     |
|--------------|--------|------|-------------------------------|--------------|
| stock_code   | string | 是   | 股票代码（6 位，A 股）          | "600036"     |
| date         | string | 否   | 查询日期（YYYY-MM-DD），默认最新 | "2024-10-20" |

返回数据示例：
json
{
  "code": 200,
  "msg": "success",
  "data": {
    "stock_code": "600036",
    "stock_name": "招商银行",
    "query_date": "2024-10-20",
    "comprehensive_score": 88,  // 综合得分
    "rating": "优质",           // 评级
    "fundamental_score": {      // 基本面评分明细
      "operation": 90,          // 经营能力
      "finance": 85,            // 财务健康
      "competitiveness": 92,    // 竞争力
      "potential": 86,          // 发展潜力
      "risk_control": 88        // 风险控制
    },
    "risk_score": {             // 风险评分明细
      "legal": 90,              // 法律风险
      "compliance": 95,         // 合规风险
      "operation": 85,          // 经营风险
      "credit": 92,             // 信用风险
      "total_risk_score": 90,   // 风险总分
      "risk_level": "无明显风险" // 风险等级
    },
    "core_strengths": ["毛利率行业领先", "现金流稳定", "风控体系完善"],
    "core_shortcomings": ["研发投入占比低于行业均值"]
  }
}
2. 个股最新预警信息接口
接口路径：/stock/warning
请求方式：GET
功能描述：获取单只股票的最新预警信息（一级 / 二级 / 三级）
请求参数：| 参数名 | 类型 | 必传 | 说明 | 示例值 ||--------------|--------|------|-------------------------------|--------------|| stock_code | string | 是 | 股票代码（6 位，A 股） | "600036" || warning_level| string | 否 | 预警等级（all/1/2/3），默认 all | "all" |
返回数据示例：
json
{
  "code": 200,
  "msg": "success",
  "data": {
    "stock_code": "600036",
    "stock_name": "招商银行",
    "warning_list": [
      {
        "warning_id": "W20241020001",
        "warning_level": 2,
        "warning_type": "经营风险",
        "warning_reason": "应收账款回收率同比下滑22%，超出异动阈值",
        "related_data": {
          "recovery_rate_2024": 65%,
          "recovery_rate_2023": 87%,
          "threshold": 20%
        },
        "occur_date": "2024-10-20",
        "history_case": "2023年XX银行因应收账款回收率下滑25%，股价1个月内下跌5%",
        "suggestion": "持续跟踪应收账款回收进度，关注现金流变化"
      }
    ],
    "total_warning_count": 1
  }
}
3. 个股舆情情绪数据接口
接口路径：/stock/sentiment
请求方式：GET
功能描述：获取单只股票指定时间范围内的舆情情绪数据
请求参数：| 参数名 | 类型 | 必传 | 说明 | 示例值 ||--------------|--------|------|-------------------------------|--------------|| stock_code | string | 是 | 股票代码（6 位，A 股） | "600036" || start_date | string | 是 | 开始日期（YYYY-MM-DD） | "2024-09-20" || end_date | string | 是 | 结束日期（YYYY-MM-DD） | "2024-10-20" || source_type | string | 否 | 来源类型（all/announcement/news/social） | "all" |
返回数据示例：
json
{
  "code": 200,
  "msg": "success",
  "data": {
    "stock_code": "600036",
    "stock_name": "招商银行",
    "time_range": "2024-09-20至2024-10-20",
    "sentiment_summary": {
      "average_score": 7.2,  // 平均情绪得分（0-10）
      "positive_count": 35,  // 正面舆情数量
      "neutral_count": 52,   // 中性舆情数量
      "negative_count": 8    // 负面舆情数量
    },
    "daily_sentiment": [
      {
        "date": "2024-10-20",
        "score": 8.5,
        "positive_count": 5,
        "neutral_count": 3,
        "negative_count": 0,
        "hot_topic": "公司发布三季度业绩预告，净利润增长15%"
      }
    ],
    "hot_negative_news": []  // 热门负面舆情（无则为空）
  }
}
4. 股票二次筛选接口
接口路径：/stock/secondary-screen
请求方式：POST
功能描述：输入技术面筛选后的股票列表，返回非技术面二次筛选结果
请求参数（Body）：
json
{
  "stock_list": ["600036", "000858", "601318"],  // 待筛选股票代码列表
  "筛选条件": {
    "risk_level": "低风险及以上",  // 风险等级（无明显风险/低风险及以上/中风险及以上）
    "fundamental_rating": "良好及以上",  // 基本面评级（优质/良好及以上/一般及以上）
    "sentiment_avg_score": 6  // 近3个月舆情平均得分≥6
  }
}
返回数据示例：
json
{
  "code": 200,
  "msg": "success",
  "data": {
    "pass_list": [
      {
        "stock_code": "600036",
        "stock_name": "招商银行",
        "risk_score": 90,
        "fundamental_rating": "优质",
        "sentiment_avg_score": 7.2,
        "core_highlights": ["风险低", "基本面扎实", "舆情正面"]
      }
    ],
    "fail_list": [
      {
        "stock_code": "000858",
        "stock_name": "五粮液",
        "fail_reason": "近3个月舆情平均得分5.8，未达到筛选阈值6分"
      },
      {
        "stock_code": "601318",
        "stock_name": "中国平安",
        "fail_reason": "风险等级为中风险，未达到低风险及以上要求"
      }
    ],
    "pass_rate": 33.33%
  }
}
（三）接口部署与测试
容器化部署：构建 FastAPI Docker 镜像，配置文件挂载 NAS 目录，Docker Compose 配置如下（片段）：
yaml
version: '3'
services:
  api-service:
    image: fastapi-stock:v1.0
    container_name: stock-api
    ports:
      - "8000:8000"
    environment:
      - API_KEY=${API_KEY}
      - DB_HOST=${DB_HOST}
      - DB_PORT=${DB_PORT}
    volumes:
      - /mnt/truenas/api/config:/app/config
      - /mnt/truenas/api/logs:/app/logs
    depends_on:
      - postgres
      - redis
接口测试：使用 Swagger UI（http://{host}:8000/docs）或 Postman 测试接口，验证参数合法性、返回数据准确性、响应速度（单接口响应时间≤500ms）
三、Docker Compose 编排文件模板（完整版）
yaml
version: '3.8'

# 环境变量配置（可单独创建.env文件挂载）
env_file:
  - .env

services:
  # 1. 数据抓取服务
  crawler-service:
    build:
      context: ./crawler
      dockerfile: Dockerfile
    container_name: stock-crawler
    restart: always
    volumes:
      - /mnt/truenas/stock/data/raw:/app/raw_data
      - /mnt/truenas/stock/data/cleaned:/app/cleaned_data
      - ./crawler/config:/app/config
      - ./crawler/logs:/app/logs
    environment:
      - SPIDER_THREADS=10
      - PROXY_POOL_URL=${PROXY_POOL_URL}
      - DATA_SOURCE_API_KEY=${DATA_SOURCE_API_KEY}
    networks:
      - stock-network
    depends_on:
      - redis

  # 2. LLM服务（本地化部署）
  llm-service:
    build:
      context: ./llm
      dockerfile: Dockerfile
    container_name: stock-llm
    restart: always
    volumes:
      - /mnt/truenas/stock/llm/model:/app/model
      - /mnt/truenas/stock/llm/corpus:/app/corpus
      - ./llm/logs:/app/logs
    environment:
      - MODEL_PATH=/app/model/llama-3-70b-int4
      - GPU_MEMORY=16GB
      - MAX_BATCH_SIZE=8
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    networks:
      - stock-network
    depends_on:
      - redis

  # 3. NLP引擎服务
  nlp-service:
    build:
      context: ./nlp
      dockerfile: Dockerfile
    container_name: stock-nlp
    restart: always
    volumes:
      - ./nlp/model:/app/model
      - ./nlp/logs:/app/logs
    environment:
      - LLM_SERVICE_URL=http://llm-service:8001
      - SENTIMENT_MODEL_PATH=/app/model/textcnn-bilstm
    networks:
      - stock-network
    depends_on:
      - llm-service
      - redis

  # 4. API服务（FastAPI）
  api-service:
    build:
      context: ./api
      dockerfile: Dockerfile
    container_name: stock-api
    restart: always
    ports:
      - "8000:8000"
    volumes:
      - ./api/config:/app/config
      - ./api/logs:/app/logs
    environment:
      - API_KEY=${API_KEY}
      - DB_HOST=postgres
      - DB_PORT=5432
      - DB_USER=${DB_USER}
      - DB_PASSWORD=${DB_PASSWORD}
      - DB_NAME=${DB_NAME}
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - NLP_SERVICE_URL=http://nlp-service:8002
    networks:
      - stock-network
    depends_on:
      - postgres
      - redis
      - nlp-service

  # 5. 前端服务（Vue3+ECharts）
  frontend-service:
    build:
      context: ./frontend
      dockerfile: Dockerfile
    container_name: stock-frontend
    restart: always
    ports:
      - "80:80"
    volumes:
      - ./frontend/dist:/usr/share/nginx/html
      - ./frontend/nginx.conf:/etc/nginx/conf.d/default.conf
    networks:
      - stock-network
    depends_on:
      - api-service

  # 6. 数据库服务（PostgreSQL）
  postgres:
    image: postgres:15-alpine
    container_name: stock-postgres
    restart: always
    ports:
      - "5432:5432"
    volumes:
      - /mnt/truenas/stock/db/postgres:/var/lib/postgresql/data
      - ./db/init.sql:/docker-entrypoint-initdb.d/init.sql
    environment:
      - POSTGRES_USER=${DB_USER}
      - POSTGRES_PASSWORD=${DB_PASSWORD}
      - POSTGRES_DB=${DB_NAME}
    networks:
      - stock-network

  # 7. 缓存服务（Redis）
  redis:
    image: redis:7-alpine
    container_name: stock-redis
    restart: always
    ports:
      - "6379:6379"
    volumes:
      - /mnt/truenas/stock/db/redis:/data
    command: redis-server --requirepass ${REDIS_PASSWORD}
    networks:
      - stock-network

  # 8. 对象存储服务（MinIO，存储非结构化数据）
  minio:
    image: minio/minio
    container_name: stock-minio
    restart: always
    ports:
      - "9000:9000"
      - "9001:9001"
    volumes:
      - /mnt/truenas/stock/minio:/data
    environment:
      - MINIO_ROOT_USER=${MINIO_ACCESS_KEY}
      - MINIO_ROOT_PASSWORD=${MINIO_SECRET_KEY}
    command: server /data --console-address ":9001"
    networks:
      - stock-network

networks:
  stock-network:
    driver: bridge
.env 文件模板（需自行配置）
env
# 数据库配置
DB_USER=stock_user
DB_PASSWORD=your_strong_password
DB_NAME=stock_analysis_db
DB_HOST=postgres
DB_PORT=5432

# Redis配置
REDIS_PASSWORD=your_redis_password
REDIS_HOST=redis
REDIS_PORT=6379

# MinIO配置
MINIO_ACCESS_KEY=minio_access_key
MINIO_SECRET_KEY=minio_secret_key

# API配置
API_KEY=your_api_key_123456

# 数据源配置
DATA_SOURCE_API_KEY=your_choice_api_key
PROXY_POOL_URL=http://proxy-pool:8080

# LLM配置
MODEL_PATH=/app/model/llama-3-70b-int4



针对股票非技术面分析的法律、运营、舆情三大核心维度，我整理了对应的数据接口和获取渠道，涵盖官方权威平台、商业数据服务商、第三方 API 接口等类型，可直接适配你的 AI + 大数据股票分析项目：
一、法律层面数据获取渠道与接口
聚焦公司涉诉、处罚、合规风险等数据，分为官方免费渠道和商业 API 接口两类：
官方权威平台（免费，需爬虫 / 手动整理）
中国裁判文书网：提供上市公司涉诉判决书、裁定书，可通过网页爬虫抓取涉案金额、案件类型、判决结果等核心数据。
中国执行信息公开网：查询上市公司及实控人被执行信息，包括执行标的、执行状态。
证监会 / 交易所官网：披露上市公司监管处罚、立案调查、违规公示等信息，上交所 / 深交所官网可通过公告板块筛选 “处罚类” 公告。
国家企业信用信息公示系统：提供企业工商变更、行政处罚、经营异常名录等基础法律信息。
商业 API 接口（付费，结构化数据）
企查查 / 天眼查 API：支持查询上市公司司法涉诉、被执行人、行政处罚、股权冻结等法律风险数据，返回结构化 JSON 格式，可直接接入项目。
启信宝 API：与企查查 / 天眼查功能类似，新增 “法律风险评分” 维度，量化企业涉诉风险等级。
二、运营层面数据获取渠道与接口
覆盖公司财务、经营、产业链、机构持仓等核心运营数据，以商业金融数据终端 API为主，官方平台为辅：
商业金融数据终端 API（付费，专业级）
Wind（万得）API：提供上市公司财报、营收、毛利率、研发投入、机构持仓、大宗交易等运营数据，支持产业链关联分析，是国内金融机构主流选择。
同花顺 iFinD API：覆盖财务指标、北向资金流向、融资融券、股东人数变动等数据，接口适配 Python，适合量化分析。
东方财富 Choice API：提供上市公司经营数据、产业链数据（如原材料价格、上下游企业关联）、机构研报核心观点提取等功能。
官方平台（免费，基础数据）
巨潮资讯网：证监会指定的上市公司公告披露平台，可抓取财报、经营数据公告、并购重组等信息，需通过爬虫解析非结构化文本。
行业协会官网：如中国汽车工业协会、电子元件行业协会，提供行业运营数据（如产能、销量），关联上市公司业务占比分析。
第三方商业接口（特色数据）
数行者科技（Datago）供应链数据库：提供上市公司上下游交易数据、供应链稳定性分析，量化运营链风险。
三、舆情层面数据获取渠道与接口
涵盖新闻、社交媒体、股吧等舆情数据，分为专业舆情 API和免费爬虫渠道：
专业舆情 API 接口（付费，量化分析）
数行者科技（Datago）API：提供网络新闻 / 报刊新闻量化舆情数据，包括情感评分、传播热度、原创性指标，覆盖 A 股上市公司逾 1500 万篇新闻分析结果，支持接口调用。
谷谷数据 AI 舆情 API：提供文本情感分析、关键词提取功能，适配股吧、微博等社交媒体舆情解析。
WallstreetBets API：针对海外股票，分析 Reddit 论坛散户讨论的情感倾向，适合美股舆情分析。
免费爬虫渠道（需自研爬虫）
财经新闻平台：证券时报网、中国证券报、财新网等官网，可抓取上市公司相关新闻，通过 NLP 做情感分析。
社交媒体 / 社区：微博财经、雪球、东方财富股吧，需模拟登录 + 反爬策略抓取用户评论、讨论热度。
新闻聚合平台：新浪财经、腾讯财经 API（部分免费），提供基础新闻资讯接口，可获取上市公司相关新闻标题、摘要。
金融终端集成舆情（付费，一站式）
Wind / 东方财富 Choice：内置舆情监控模块，提供上市公司舆情情绪指数、热点舆情关联分析，支持 API 导出舆情数据。
需要我为你整理各接口的接入流程与费用对比表，方便你根据项目预算选择适配的数据源吗？

9 篇资料
那如果我使用，把刚才的这些所有的范畴，使用多 Agent，然后不同的角色设置不同的 AI 分析师，用大模型在线联网了。比如 gemini 或者 GPT，或者是其他在线联网的大模型。直接来分析，可以吧？然后你直接给我设出这些各种角色的 Agent 的提示词和相关的设置信息。
多 Agent 股票非技术面分析系统：角色设置、提示词与配置方案
基于你的需求，采用「多 Agent 分工协作」模式，将非技术面分析拆解为 6 个核心角色 AI 分析师，均适配 Gemini、GPT-4o、Claude 3 等在线联网大模型（需开启联网功能），每个 Agent 聚焦单一维度，最终汇总形成综合分析结论。以下是各 Agent 的「角色定位、核心职责、提示词模板、配置参数」：
一、通用配置要求（所有 Agent 统一设置）
联网功能：必须开启（需获取实时数据、最新公告、舆情动态）
输出格式：结构化 JSON + 核心结论（方便后续聚合分析）
数据优先级：官方来源（交易所 / 证监会公告）＞商业数据＞非官方舆情
分析原则：基于事实数据，不做主观预测，仅输出 “风险点 / 利好点 + 数据支撑”
交互机制：各 Agent 完成独立分析后，向「汇总 Agent」提交结果，支持跨 Agent 追问补充数据
二、各角色 Agent 详细设置（提示词 + 职责）
Agent 1：法律合规风险分析师
角色定位
聚焦上市公司法律涉诉、监管处罚、合规缺陷等风险，量化法律风险等级。
核心职责
联网查询目标公司最新涉诉案件（涉案金额、类型、影响范围）
核查监管处罚记录（证监会 / 交易所罚款、立案调查、违规公示）
识别合规风险点（信息披露、关联交易、内控缺陷）
输出法律风险评分（0-100 分）及核心风险清单
提示词模板
plaintext
你是资深股票法律合规风险分析师，专注A股上市公司非技术面分析，需基于联网获取的最新数据完成以下任务：

【分析目标】：{股票代码}_{股票名称}（如600036_招商银行）
【核心任务】：
1. 联网查询该公司近12个月内的法律涉诉信息：包括涉案金额、案件类型（合同纠纷/知识产权/证券虚假陈述等）、审理进度、判决结果，若涉案金额占公司最新净资产比例≥10%，标记为“高风险”。
2. 核查该公司及实控人近12个月内的监管处罚记录：包括处罚机构（证监会/交易所/地方监管局）、处罚原因（信息披露违规/内幕交易/环保违规等）、处罚金额、是否涉及立案调查。
3. 检索公司最新公告（上交所/深交所），识别合规风险点：如关联交易未披露、内控审计非标、经营异常名录等。
4. 基于上述信息，给出法律风险评分（0-100分，≤30分为高风险，31-60分为中风险，≥61分为低风险），并列出Top3核心风险点及数据支撑。

【输出要求】：
1. 必须标注数据来源（如中国裁判文书网、证监会官网、公司公告编号）。
2. 结构化输出，格式如下：
{
  "股票代码": "",
  "股票名称": "",
  "分析时间": "",
  "法律风险评分": "",
  "风险等级": "",
  "核心涉诉信息": [{"涉案金额": "", "案件类型": "", "审理进度": "", "数据来源": ""}],
  "监管处罚记录": [{"处罚机构": "", "处罚原因": "", "处罚金额": "", "数据来源": ""}],
  "合规风险点": [{"风险类型": "", "具体描述": "", "数据来源": ""}],
  "结论建议": ""
}
3. 若未查询到相关风险信息，明确标注“近12个月无重大法律合规风险”。
Agent 2：公司运营基本面分析师
角色定位
深挖公司经营、财务、竞争力核心数据，评估运营健康度与发展潜力。
核心职责
联网获取最新财报数据（营收、净利润、毛利率等核心指标）
分析经营效率（库存周转、应收账款回收、研发转化）
评估核心竞争力（市场占有率、护城河、团队稳定性）
识别经营隐性风险（单一业务依赖、供应链波动等）
提示词模板
plaintext
你是资深股票运营基本面分析师，专注A股上市公司非技术面分析，需基于联网获取的最新数据完成以下任务：

【分析目标】：{股票代码}_{股票名称}（如600036_招商银行）
【核心任务】：
1. 联网获取该公司最新一期财报（年报/半年报/季报）核心数据：营收及同比增速、净利润及同比增速、毛利率、净利率、资产负债率、经营现金流净额、研发投入占比，对比行业均值，标记偏离超30%的指标。
2. 分析经营效率指标：近3个季度库存周转天数、应收账款回收率的变动趋势，若应收账款回收率同比下滑超20%，标记为“经营风险点”。
3. 评估核心竞争力：查询公司市场占有率、核心业务占比、行业地位（如是否为行业龙头）、核心团队稳定性（近12个月高管离职率）、客户集中度（前5大客户占比）。
4. 识别经营隐性风险：单一业务/单一市场依赖度、原材料价格波动影响、政策适配性（若为强监管行业，需核查最新政策影响）。
5. 给出运营健康度评分（0-100分，≥80分为优质，60-79分为良好，＜60分为较差）及Top3核心优势/短板。

【输出要求】：
1. 必须标注数据来源（如巨潮资讯网、同花顺iFinD、行业协会报告）。
2. 结构化输出，格式如下：
{
  "股票代码": "",
  "股票名称": "",
  "分析时间": "",
  "最新财报期": "",
  "核心财务指标": [{"指标名称": "", "数值": "", "同比增速": "", "行业均值": "", "偏离度": ""}],
  "经营效率分析": [{"指标名称": "", "近3季度变动": "", "风险标记": ""}],
  "核心竞争力评估": {"市场占有率": "", "核心业务占比": "", "高管离职率": "", "客户集中度": ""},
  "经营隐性风险": [{"风险类型": "", "具体描述": "", "数据来源": ""}],
  "运营健康度评分": "",
  "核心优势/短板": [{"类型": "优势/短板", "描述": "", "数据支撑": ""}]
}
Agent 3：产业链关联分析师
角色定位
联动上下游数据，预判公司经营拐点，识别产业链传导风险 / 机遇。
核心职责
联网获取行业景气度数据（PMI、库存周期、需求增速）
分析上下游波动影响（原材料价格、下游需求变化）
关联竞争对手动态（产能扩张、价格战、新品发布）
预判业绩拐点信号（成本下降 / 上升、需求景气 / 疲软）
提示词模板
plaintext
你是资深股票产业链关联分析师，专注A股上市公司非技术面分析，需基于联网获取的最新数据完成以下任务：

【分析目标】：{股票代码}_{股票名称}（如600036_招商银行）
【核心任务】：
1. 明确公司所属行业及核心业务对应的产业链位置（如锂电池企业：上游锂矿→中游锂电池→下游新能源汽车）。
2. 联网获取行业最新景气度数据：行业PMI、库存周期（补库/去库）、需求增速、政策导向（扶持/限制）。
3. 分析上下游波动影响：上游原材料价格近3个月变动（如锂矿价格下跌10%）、下游终端需求变动（如新能源汽车销量增速），量化对公司成本/营收的影响。
4. 查询核心竞争对手动态：近3个月产能扩张、新品发布、价格调整、市场份额变动等信息，评估对目标公司的竞争冲击。
5. 基于产业链数据，预判公司业绩拐点（增长/下滑/持平），给出产业链风险/机遇清单。

【输出要求】：
1. 必须标注数据来源（如行业协会官网、Wind、同花顺产业链数据库）。
2. 结构化输出，格式如下：
{
  "股票代码": "",
  "股票名称": "",
  "分析时间": "",
  "产业链定位": "",
  "行业景气度数据": [{"指标名称": "", "数值": "", "变动趋势": "", "数据来源": ""}],
  "上下游波动影响": [{"环节": "上游/下游", "变动描述": "", "对公司影响": "", "数据支撑": ""}],
  "竞争对手动态": [{"对手名称": "", "动态描述": "", "竞争冲击评估": ""}],
  "业绩拐点预判": "",
  "产业链风险/机遇": [{"类型": "风险/机遇", "描述": "", "数据支撑": ""}]
}
Agent 4：舆情智能分析分析师
角色定位
实时抓取多渠道舆情，量化情绪倾向，识别舆情发酵风险 / 利好共振。
核心职责
联网抓取最新舆情（公司公告、财经新闻、社交媒体、股吧）
量化情绪评分（正面 / 负面 / 中性），分析传播热度
关联舆情与核心事件（业绩预增、重大合同、负面丑闻）
识别舆情发酵信号（短时间高传播、多平台扩散）
提示词模板
plaintext
你是资深股票舆情智能分析分析师，专注A股上市公司非技术面分析，需基于联网获取的最新实时数据完成以下任务：

【分析目标】：{股票代码}_{股票名称}（如600036_招商银行）
【核心任务】：
1. 联网抓取近7天内的多渠道舆情：
   - 官方渠道：公司公告（巨潮资讯网）、交易所公告；
   - 新闻渠道：证券时报、中国证券报、财新网等权威财经媒体；
   - 社交渠道：微博财经、雪球、东方财富股吧；
2. 对每条舆情进行情绪量化评分（0-10分，0为极度负面，10为极度正面），统计近7天平均情绪得分、正面/中性/负面舆情占比。
3. 提取核心舆情事件：如重大合同签订、业绩预告、减持增持、负面丑闻（如环保问题、产品质量），关联对应的情绪评分。
4. 识别舆情发酵信号：若单条负面舆情1小时内传播量超1万次，或连续3条以上负面舆情关联同一事件，标记为“舆情高风险”；若正面舆情关联“重大合同/业绩暴增”且传播量超5万次，标记为“利好共振”。
5. 给出舆情综合评分（0-100分，≥80分为正面，60-79分为中性，＜60分为负面）。

【输出要求】：
1. 必须标注舆情来源（如公司公告编号、证券时报链接、雪球帖子链接）。
2. 结构化输出，格式如下：
{
  "股票代码": "",
  "股票名称": "",
  "分析时间": "",
  "舆情时间范围": "近7天",
  "舆情情绪统计": {"平均情绪得分": "", "正面占比": "", "中性占比": "", "负面占比": ""},
  "核心舆情事件": [{"事件类型": "", "舆情内容": "", "情绪得分": "", "传播热度": "", "来源": ""}],
  "舆情发酵信号": [{"信号类型": "高风险/利好共振", "描述": "", "数据支撑": ""}],
  "舆情综合评分": "",
  "舆情影响评估": ""
}
Agent 5：资金流向关联分析师
角色定位
联动资金数据与非技术面因素，识别资金 + 基本面 / 舆情共振信号。
核心职责
联网获取最新资金数据（北向资金、机构持仓、大宗交易、融资融券）
关联资金变动与基本面 / 舆情事件（如北向增持 + 业绩预增）
分析筹码集中度（股东人数、户均持股）
识别资金潜伏 / 出逃信号（大宗交易折价率、融资余额变动）
提示词模板
plaintext
你是资深股票资金流向关联分析师，专注A股上市公司非技术面分析，需基于联网获取的最新数据完成以下任务：

【分析目标】：{股票代码}_{股票名称}（如600036_招商银行）
【核心任务】：
1. 联网获取近30天资金数据：
   - 北向资金：累计增减持数量、持仓占比变动；
   - 机构持仓：公募/私募/社保基金增减持变动；
   - 大宗交易：交易频次、成交金额、折价率（＞5%为高折价）；
   - 融资融券：融资余额变动、融券余额变动；
2. 关联资金变动与基本面/舆情事件：如北向资金增持是否对应业绩预增、机构减持是否关联负面舆情。
3. 分析筹码集中度：近3个月股东人数变动率、户均持股数变动，股东人数减少+户均持股增加标记为“筹码集中”。
4. 识别资金信号：高折价大宗交易密集→“资金出逃信号”；北向增持+融资余额上升→“资金共振利好信号”。
5. 给出资金健康度评分（0-100分，≥80分为优质，60-79分为良好，＜60分为较差）。

【输出要求】：
1. 必须标注数据来源（如同花顺iFinD、东方财富Choice、交易所资金流向公告）。
2. 结构化输出，格式如下：
{
  "股票代码": "",
  "股票名称": "",
  "分析时间": "",
  "近30天资金数据": [{"资金类型": "", "变动金额/数量": "", "持仓占比变动": "", "数据来源": ""}],
  "资金与事件关联": [{"资金变动": "", "关联事件": "", "共振类型": "利好/利空", "数据支撑": ""}],
  "筹码集中度分析": {"股东人数变动率": "", "户均持股变动": "", "筹码状态": ""},
  "资金信号识别": [{"信号类型": "", "描述": "", "数据支撑": ""}],
  "资金健康度评分": "",
  "资金层面结论": ""
}
Agent 6：综合汇总分析师（核心协调角色）
角色定位
接收其他 5 个 Agent 的分析结果，交叉验证、权重赋值，输出最终综合分析报告。
核心职责
汇总各 Agent 评分（法律风险、运营健康度、舆情、资金、产业链）
按权重计算综合评分（法律 30%、运营 25%、产业链 20%、舆情 15%、资金 10%）
交叉验证单一维度信号（如运营利好需配合资金 / 舆情共振）
输出最终筛选结论（通过 / 不通过）及核心依据
提示词模板
plaintext
你是股票非技术面分析综合汇总分析师，核心职责是接收以下5个专项Agent的分析结果，完成交叉验证、权重汇总，输出最终综合报告：

【输入数据】：
1. 法律合规风险分析师结果（含法律风险评分）；
2. 公司运营基本面分析师结果（含运营健康度评分）；
3. 产业链关联分析师结果（含业绩拐点预判）；
4. 舆情智能分析分析师结果（含舆情综合评分）；
5. 资金流向关联分析师结果（含资金健康度评分）。

【核心任务】：
1. 权重赋值计算综合评分：
   综合评分=法律风险评分×30% + 运营健康度评分×25% + 产业链适配评分（按业绩拐点预判赋值：增长=90分，持平=70分，下滑=40分）×20% + 舆情综合评分×15% + 资金健康度评分×10%；
   综合评分≥80分为“优质标的”，70-79分为“合格标的”，＜70分为“淘汰标的”。
2. 交叉验证：单一维度评分≤30分（高风险），直接标记为“淘汰标的”；若某维度利好（评分≥80分），需关联其他至少1个维度利好，否则标记为“单一利好，需谨慎”。
3. 提取核心亮点与风险：汇总各Agent的Top3核心优势/风险，去重后整理。
4. 输出最终筛选结论：通过（优质/合格）/不通过（淘汰），并明确淘汰原因（若淘汰）。

【输出要求】：
1. 结构化输出，格式如下：
{
  "股票代码": "",
  "股票名称": "",
  "分析时间": "",
  "各维度评分汇总": [{"维度": "", "评分": "", "权重": "", "加权得分": ""}],
  "综合评分": "",
  "综合评级": "优质/合格/淘汰",
  "核心亮点": [{"描述": "", "数据支撑": ""}],
  "核心风险": [{"描述": "", "数据支撑": ""}],
  "交叉验证结论": "",
  "最终筛选结果": "通过/不通过",
  "淘汰原因（若淘汰）": ""
}
2. 若某专项Agent未获取到有效数据，需标注“该维度数据缺失，按60分基准分计算加权得分”。
三、Agent 协作流程（适配你的项目）
启动触发：输入技术面筛选后的股票代码列表，依次触发 6 个 Agent；
数据流转：各专项 Agent（1-5）独立联网分析，输出结构化结果，同步至综合汇总 Agent（6）；
综合决策：Agent6 完成权重汇总与交叉验证，输出最终筛选报告；
异常处理：某专项 Agent 分析失败 / 数据缺失，按基准分计算，标记 “数据缺失” 提示补充数据源。
需要我针对某类 Agent（如舆情 / 法律）的提示词做进一步优化，或补充各 Agent 的联网数据源优先级清单吗？
好的，继续补充各 Agent 的优先级和清单，包括提示词。
多 Agent 股票分析：数据源优先级清单 + 提示词优化（适配联网抓取）
基于各 Agent 核心职责，补充 数据源优先级排序（从权威、结构化、易接入维度划分），并优化提示词（明确优先抓取渠道），确保数据获取高效、精准：
一、各 Agent 数据源优先级清单（按优先级从高到低）
Agent 1：法律合规风险分析师
数据类型	优先级 1（首选，结构化 / API）	优先级 2（备选，免费爬虫）	优先级 3（补充，兜底）	核心字段抓取要求
司法涉诉	企查查 API（付费）	中国裁判文书网（网页爬虫）	第三方法律数据平台（如聚法案例）	涉案金额、案件类型、审理进度、判决结果
监管处罚	同花顺 iFinD API（处罚模块）	证监会 / 交易所官网（公告爬虫）	巨潮资讯网（“处罚类” 公告）	处罚机构、原因、金额、立案状态
被执行人 / 冻结	天眼查 API（付费）	中国执行信息公开网（爬虫）	国家企业信用信息公示系统	执行标的、执行状态、冻结金额
合规缺陷	启信宝 API（合规评分模块）	公司年报 “内控缺陷” 章节（爬虫）	审计机构非标意见公告	内控缺陷类型、整改进度
优化后提示词（新增数据源优先级）
plaintext
你是资深股票法律合规风险分析师，专注A股上市公司非技术面分析，需按以下优先级联网获取数据，完成任务：

【分析目标】：{股票代码}_{股票名称}
【数据源优先级】：
1. 首选API/结构化数据：企查查API（涉诉/处罚）、同花顺iFinD API（监管处罚）；
2. 次选免费爬虫：中国裁判文书网、证监会/交易所官网、中国执行信息公开网；
3. 兜底补充：国家企业信用信息公示系统、公司公告。

【核心任务】：
1. 按优先级查询近12个月法律涉诉：优先调用企查查API，无权限则爬取中国裁判文书网，需获取涉案金额、案件类型、审理进度、判决结果，涉案金额占最新净资产≥10%标记“高风险”；
2. 监管处罚核查：优先同花顺iFinD API，无则爬取证监会官网，重点抓取处罚原因（如信息披露违规）、金额、是否立案；
3. 合规风险点：优先启信宝API合规评分，无则爬取公司年报内控章节，识别关联交易未披露、经营异常等问题；
4. 法律风险评分（0-100分）：按涉诉金额占比（40%）、处罚严重程度（30%）、整改进度（30%）加权计算，≤30分为高风险。

【输出要求】：
{
  "股票代码": "",
  "股票名称": "",
  "分析时间": "",
  "法律风险评分": "",
  "风险等级": "",
  "核心涉诉信息": [{"涉案金额": "", "案件类型": "", "审理进度": "", "数据来源": "", "优先级": ""}],
  "监管处罚记录": [{"处罚机构": "", "处罚原因": "", "处罚金额": "", "数据来源": "", "优先级": ""}],
  "合规风险点": [{"风险类型": "", "具体描述": "", "数据来源": "", "优先级": ""}],
  "结论建议": "",
  "数据缺失提示": ""  // 若某类数据无获取，标注缺失类型
}
Agent 2：公司运营基本面分析师
数据类型	优先级 1（首选，结构化 / API）	优先级 2（备选，免费爬虫）	优先级 3（补充，兜底）	核心字段抓取要求
核心财务指标	Wind API（财报模块）	巨潮资讯网（财报 PDF 解析）	东方财富网（财务摘要）	营收、净利润、毛利率、资产负债率、现金流
经营效率指标	同花顺 iFinD API（运营模块）	公司年报 “经营情况讨论” 章节	行业协会报告（如中国上市公司协会）	库存周转、应收账款回收率、研发投入占比
核心竞争力	Choice API（市场占有率模块）	公司年报 “核心业务” 章节	券商研报（东方财富网）	市场份额、核心业务占比、高管离职率
经营隐性风险	数行者科技 API（供应链模块）	公司公告 “风险提示” 章节	行业新闻（证券时报）	单一业务依赖度、原材料价格影响
优化后提示词（新增数据源优先级）
plaintext
你是资深股票运营基本面分析师，专注A股上市公司非技术面分析，需按以下优先级联网获取数据，完成任务：

【分析目标】：{股票代码}_{股票名称}
【数据源优先级】：
1. 首选API/结构化数据：Wind API（财务）、同花顺iFinD API（运营）、Choice API（竞争力）；
2. 次选免费爬虫：巨潮资讯网（财报）、公司年报、行业协会官网；
3. 兜底补充：东方财富网、券商研报摘要。

【核心任务】：
1. 最新财报数据：优先Wind API，无则爬取巨潮资讯网最新财报，获取营收/净利润增速、毛利率等核心指标，对比行业均值（优先Choice API行业数据），偏离超30%标注；
2. 经营效率分析：优先iFinD API库存/应收数据，无则爬取年报经营章节，近3季度变动趋势下滑超20%标记“风险点”；
3. 核心竞争力：优先Choice API市场占有率数据，无则提取年报核心业务占比、高管离职率（爬取公司公告“人事变动”）；
4. 经营隐性风险：优先数行者供应链API，无则爬取年报“风险提示”，识别单一业务依赖、政策影响等问题；
5. 运营健康度评分（0-100分）：财务健康（40%）、经营效率（30%）、竞争力（30%）加权计算，≥80分为优质。

【输出要求】：
{
  "股票代码": "",
  "股票名称": "",
  "分析时间": "",
  "最新财报期": "",
  "核心财务指标": [{"指标名称": "", "数值": "", "同比增速": "", "行业均值": "", "偏离度": "", "数据来源": "", "优先级": ""}],
  "经营效率分析": [{"指标名称": "", "近3季度变动": "", "风险标记": "", "数据来源": "", "优先级": ""}],
  "核心竞争力评估": {"市场占有率": "", "核心业务占比": "", "高管离职率": "", "客户集中度": "", "数据来源": ""},
  "经营隐性风险": [{"风险类型": "", "具体描述": "", "数据来源": "", "优先级": ""}],
  "运营健康度评分": "",
  "核心优势/短板": [{"类型": "优势/短板", "描述": "", "数据支撑": ""}],
  "数据缺失提示": ""
}
Agent 3：产业链关联分析师
数据类型	优先级 1（首选，结构化 / API）	优先级 2（备选，免费爬虫）	优先级 3（补充，兜底）	核心字段抓取要求
行业景气度	Wind API（行业 PMI 模块）	国家统计局官网（行业数据）	行业协会月报（如中国汽车工业协会）	行业 PMI、需求增速、库存周期
上下游价格	同花顺 iFinD API（产业链模块）	大宗商品交易所官网（如上海金属网）	第三方价格平台（如生意社）	原材料价格、近 3 月变动率、影响逻辑
竞争对手动态	数行者科技 API（竞品模块）	竞争对手公告（巨潮资讯网爬虫）	行业新闻（财新网）	产能扩张、新品发布、市场份额变动
政策导向	证监会 / 行业主管部门官网（API）	政府公告（爬虫）	权威媒体报道（证券日报）	政策类型（扶持 / 限制）、实施时间
优化后提示词（新增数据源优先级）
plaintext
你是资深股票产业链关联分析师，专注A股上市公司非技术面分析，需按以下优先级联网获取数据，完成任务：

【分析目标】：{股票代码}_{股票名称}
【数据源优先级】：
1. 首选API/结构化数据：Wind API（行业景气）、iFinD API（产业链价格）、数行者竞品API；
2. 次选免费爬虫：国家统计局、大宗商品交易所官网、竞争对手公告；
3. 兜底补充：行业协会报告、权威财经媒体新闻。

【核心任务】：
1. 产业链定位：明确公司核心业务上下游（如锂电池→上游锂矿、下游新能源汽车）；
2. 行业景气度：优先Wind API获取行业PMI、需求增速，无则爬取国家统计局数据，判断行业周期（补库/去库）；
3. 上下游影响：优先iFinD API原材料价格，无则爬取上海金属网，量化价格变动对公司成本/营收的影响（如锂价跌10%→成本降5%）；
4. 竞争对手动态：优先数行者API，无则爬取竞品公告，分析产能、新品对目标公司的冲击；
5. 政策与拐点：优先行业主管部门官网政策，无则爬取权威报道，预判业绩拐点（增长/下滑/持平）。

【输出要求】：
{
  "股票代码": "",
  "股票名称": "",
  "分析时间": "",
  "产业链定位": "",
  "行业景气度数据": [{"指标名称": "", "数值": "", "变动趋势": "", "数据来源": "", "优先级": ""}],
  "上下游波动影响": [{"环节": "上游/下游", "变动描述": "", "对公司影响": "", "数据支撑": "", "优先级": ""}],
  "竞争对手动态": [{"对手名称": "", "动态描述": "", "竞争冲击评估": "", "数据来源": ""}],
  "业绩拐点预判": "",
  "产业链风险/机遇": [{"类型": "风险/机遇", "描述": "", "数据支撑": "", "优先级": ""}],
  "数据缺失提示": ""
}
Agent 4：舆情智能分析分析师
数据类型	优先级 1（首选，结构化 / API）	优先级 2（备选，免费爬虫）	优先级 3（补充，兜底）	核心字段抓取要求
官方舆情	巨潮资讯网 API（公告）	巨潮资讯网（公告爬虫）	交易所官网公告板块	公告类型、核心内容、发布时间
权威新闻	数行者舆情 API（财经新闻）	证券时报 / 中国证券报官网爬虫	新浪财经新闻 API（免费版）	新闻标题、内容、情绪倾向、传播量
社交舆情	谷谷数据 AI 舆情 API（社交媒体）	雪球 / 股吧爬虫（模拟登录）	微博财经热搜爬虫	评论内容、情绪、互动量（点赞 / 转发）
舆情量化	数行者情绪评分 API	自研 NLP 模型（基于爬虫文本）	-	情绪得分（0-10 分）、传播热度
优化后提示词（新增数据源优先级）
plaintext
你是资深股票舆情智能分析分析师，专注A股上市公司非技术面分析，需按以下优先级联网获取实时数据，完成任务：

【分析目标】：{股票代码}_{股票名称}
【数据源优先级】：
1. 首选API/结构化数据：巨潮资讯网API（公告）、数行者舆情API（新闻+情绪）、谷谷数据社交API；
2. 次选免费爬虫：证券时报官网、雪球/股吧（模拟登录）、微博财经；
3. 兜底补充：新浪财经新闻、腾讯财经资讯。

【核心任务】：
1. 舆情抓取（近7天）：按优先级获取官方公告、权威新闻、社交讨论，每条舆情标注来源优先级；
2. 情绪量化：优先数行者API情绪得分，无则用自研逻辑评分（0-10分），统计平均得分、正/负/中性占比；
3. 核心事件提取：关联业绩、合同、减持等事件，匹配对应舆情情绪，如“重大合同+情绪9分”；
4. 发酵信号：负面舆情1小时传播超1万次/连续3条负面→“高风险”；正面舆情关联利好+传播超5万次→“利好共振”；
5. 舆情综合评分（0-100分）：情绪得分（60%）、传播热度（20%）、事件重要性（20%）加权计算。

【输出要求】：
{
  "股票代码": "",
  "股票名称": "",
  "分析时间": "",
  "舆情时间范围": "近7天",
  "舆情情绪统计": {"平均情绪得分": "", "正面占比": "", "中性占比": "", "负面占比": ""},
  "核心舆情事件": [{"事件类型": "", "舆情内容": "", "情绪得分": "", "传播热度": "", "来源": "", "优先级": ""}],
  "舆情发酵信号": [{"信号类型": "高风险/利好共振", "描述": "", "数据支撑": "", "来源优先级": ""}],
  "舆情综合评分": "",
  "舆情影响评估": "",
  "数据缺失提示": ""
}
Agent 5：资金流向关联分析师
数据类型	优先级 1（首选，结构化 / API）	优先级 2（备选，免费爬虫）	优先级 3（补充，兜底）	核心字段抓取要求
北向资金	Wind API（北向资金模块）	东方财富网（北向资金爬虫）	同花顺北向资金实时接口	增减持数量、持仓占比、变动金额
机构持仓	Choice API（机构持仓模块）	基金季报（巨潮资讯网爬虫）	上市公司 “机构调研” 公告	公募 / 私募增减持、持仓变动比例
大宗交易	iFinD API（大宗交易模块）	交易所大宗交易平台爬虫	东方财富大宗交易板块	成交金额、折价率、买卖方
融资融券	Wind API（两融模块）	同花顺两融数据爬虫	交易所两融余额公告	融资 / 融券余额、变动率
筹码集中度	Choice API（股东模块）	公司公告 “股东人数” 章节	东方财富股东结构板块	股东人数变动率、户均持股数
优化后提示词（新增数据源优先级）
plaintext
你是资深股票资金流向关联分析师，专注A股上市公司非技术面分析，需按以下优先级联网获取数据，完成任务：

【分析目标】：{股票代码}_{股票名称}
【数据源优先级】：
1. 首选API/结构化数据：Wind API（北向/两融）、Choice API（机构/筹码）、iFinD API（大宗交易）；
2. 次选免费爬虫：东方财富网、交易所大宗平台、公司股东公告；
3. 兜底补充：同花顺数据中心、券商APP数据摘要。

【核心任务】：
1. 资金数据（近30天）：按优先级获取北向、机构、大宗、两融数据，标注来源优先级；
2. 关联分析：优先匹配基本面/舆情事件（如北向增持+业绩预增→共振利好），无事件则单独分析资金趋势；
3. 筹码集中度：优先Choice API股东数据，无则爬取公司公告，股东人数减少+户均增加→“筹码集中”；
4. 资金信号：高折价大宗密集→“出逃”；北向+融资双增持→“共振利好”；
5. 资金健康度评分（0-100分）：北向/机构资金（40%）、大宗交易（20%）、两融（20%）、筹码（20%）加权计算。

【输出要求】：
{
  "股票代码": "",
  "股票名称": "",
  "分析时间": "",
  "近30天资金数据": [{"资金类型": "", "变动金额/数量": "", "持仓占比变动": "", "数据来源": "", "优先级": ""}],
  "资金与事件关联": [{"资金变动": "", "关联事件": "", "共振类型": "利好/利空", "数据支撑": "", "事件来源": ""}],
  "筹码集中度分析": {"股东人数变动率": "", "户均持股变动": "", "筹码状态": "", "数据来源": ""},
  "资金信号识别": [{"信号类型": "", "描述": "", "数据支撑": "", "来源优先级": ""}],
  "资金健康度评分": "",
  "资金层面结论": "",
  "数据缺失提示": ""
}
Agent 6：综合汇总分析师（数据源优先级适配）
优化后提示词（新增数据优先级权重调整）
plaintext
你是股票非技术面分析综合汇总分析师，核心职责是接收以下5个专项Agent的分析结果，按数据源优先级加权调整，完成交叉验证与汇总：

【输入数据】：
1. 法律合规风险分析师结果（含数据来源优先级）；
2. 公司运营基本面分析师结果（含数据来源优先级）；
3. 产业链关联分析师结果（含数据来源优先级）；
4. 舆情智能分析分析师结果（含数据来源优先级）；
5. 资金流向关联分析师结果（含数据来源优先级）。

【核心任务】：
1. 权重与优先级调整：
   - 基础权重：法律30%、运营25%、产业链20%、舆情15%、资金10%；
   - 优先级调整：数据源为“优先级1”（API）的维度，权重上浮5%；“优先级3”（兜底）的维度，权重下浮5%；数据缺失按60分基准分计算；
   - 综合评分=（法律评分×调整后权重）+...+（资金评分×调整后权重），≥80优质、70-79合格、＜70淘汰。
2. 交叉验证：优先级1数据源的利好/风险信号，权重高于低优先级；单一维度高风险（≤30分）直接淘汰；
3. 核心信息提取：汇总各Agent Top3亮点/风险，优先保留高优先级数据源的信息。

【输出要求】：
{
  "股票代码": "",
  "股票名称": "",
  "分析时间": "",
  "各维度评分汇总": [{"维度": "", "原始评分": "", "数据源优先级占比": "", "调整后权重": "", "加权得分": ""}],
  "综合评分": "",
  "综合评级": "优质/合格/淘汰",
  "核心亮点": [{"描述": "", "数据支撑": "", "数据源优先级": ""}],
  "核心风险": [{"描述": "", "数据支撑": "", "数据源优先级": ""}],
  "交叉验证结论": "",
  "最终筛选结果": "通过/不通过",
  "淘汰原因（若淘汰）": "",
  "数据优化建议": ""  // 针对低优先级/缺失数据，给出补充建议
}
二、核心补充说明
优先级适配逻辑：优先级 1（API）数据结构化、接入效率高，适合 AI 自动解析；优先级 2（免费爬虫）需自研反爬策略，适合补充基础数据；优先级 3（兜底）用于数据缺失时的应急补充。
成本平衡建议：核心维度（如财务、法律）优先选择付费 API（企查查、Wind），降低爬虫开发成本；非核心维度（如社交舆情）用免费爬虫兜底。
数据一致性校验：不同优先级数据源抓取到同一指标（如营收）时，优先以优先级 1 数据为准，交叉验证偏差超 10% 时标记 “数据冲突”，提示人工核查。



股票非技术面分析数据渠道综合汇总表
为便于你决策数据渠道的选择，以下表格从渠道类型（API / 网页爬虫 / 公开平台）、覆盖分析维度、全面性（★代表覆盖度，5★为最高）、稳定性（★代表稳定性，5★为最高）、收费模式、核心资源六大维度，汇总所有涉及渠道的关键信息：
渠道分类	具体渠道名称	覆盖分析维度	全面性	稳定性	收费模式	核心资源
付费 API 类	企查查 API	法律合规（涉诉 / 处罚）	★★★★☆	★★★★★	按次计费（如企业模糊搜索 0.1 元 / 次）、套餐包（1000 元 / 10000 次）、企业户年付	企业涉诉、被执行人、股权冻结、行政处罚等法律数据
同花顺 iFinD API	运营基本面 / 资金 / 产业链	★★★★★	★★★★★	基础数据包 9800 元 / 年，API 调用基础费 800 元 / 月（百万次请求），企业旗舰套餐 38000 元 / 年	财报核心指标、产业链价格、北向资金、大宗交易数据
Wind API	运营基本面 / 产业链 / 资金	★★★★★	★★★★☆	个人版年费 2-4 万元，机构版 5-20 万元 / 终端，API 按模块定制收费	行业 PMI、财报明细、融资融券、ESG 数据、高频交易数据
启信宝 API	法律合规 / 运营基本面	★★★★☆	★★★★★	按次计费、场景包年（根据需求定制年费）	企业工商信息、内控缺陷、合规评分、经营异常名录
Choice API	运营基本面 / 资金	★★★★☆	★★★★★	年费约 1-3 万元，量化研究版 5 万元以上	市场占有率、机构持仓、股东人数、筹码集中度数据
数行者科技 API	产业链 / 舆情 / 运营基本面	★★★★☆	★★★★☆	按模块订阅（竞品数据 / 供应链数据单独计费）	竞争对手动态、供应链波动、原材料价格、财经新闻舆情
谷谷数据 AI 舆情 API	舆情分析	★★★★☆	★★★★☆	按调用量计费（社交舆情数据万次调用约 50-100 元）	雪球 / 股吧 / 微博情绪评分、传播热度、互动量统计
数行者舆情 API	舆情分析	★★★★★	★★★★★	按年订阅（基础舆情模块约 5000 元 / 年）	权威财经新闻、情绪量化、事件关联分析
免费爬虫类	中国裁判文书网	法律合规（涉诉）	★★★☆☆	★★★☆☆	免费（需自研反爬策略）	企业司法涉诉案件详情、判决结果、涉案金额
证监会 / 交易所官网	法律合规 / 资金 / 运营基本面	★★★★☆	★★★★★	免费（部分公告需爬虫解析）	监管处罚公告、公司年报 / 季报、大宗交易公示、北向资金明细
巨潮资讯网	运营基本面 / 舆情（官方公告）	★★★★★	★★★★★	免费（API 需企业认证，网页爬虫免费）	上市公司公告、财报 PDF、信息披露文件、业绩预告
国家统计局官网	产业链（行业景气）	★★★★☆	★★★★★	免费（公开数据下载）	行业 PMI、需求增速、宏观经济数据、库存周期统计
上海金属网	产业链（原材料价格）	★★★☆☆	★★★★☆	免费（部分价格需注册后查看）	大宗商品价格、近 3 月变动率、行业供需分析
证券时报 / 中国证券报	舆情 / 产业链	★★★★☆	★★★★★	免费（网页爬虫）	权威财经新闻、行业政策、公司动态、产业链事件
雪球 / 东方财富股吧	舆情（社交）	★★★★☆	★★★☆☆	免费（需模拟登录反爬）	投资者讨论、情绪倾向、热门话题、个股互动量
微博财经	舆情（社交）	★★★☆☆	★★★★☆	免费（热搜 / 评论爬虫）	财经热搜、个股相关话题、用户情绪、传播趋势
公开兜底类	国家企业信用信息公示系统	法律合规（经营异常）	★★★☆☆	★★★★★	免费（单条查询）	企业经营异常名录、行政处罚、股权质押、年报简版
行业协会官网	产业链（行业数据）	★★★☆☆	★★★★★	免费（月报 / 年报下载）	行业产销数据、景气度报告、政策解读、企业排名
新浪财经 / 腾讯财经	舆情 / 资金	★★★☆☆	★★★★☆	免费（资讯聚合）	财经新闻摘要、北向资金简版、个股舆情热点
券商研报平台	运营基本面 / 产业链	★★★☆☆	★★★★☆	免费（部分研报需付费）	公司分析报告、行业趋势预判、核心竞争力评估
表格使用建议
成本优先：法律合规维度可先用「中国裁判文书网 + 证监会官网」免费爬虫兜底，核心数据再补充企查查 API；舆情维度优先爬取证券时报、雪球等免费渠道，情绪量化用自研逻辑替代付费舆情 API。
稳定性优先：运营基本面、资金流向等核心维度，建议选择 Wind/iFinD/Choice 等付费 API，数据结构化且更新及时，减少爬虫维护成本。
综合方案：付费 API（Wind/iFinD）+ 免费爬虫（巨潮资讯网 / 裁判文书网）+ 公开平台（行业协会）组合，既保证核心数据精准，又控制整体成本。



你帮我详细的设计一套情绪化，情绪量化，自研了，然后舆情，是，舆情功能。对于说，有情，情绪量量化来来代替付费舆情，然后把这个给我详细的展开设计一下。
自研舆情情绪量化功能详细设计方案（替代付费 API）
一、核心目标
基于免费爬虫获取的非结构化舆情文本（新闻、公告、股吧评论等），实现情绪量化评分（0-10 分，0 极度负面、10 极度正面） 、情绪分类（正面 / 中性 / 负面） 、核心事件关联，完全替代付费舆情 API 的核心功能，适配股票非技术面分析场景。
二、整体架构（适配本地化部署 + Docker 容器）
模块	核心功能	技术选型	数据流转逻辑
文本预处理模块	清洗、分词、特征提取	Python+Jieba/LTP、NLTK	原始舆情文本→清洗→分词→特征向量
情绪词典构建模块	金融领域专属情绪词典 + 权重标注	人工标注 + Word2Vec 预训练	通用词典→金融场景扩展→权重校准
情绪量化模型模块	规则引擎 + 机器学习模型双驱动	朴素贝叶斯、TextCNN、LightGBM	特征向量→规则匹配→模型预测→情绪得分
事件关联模块	舆情与股票事件绑定（如业绩预增）	关键词匹配 + 正则表达式	情绪文本→事件关键词提取→关联事件库
结果输出模块	结构化数据输出 + 异常校验	FastAPI+JSON	情绪得分→事件关联→结构化结果→接口输出
三、各模块详细设计
（一）文本预处理模块（基础：保证输入数据质量）
1. 数据来源（适配免费爬虫）
输入文本类型：证券时报新闻、巨潮资讯网公告、雪球 / 股吧评论、微博财经话题
文本格式：纯文本（.txt）、HTML 解析后文本、JSON 字段文本
2. 预处理流程（分步执行）
步骤	处理逻辑	工具 / 代码示例	核心目标
去噪清洗	1. 删除 HTML 标签、乱码、特殊符号（如 @#￥）；2. 过滤广告、无意义灌水（如 “哈哈哈”“沙发”）；3. 剔除重复文本（基于文本哈希去重）	Python+re 正则表达式：
text = re.sub(r'<.*?>', '', html_text)
text = re.sub(r'[^\u4e00-\u9fa5a-zA-Z0-9]', ' ', text)	保留有效文本，减少噪音干扰
分词与词性标注	1. 金融专业词汇优先分词（如 “计提减值”“北向资金”）；2. 剔除停用词（如 “的”“了”“在”）；3. 标注词性（名词 / 动词 / 形容词）	Jieba 分词 + 自定义金融词典：
jieba.load_userdict('finance_dict.txt')
words = jieba.cut(text, cut_all=False)	提取核心词汇，适配金融场景
特征提取	1. 提取 TF-IDF 特征（词频 - 逆文档频率）；2. 提取 Word2Vec 词向量（基于金融语料预训练）；3. 构建文本特征矩阵	Python+TfidfVectorizer：
tfidf = TfidfVectorizer(max_features=5000)
X = tfidf.fit_transform(words_list)	将文本转化为模型可识别的向量
（二）金融领域情绪词典构建模块（核心：保证情绪识别精准度）
1. 词典构成（三层结构，适配股票场景）
词典类型	核心内容	权重标注规则（-5~+5 分，负为负面，正为正面）	示例词汇
核心情绪词典	直接表达情绪的词汇	程度越强，权重绝对值越大	正面：暴涨（+5）、业绩暴增（+4）、重大合同（+3）；
负面：暴跌（-5）、违规被罚（-4）、涉诉（-3）；
中性：公告（0）、披露（0）
修饰词词典	强化 / 弱化情绪的副词、形容词	正面修饰词 + 1~+2 分，负面修饰词 - 1~-2 分	正面修饰：大幅（+1）、超预期（+2）；
负面修饰：大幅（-1）、未达标（-2）
金融专属词典	股票领域专业术语（无直接情绪，辅助判断）	权重 0 分，用于事件关联	北向资金、毛利率、研发投入、大宗交易
2. 词典构建与更新流程
基础词典来源：知网 Hownet 情感词典、BosonNLP 情感词典，筛选金融相关词汇；
扩展补充：爬取券商研报、上市公司公告，提取高频金融情绪词汇（如 “计提坏账”“产能扩张”）；
人工标注：邀请金融从业者对词汇权重进行校准（如 “立案调查” 权重从 - 3 调整为 - 5）；
增量更新：每月爬取最新舆情文本，提取新增词汇（如 “注册制改革”“AI 算力”），补充到词典中；
存储格式：Excel/JSON，便于代码调用，示例：
json
{
  "核心情绪词典": {
    "暴涨": 5,
    "业绩暴增": 4,
    "违规被罚": -4,
    "涉诉": -3
  },
  "修饰词词典": {
    "大幅": 1,
    "超预期": 2,
    "未达标": -2
  }
}
（三）情绪量化模型模块（双驱动：规则引擎保证基础准确率，机器学习提升精度）
1. 规则引擎（基础层，准确率≥80%）
核心逻辑：基于情绪词典，通过 “词汇匹配 + 权重累加 + 规则修正” 计算初始情绪得分
计算步骤	逻辑说明	公式 / 示例
词汇权重累加	遍历分词结果，匹配情绪词典，累加所有词汇权重	示例文本：“公司业绩暴增 50%，大幅超预期”
权重：业绩暴增（+4）+ 大幅（+1）+ 超预期（+2）= +7
修饰词修正	修饰词权重与被修饰词权重相乘（强化情绪）	示例：“公司业绩小幅增长”
权重：小幅（+0.5）× 增长（+2）= +1
否定词修正	否定词（如 “不”“未”“无”）后接情绪词，权重取反	示例：“公司业绩未增长”
权重：未（否定）× 增长（+2）= -2
初始得分归一化	将累加权重（-∞~+∞）归一化为 0-10 分	归一化公式：
score = (weight + 5) / 10 × 10（权重范围 - 5~+5，对应 0-10 分）
2. 机器学习模型（优化层，提升准确率至≥90%）
核心逻辑：基于标注好的舆情文本数据集，训练模型修正规则引擎得分，适配复杂语境
步骤	详细设计	工具 / 代码示例
标注数据集构建	1. 从免费爬虫文本中随机抽取 10000 条；2. 人工标注情绪得分（0-10 分）；3. 按 7:3 拆分训练集 / 测试集	标注格式：
文本：“公司涉诉金额5000万”，得分：3分
文本：“新增海外订单10亿”，得分：8分
模型选择与训练	1. 基础模型：朴素贝叶斯（快速训练，适配小数据集）；2. 优化模型：TextCNN（捕捉文本局部特征）；3. 最终模型：LightGBM（融合规则得分 + TF-IDF 特征，提升精度）	Python+LightGBM：
model = lgb.LGBMRegressor(n_estimators=100, learning_rate=0.1)
model.fit(X_train, y_train, eval_set=(X_test, y_test))
模型融合	规则引擎得分（权重 0.6）+ 模型预测得分（权重 0.4），得到最终情绪得分	最终得分 = 规则得分 ×0.6 + 模型得分 ×0.4
模型迭代	每月新增标注 1000 条文本，重新训练模型，更新模型参数	构建模型版本管理机制，保留历史最优模型
3. 情绪分类规则（基于最终得分）
情绪类别	得分范围	判定标准	示例文本
正面	7~10 分	包含利好事件（业绩增长、重大合同等），情绪积极	“公司 2024 年净利润同比增长 60%，新增订单 5 亿”
中性	3~6 分	无明显利好 / 利空，以客观陈述为主	“公司发布 2024 年一季度财报，营收 10 亿元”
负面	0~2 分	包含利空事件（违规、涉诉、业绩下滑等），情绪消极	“公司因信息披露违规被证监会罚款 300 万元”
（四）事件关联模块（将情绪与股票核心事件绑定，提升分析价值）
1. 核心事件库构建（覆盖股票非技术面关键事件）
事件类别	关键词列表	情绪倾向	关联维度
业绩相关	净利润、营收、增长、下滑、暴增、亏损	增长（正面）、下滑（负面）	运营基本面
法律合规相关	违规、被罚、涉诉、立案、执行、冻结	负面	法律风险
经营相关	重大合同、产能扩张、研发突破、裁员	合同 / 突破（正面）、裁员（负面）	运营基本面、产业链
资金相关	增持、减持、回购、北向资金、大宗交易	增持 / 回购（正面）、减持（负面）	资金流向
政策相关	扶持、补贴、限制、监管、改革	扶持（正面）、限制（负面）	产业链
2. 事件关联逻辑
关键词匹配：遍历预处理后的文本，匹配事件库中的关键词，提取核心事件（如匹配 “净利润增长 60%”→ 业绩增长事件）；
事件属性提取：提取事件关键信息（如业绩增长的幅度、涉诉的金额、合同的金额）；
情绪与事件绑定：将最终情绪得分与提取的事件关联，形成 “事件 + 情绪” 组合（如 “业绩增长 60%+ 情绪得分 8.5 分”）；
代码示例（Python + 正则）：
python
运行
# 业绩增长事件匹配
performance_pattern = re.compile(r'净利润(增长|上升|暴增)(\d+%)')
match = performance_pattern.search(text)
if match:
    event = "业绩增长"
    detail = f"净利润{match.group(1)}{match.group(2)}"
    event_sentiment = {"event": event, "detail": detail, "sentiment_score": final_score}
（五）结果输出模块（结构化输出，适配多 Agent 调用）
1. 输出格式（JSON，与之前 Agent 接口兼容）
json
{
  "text_id": "舆情文本唯一ID",
  "stock_code": "600036",
  "stock_name": "招商银行",
  "text_content": "公司2024年一季度净利润同比增长50%，新增海外订单10亿元",
  "source": "证券时报（免费爬虫）",
  "publish_time": "2024-10-20 09:30:00",
  "sentiment_score": 8.5,  // 最终情绪得分（0-10分）
  "sentiment_category": "正面",  // 情绪分类
  "event_relation": {  // 事件关联结果
    "event_type": "业绩增长",
    "event_detail": "净利润同比增长50%",
    "event_sentiment": "正面"
  },
  "confidence": 0.92,  // 情绪判断置信度（0-1）
  "process_time": "0.5s"
}
2. 接口部署（FastAPI，适配 Docker）
接口路径：/api/v1/sentiment/quantify
请求方式：POST
请求参数（Body）：
json
{
  "stock_code": "600036",
  "stock_name": "招商银行",
  "text_list": [
    {"text_id": "1", "content": "公司净利润增长50%", "source": "证券时报", "publish_time": "2024-10-20"},
    {"text_id": "2", "content": "公司涉诉金额1000万", "source": "中国裁判文书网", "publish_time": "2024-10-19"}
  ]
}
响应结果：返回每条文本的情绪量化结果，格式如上。
（六）性能优化与部署适配
1. 性能优化（保证批量处理效率）
批量处理：支持一次输入 1000 条文本，单条处理时间≤0.5s，批量处理时间≤30s；
缓存机制：使用 Redis 缓存高频词汇的分词结果、情绪权重，减少重复计算；
模型量化：将 LightGBM 模型量化为 INT8 格式，降低内存占用，提升推理速度。
2. Docker 容器化部署（适配 NAS 环境）
Dockerfile 示例：
dockerfile
FROM python:3.9-slim
WORKDIR /app
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt
COPY . .
EXPOSE 8001
CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8001"]
依赖包（requirements.txt）：
plaintext
jieba==0.42.1
scikit-learn==1.2.2
lightgbm==3.3.5
fastapi==0.104.1
uvicorn==0.24.0
redis==5.0.1
numpy==1.24.3
pandas==2.0.3
四、落地里程碑与效果验证
阶段	核心任务	时间周期	关键产出	效果验证标准
第一阶段	文本预处理模块开发 + 情绪词典构建	1-2 周	预处理代码、金融情绪词典（≥5000 词）	文本去噪率≥90%，分词准确率≥95%
第二阶段	规则引擎开发 + 标注数据集构建	2 周	规则引擎代码、标注数据集（10000 条）	规则引擎情绪判断准确率≥80%
第三阶段	机器学习模型训练 + 模型融合	1-2 周	融合模型、模型参数文件	最终情绪判断准确率≥90%，置信度≥0.8
第四阶段	事件关联模块开发 + 接口部署	1 周	事件关联代码、FastAPI 接口	事件提取准确率≥85%，接口响应时间≤0.5s
第五阶段	性能优化 + Docker 部署 + 联调	1 周	Docker 镜像、部署文档	批量处理 1000 条文本≤30s，适配多 Agent 调用
五、核心优势（对比付费舆情 API）
成本优势：基于免费开源工具开发，无调用费、年费，仅需投入开发时间；
适配性强：完全自定义金融词典和事件库，精准匹配股票非技术面分析需求；
可扩展性：支持新增舆情来源、扩展事件类型，灵活适配项目迭代；
本地化部署：数据不对外传输，保障数据隐私和自主控制（契合你的本地化部署需求）。




金融舆情情绪词典构建脚本（含自动扩展 + 人工校准）
以下脚本实现 通用情绪词典扩展→金融词汇筛选→权重自动初始化→人工校准 全流程，最终生成适配股票舆情分析的结构化情绪词典（JSON/Excel 格式），可直接对接后续情绪量化模块。
一、脚本核心功能
基于通用情感词典（Hownet/BosonNLP）自动扩展金融领域词汇；
爬取券商研报、上市公司公告，提取高频金融情绪词汇；
自动初始化词汇权重（-5~+5 分），生成人工校准模板；
校准后输出标准化情绪词典（支持 JSON/Excel 双格式）。
二、环境依赖
txt
# requirements.txt
jieba==0.42.1
requests==2.31.0
beautifulsoup4==4.12.2
pandas==2.0.3
numpy==1.24.3
scikit-learn==1.2.2
lxml==4.9.3
安装命令：pip install -r requirements.txt
三、完整脚本（分模块实现）
1. 基础配置与工具函数
python
运行
import json
import pandas as pd
import jieba
import re
from collections import Counter
import requests
from bs4 import BeautifulSoup
import os

# ---------------------- 配置参数 ----------------------
# 输出路径（适配NAS存储，可修改为你的目录）
OUTPUT_DIR = "/mnt/truenas/stock/nlp/emotion_dict"
os.makedirs(OUTPUT_DIR, exist_ok=True)

# 通用情感词典路径（需提前下载，文末附下载链接）
HOWNET_DICT_PATH = "./dict/hownet情感词典.txt"
BOSON_DICT_PATH = "./dict/bosonNLP情感词典.txt"

# 金融文本爬取配置（券商研报/公告URL模板）
RESEARCH_REPORT_URL = "https://www.eastmoney.com/report/list_1_1.html"  # 东方财富研报列表
ANNOUNCEMENT_URL = "https://www.cninfo.com.cn/new/disclosure/stock?stockCode={}&announcementTime=&pageNum={}"  # 巨潮资讯网公告

# 金融领域种子词汇（用于扩展）
FINANCE_SEED_WORDS = [
    "净利润", "营收", "毛利率", "计提减值", "涉诉", "违规", "被罚", "增持", "减持",
    "回购", "大宗交易", "北向资金", "研发投入", "产能扩张", "重大合同", "立案调查"
]

# ---------------------- 工具函数 ----------------------
def load_stopwords():
    """加载停用词（过滤无意义词汇）"""
    stopwords = set()
    # 免费停用词表：https://github.com/goto456/stopwords/blob/master/cn_stopwords.txt
    with open("./dict/cn_stopwords.txt", "r", encoding="utf-8") as f:
        for line in f:
            stopwords.add(line.strip())
    return stopwords

def clean_text(text):
    """文本清洗：去除特殊符号、乱码、无意义内容"""
    text = re.sub(r'<.*?>', '', text)  # 去除HTML标签
    text = re.sub(r'[^\u4e00-\u9fa5a-zA-Z0-9]', ' ', text)  # 保留中文、英文、数字
    text = re.sub(r'\s+', ' ', text).strip()  # 去除多余空格
    # 过滤灌水内容（长度<5或无金融词汇）
    if len(text) < 5 or not any(word in text for word in FINANCE_SEED_WORDS):
        return ""
    return text

def crawl_finance_texts(stock_codes=["600036", "000858"], report_pages=5, announcement_pages=5):
    """爬取金融文本（券商研报+上市公司公告），用于提取高频词汇"""
    finance_texts = []
    stopwords = load_stopwords()

    # 1. 爬取券商研报摘要
    print("开始爬取券商研报...")
    for page in range(1, report_pages+1):
        try:
            url = RESEARCH_REPORT_URL.replace("pageNum={}", str(page))
            response = requests.get(url, headers={"User-Agent": "Mozilla/5.0"})
            response.encoding = "utf-8"
            soup = BeautifulSoup(response.text, "lxml")
            # 提取研报摘要（根据网页结构调整选择器）
            report_summaries = soup.select(".report-summary")
            for summary in report_summaries:
                text = clean_text(summary.get_text())
                if text:
                    finance_texts.append(text)
        except Exception as e:
            print(f"研报第{page}页爬取失败：{e}")

    # 2. 爬取上市公司公告文本
    print("开始爬取上市公司公告...")
    for stock_code in stock_codes:
        for page in range(1, announcement_pages+1):
            try:
                url = ANNOUNCEMENT_URL.format(stock_code, page)
                response = requests.get(url, headers={"User-Agent": "Mozilla/5.0"})
                response.encoding = "utf-8"
                soup = BeautifulSoup(response.text, "lxml")
                # 提取公告链接
                announcement_links = soup.select(".announcement-link")
                for link in announcement_links:
                    ann_url = "https://www.cninfo.com.cn" + link.get("href")
                    # 爬取公告详情
                    ann_response = requests.get(ann_url, headers={"User-Agent": "Mozilla/5.0"})
                    ann_response.encoding = "utf-8"
                    ann_soup = BeautifulSoup(ann_response.text, "lxml")
                    ann_text = clean_text(ann_soup.select_one(".detail-content").get_text())
                    if ann_text:
                        finance_texts.append(ann_text)
            except Exception as e:
                print(f"股票{stock_code}公告第{page}页爬取失败：{e}")

    # 保存爬取的金融文本（用于后续复用）
    with open(f"{OUTPUT_DIR}/finance_texts.txt", "w", encoding="utf-8") as f:
        f.write("\n".join(finance_texts))
    print(f"共爬取金融文本{len(finance_texts)}条，已保存至{OUTPUT_DIR}/finance_texts.txt")
    return finance_texts
2. 通用情绪词典扩展与金融词汇筛选
python
运行
def expand_finance_dict(finance_texts):
    """基于通用情感词典，扩展金融领域情绪词典"""
    # 1. 加载通用情感词典
    print("加载通用情感词典...")
    general_positive = set()  # 通用正面词汇
    general_negative = set()  # 通用负面词汇

    # 加载Hownet情感词典（格式：词汇 词性 情感倾向）
    with open(HOWNET_DICT_PATH, "r", encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if not line:
                continue
            word, pos, sentiment = line.split("\t")[:3]
            if sentiment == "正面":
                general_positive.add(word)
            elif sentiment == "负面":
                general_negative.add(word)

    # 加载BosonNLP情感词典（格式：词汇 权重，正数正面，负数负面）
    with open(BOSON_DICT_PATH, "r", encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if not line:
                continue
            word, weight = line.split()
            weight = float(weight)
            if weight > 0:
                general_positive.add(word)
            elif weight < 0:
                general_negative.add(word)

    # 2. 从金融文本中提取高频词汇（结合jieba分词）
    print("提取金融高频词汇...")
    jieba.load_userdict("./dict/finance_seed_dict.txt")  # 加载金融种子词典
    stopwords = load_stopwords()
    finance_word_counter = Counter()

    for text in finance_texts:
        words = jieba.cut(text, cut_all=False)
        for word in words:
            # 筛选条件：非停用词、长度≥2、包含金融种子词汇或在通用情感词典中
            if (len(word) >= 2 
                and word not in stopwords 
                and (any(seed in word for seed in FINANCE_SEED_WORDS) or word in general_positive or word in general_negative)):
                finance_word_counter[word] += 1

    # 3. 筛选高频金融词汇（出现次数≥3）
    finance_high_freq_words = [word for word, count in finance_word_counter.most_common() if count >= 3]
    print(f"共筛选出金融高频词汇{len(finance_high_freq_words)}个")

    # 4. 分类：金融正面/负面/中性词汇（基于通用词典映射）
    finance_positive = [word for word in finance_high_freq_words if word in general_positive]
    finance_negative = [word for word in finance_high_freq_words if word in general_negative]
    # 中性词汇：金融专业术语（无情感倾向，辅助事件关联）
    finance_neutral = [word for word in finance_high_freq_words 
                       if word not in general_positive and word not in general_negative]

    print(f"金融正面词汇：{len(finance_positive)}个")
    print(f"金融负面词汇：{len(finance_negative)}个")
    print(f"金融中性词汇：{len(finance_neutral)}个")

    return finance_positive, finance_negative, finance_neutral
3. 权重自动初始化与人工校准模板生成
python
运行
def init_weight_and_calibrate(finance_positive, finance_negative, finance_neutral):
    """自动初始化词汇权重，生成人工校准模板"""
    # ---------------------- 权重初始化规则 ----------------------
    # 正面词汇：基础权重1~5分（根据词汇情感强度）
    # 负面词汇：基础权重-1~-5分（根据词汇情感强度）
    # 强度判断：包含“暴增”“暴涨”“重大”等修饰词→权重+2；包含“小幅”“略有”等→权重-1
    strong_pos_modifiers = ["暴增", "暴涨", "重大", "超预期", "大幅", "突破"]  # 强正面修饰词
    weak_pos_modifiers = ["小幅", "略有", "温和"]  # 弱正面修饰词
    strong_neg_modifiers = ["暴跌", "巨亏", "重大", "严重", "大幅", "立案"]  # 强负面修饰词
    weak_neg_modifiers = ["小幅", "略有", "轻微"]  # 弱负面修饰词

    # 1. 初始化正面词汇权重
    positive_dict = {}
    for word in finance_positive:
        weight = 3  # 基础权重
        # 强修饰词：权重+2
        if any(mod in word for mod in strong_pos_modifiers):
            weight = min(weight + 2, 5)
        # 弱修饰词：权重-1
        elif any(mod in word for mod in weak_pos_modifiers):
            weight = max(weight - 1, 1)
        positive_dict[word] = weight

    # 2. 初始化负面词汇权重
    negative_dict = {}
    for word in finance_negative:
        weight = -3  # 基础权重
        # 强修饰词：权重-2
        if any(mod in word for mod in strong_neg_modifiers):
            weight = max(weight - 2, -5)
        # 弱修饰词：权重+1
        elif any(mod in word for mod in weak_neg_modifiers):
            weight = min(weight + 1, -1)
        negative_dict[word] = weight

    # 3. 中性词汇权重（0分）
    neutral_dict = {word: 0 for word in finance_neutral}

    # 4. 生成人工校准模板（Excel格式，便于手动调整）
    print("生成人工校准模板...")
    calibrate_data = []
    # 正面词汇
    for word, weight in positive_dict.items():
        calibrate_data.append({"词汇": word, "初始权重": weight, "情绪倾向": "正面", "人工校准权重": "", "备注": ""})
    # 负面词汇
    for word, weight in negative_dict.items():
        calibrate_data.append({"词汇": word, "初始权重": weight, "情绪倾向": "负面", "人工校准权重": "", "备注": ""})
    # 中性词汇
    for word, weight in neutral_dict.items():
        calibrate_data.append({"词汇": word, "初始权重": weight, "情绪倾向": "中性", "人工校准权重": "", "备注": ""})

    # 保存校准模板
    calibrate_df = pd.DataFrame(calibrate_data)
    calibrate_df.to_excel(f"{OUTPUT_DIR}/finance_emotion_dict_calibrate.xlsx", index=False)
    print(f"人工校准模板已保存至：{OUTPUT_DIR}/finance_emotion_dict_calibrate.xlsx")

    return positive_dict, negative_dict, neutral_dict
4. 校准后词典生成（JSON/Excel）
python
运行
def generate_final_dict(calibrate_excel_path=None):
    """基于人工校准后的Excel，生成最终标准化情绪词典"""
    # 若未提供校准后的Excel，使用初始化权重
    if not calibrate_excel_path or not os.path.exists(calibrate_excel_path):
        print("未找到校准后的Excel，使用初始化权重生成词典...")
        # 加载之前初始化的权重（需确保前面函数已执行）
        try:
            with open(f"{OUTPUT_DIR}/init_dict.json", "r", encoding="utf-8") as f:
                init_data = json.load(f)
            positive_dict = init_data["positive"]
            negative_dict = init_data["negative"]
            neutral_dict = init_data["neutral"]
        except:
            print("请先执行前面的词典初始化流程！")
            return
    else:
        # 从校准后的Excel中读取权重
        print("从校准后的Excel加载权重...")
        calibrate_df = pd.read_excel(calibrate_excel_path)
        positive_dict = {}
        negative_dict = {}
        neutral_dict = {}

        for _, row in calibrate_df.iterrows():
            word = row["词汇"]
            # 优先使用人工校准权重，无则用初始权重
            weight = row["人工校准权重"] if pd.notna(row["人工校准权重"]) else row["初始权重"]
            weight = int(weight)  # 确保权重为整数
            sentiment = row["情绪倾向"]

            if sentiment == "正面":
                positive_dict[word] = weight
            elif sentiment == "负面":
                negative_dict[word] = weight
            elif sentiment == "中性":
                neutral_dict[word] = weight

    # 1. 生成JSON格式词典（用于代码调用）
    final_dict = {
        "核心情绪词典": {**positive_dict, **negative_dict},
        "金融中性词典": neutral_dict,
        "修饰词词典": {
            "正面修饰": {"大幅": 1, "超预期": 2, "重大": 2, "小幅": 0.5},
            "负面修饰": {"大幅": -1, "严重": -2, "重大": -2, "小幅": -0.5},
            "否定词": ["不", "未", "无", "非", "没有"]
        }
    }

    with open(f"{OUTPUT_DIR}/finance_emotion_dict.json", "w", encoding="utf-8") as f:
        json.dump(final_dict, f, ensure_ascii=False, indent=2)
    print(f"JSON格式情绪词典已保存至：{OUTPUT_DIR}/finance_emotion_dict.json")

    # 2. 生成Excel格式词典（用于查看和更新）
    excel_data = []
    for word, weight in final_dict["核心情绪词典"].items():
        sentiment = "正面" if weight > 0 else "负面"
        excel_data.append({"词汇类型": "核心情绪", "词汇": word, "权重": weight, "情绪倾向": sentiment})
    for word, weight in final_dict["金融中性词典"].items():
        excel_data.append({"词汇类型": "金融中性", "词汇": word, "权重": weight, "情绪倾向": "中性"})
    # 修饰词
    for mod_type, mods in final_dict["修饰词词典"].items():
        if mod_type in ["正面修饰", "负面修饰"]:
            for mod, weight in mods.items():
                excel_data.append({"词汇类型": mod_type, "词汇": mod, "权重": weight, "情绪倾向": "修饰词"})
        else:  # 否定词
            for mod in mods:
                excel_data.append({"词汇类型": mod_type, "词汇": mod, "权重": 0, "情绪倾向": "否定词"})

    excel_df = pd.DataFrame(excel_data)
    excel_df.to_excel(f"{OUTPUT_DIR}/finance_emotion_dict_final.xlsx", index=False)
    print(f"Excel格式情绪词典已保存至：{OUTPUT_DIR}/finance_emotion_dict_final.xlsx")

    return final_dict
5. 主函数（串联全流程）
python
运行
def main():
    # 步骤1：爬取金融文本（若已爬取，可跳过，直接加载本地文件）
    if os.path.exists(f"{OUTPUT_DIR}/finance_texts.txt"):
        print("加载本地金融文本...")
        with open(f"{OUTPUT_DIR}/finance_texts.txt", "r", encoding="utf-8") as f:
            finance_texts = [line.strip() for line in f if line.strip()]
    else:
        finance_texts = crawl_finance_texts(stock_codes=["600036", "000858", "601318"], report_pages=5, announcement_pages=5)

    # 步骤2：扩展金融情绪词典
    finance_positive, finance_negative, finance_neutral = expand_finance_dict(finance_texts)

    # 步骤3：初始化权重并生成校准模板
    positive_dict, negative_dict, neutral_dict = init_weight_and_calibrate(finance_positive, finance_negative, finance_neutral)

    # 保存初始化权重（用于后续生成最终词典）
    init_data = {
        "positive": positive_dict,
        "negative": negative_dict,
        "neutral": neutral_dict
    }
    with open(f"{OUTPUT_DIR}/init_dict.json", "w", encoding="utf-8") as f:
        json.dump(init_data, f, ensure_ascii=False, indent=2)

    # 步骤4：生成最终词典（此处先使用初始化权重，人工校准后可重新执行）
    # 人工校准后，将calibrate_excel_path改为校准后的文件路径
    generate_final_dict(calibrate_excel_path=None)

    print("="*50)
    print("金融情绪词典构建完成！")
    print(f"输出文件目录：{OUTPUT_DIR}")
    print("1. finance_texts.txt：爬取的金融文本")
    print("2. finance_emotion_dict_calibrate.xlsx：人工校准模板")
    print("3. finance_emotion_dict.json：最终JSON词典（代码调用）")
    print("4. finance_emotion_dict_final.xlsx：最终Excel词典（查看更新）")

if __name__ == "__main__":
    main()
四、使用说明
1. 前置准备
下载通用情感词典：
Hownet 情感词典：https://download.csdn.net/download/weixin_42204569/12267636
BosonNLP 情感词典：https://download.csdn.net/download/qq_34356800/10604746
中文停用词表：https://github.com/goto456/stopwords/blob/master/cn_stopwords.txt
在脚本同级目录创建./dict文件夹，将上述词典放入其中；
（可选）创建finance_seed_dict.txt，添加金融种子词汇（脚本中已提供，可补充）。
2. 执行流程
首次执行：python finance_emotion_dict_builder.py，生成爬取文本、校准模板、初始词典；
人工校准：打开finance_emotion_dict_calibrate.xlsx，根据金融场景调整权重（如 “立案调查” 从 - 3 改为 - 5）；
生成最终词典：修改main()函数中generate_final_dict(calibrate_excel_path="path/to/校准后的Excel")，重新执行脚本。
3. 词典调用示例（对接情绪量化模块）
python
运行
# 加载生成的JSON词典
def load_finance_emotion_dict(dict_path):
    with open(dict_path, "r", encoding="utf-8") as f:
        return json.load(f)

# 使用示例
emotion_dict = load_finance_emotion_dict("/mnt/truenas/stock/nlp/emotion_dict/finance_emotion_dict.json")
core_emotion = emotion_dict["核心情绪词典"]
modifiers = emotion_dict["修饰词词典"]

# 匹配词汇权重
word = "业绩暴增"
if word in core_emotion:
    print(f"词汇：{word}，权重：{core_emotion[word]}")  # 输出：词汇：业绩暴增，权重：5
五、扩展与维护
词汇更新：每月重新执行crawl_finance_texts()，爬取最新研报 / 公告，扩展词典；
权重优化：根据情绪量化模块的准确率，持续调整词典权重（如某词汇误判率高，修正权重）；
领域适配：若需适配特定行业（如新能源、医药），在FINANCE_SEED_WORDS中添加行业专属词汇。




金融舆情情绪词典评估方法 + 规则引擎调用代码（全流程适配）
以下内容包含 情绪词典评估方法（确保词典精准度）、规则引擎完整调用代码（对接词典实现情绪量化）、效果验证脚本，形成从词典到量化的闭环，直接适配你的自研舆情系统。
一、情绪词典评估方法（基于标注数据，量化准确率）
核心逻辑：通过人工标注的舆情文本数据集，计算词典匹配的情绪判断准确率、召回率、F1 值，识别误判词汇并优化词典。
1. 评估数据集构建
需提前准备「标注数据集」（格式：文本 + 真实情绪得分 + 真实情绪类别），示例如下（sentiment_annotated_data.csv）：
文本 ID	股票代码	文本内容	真实得分（0-10）	真实类别
1	600036	公司 2024 年净利润同比增长 60%，新增订单 5 亿	8.5	正面
2	600036	公司因信息披露违规被证监会罚款 300 万元	2.0	负面
3	600036	公司发布 2024 年一季度财报，营收 10 亿元	5.0	中性
2. 核心评估指标
指标	定义	计算公式		
准确率（P）	词典判断正确的文本数 / 词典判断的总文本数	P = TP / (TP + FP)		
召回率（R）	词典判断正确的文本数 / 真实标注的目标类别文本数（如正面 / 负面）	R = TP / (TP + FN)		
F1 值	准确率与召回率的调和平均（综合评估指标）	F1 = 2×P×R / (P+R)		
得分误差	词典预测得分与真实得分的平均绝对误差（MAE），越小越精准	MAE = Σ	预测得分 - 真实得分	/ 样本总数
3. 评估脚本（完整代码）
python
运行
import json
import pandas as pd
import jieba
import re
import numpy as np
from sklearn.metrics import classification_report, confusion_matrix

# ---------------------- 配置参数 ----------------------
EMOTION_DICT_PATH = "/mnt/truenas/stock/nlp/emotion_dict/finance_emotion_dict.json"  # 生成的情绪词典
ANNOTATED_DATA_PATH = "./sentiment_annotated_data.csv"  # 人工标注数据集
OUTPUT_EVAL_PATH = "/mnt/truenas/stock/nlp/eval_result"
import os
os.makedirs(OUTPUT_EVAL_PATH, exist_ok=True)

# ---------------------- 工具函数 ----------------------
def load_emotion_dict(dict_path):
    """加载情绪词典"""
    with open(dict_path, "r", encoding="utf-8") as f:
        return json.load(f)

def clean_and_cut_text(text):
    """文本清洗+分词（与词典构建脚本一致）"""
    # 清洗
    text = re.sub(r'<.*?>', '', text)
    text = re.sub(r'[^\u4e00-\u9fa5a-zA-Z0-9]', ' ', text)
    text = re.sub(r'\s+', ' ', text).strip()
    # 分词（加载金融词典）
    jieba.load_userdict("/mnt/truenas/stock/nlp/emotion_dict/finance_emotion_dict_final.xlsx")
    words = jieba.cut(text, cut_all=False)
    # 过滤停用词
    stopwords = load_stopwords()
    return [word for word in words if len(word)>=2 and word not in stopwords]

def load_stopwords():
    """加载停用词"""
    stopwords = set()
    with open("./dict/cn_stopwords.txt", "r", encoding="utf-8") as f:
        for line in f:
            stopwords.add(line.strip())
    return stopwords

def calculate_sentiment_score(text, emotion_dict):
    """基于词典计算情绪得分（规则引擎核心逻辑，后续详细展开）"""
    words = clean_and_cut_text(text)
    core_emotion = emotion_dict["核心情绪词典"]
    modifiers = emotion_dict["修饰词词典"]
    neg_words = modifiers["否定词"]
    
    total_weight = 0
    neg_flag = False  # 否定词标记（否定词后接情绪词，权重取反）
    
    for i, word in enumerate(words):
        # 检测否定词（当前词是否为否定词，且下一个词为情绪词）
        if word in neg_words and (i+1 < len(words)) and words[i+1] in core_emotion:
            neg_flag = True
            continue
        
        # 匹配核心情绪词
        if word in core_emotion:
            weight = core_emotion[word]
            # 否定词修正
            if neg_flag:
                weight = -weight
                neg_flag = False  # 重置否定标记
            # 修饰词修正（匹配前一个词是否为修饰词）
            if i > 0 and words[i-1] in modifiers["正面修饰"]:
                weight *= modifiers["正面修饰"][words[i-1]]
            elif i > 0 and words[i-1] in modifiers["负面修饰"]:
                weight *= modifiers["负面修饰"][words[i-1]]
            total_weight += weight
    
    # 归一化到0-10分（权重范围-5~+5，对应0-10分）
    if total_weight > 5:
        total_weight = 5
    elif total_weight < -5:
        total_weight = -5
    sentiment_score = (total_weight + 5) / 10 * 10  # 归一化公式
    
    # 情绪分类
    if sentiment_score >=7:
        sentiment_category = "正面"
    elif sentiment_score <=3:
        sentiment_category = "负面"
    else:
        sentiment_category = "中性"
    
    return sentiment_score, sentiment_category

# ---------------------- 核心评估函数 ----------------------
def evaluate_emotion_dict():
    # 1. 加载数据
    emotion_dict = load_emotion_dict(EMOTION_DICT_PATH)
    annotated_df = pd.read_csv(ANNOTATED_DATA_PATH)
    print(f"评估数据集规模：{len(annotated_df)}条")
    
    # 2. 批量计算预测结果
    predicted_scores = []
    predicted_categories = []
    true_scores = annotated_df["真实得分（0-10）"].tolist()
    true_categories = annotated_df["真实类别"].tolist()
    
    for _, row in annotated_df.iterrows():
        text = row["文本内容"]
        pred_score, pred_cat = calculate_sentiment_score(text, emotion_dict)
        predicted_scores.append(pred_score)
        predicted_categories.append(pred_cat)
    
    # 3. 计算评估指标
    # 3.1 分类指标（准确率、召回率、F1）
    print("\n" + "="*50)
    print("情绪分类评估指标（正面/中性/负面）：")
    print(classification_report(true_categories, predicted_categories, target_names=["正面", "中性", "负面"]))
    
    # 3.2 混淆矩阵（查看误判分布）
    conf_matrix = confusion_matrix(true_categories, predicted_categories, labels=["正面", "中性", "负面"])
    conf_df = pd.DataFrame(conf_matrix, columns=["预测正面", "预测中性", "预测负面"], index=["真实正面", "真实中性", "真实负面"])
    print("\n混淆矩阵：")
    print(conf_df)
    
    # 3.3 得分误差（MAE）
    mae = np.mean(np.abs(np.array(predicted_scores) - np.array(true_scores)))
    print(f"\n得分平均绝对误差（MAE）：{mae:.2f}（越小越精准，目标<1.0）")
    
    # 4. 识别误判样本（用于词典优化）
    annotated_df["预测得分"] = predicted_scores
    annotated_df["预测类别"] = predicted_categories
    misclassified_df = annotated_df[annotated_df["真实类别"] != annotated_df["预测类别"]]
    print(f"\n误判样本数：{len(misclassified_df)}条（占比：{len(misclassified_df)/len(annotated_df)*100:.2f}%）")
    
    # 5. 保存评估结果
    # 保存评估指标
    eval_metrics = {
        "分类报告": classification_report(true_categories, predicted_categories, target_names=["正面", "中性", "负面"], output_dict=True),
        "MAE": mae,
        "误判率": f"{len(misclassified_df)/len(annotated_df)*100:.2f}%"
    }
    with open(f"{OUTPUT_EVAL_PATH}/eval_metrics.json", "w", encoding="utf-8") as f:
        json.dump(eval_metrics, f, ensure_ascii=False, indent=2)
    
    # 保存混淆矩阵
    conf_df.to_excel(f"{OUTPUT_EVAL_PATH}/confusion_matrix.xlsx")
    
    # 保存误判样本（用于词典优化）
    misclassified_df.to_excel(f"{OUTPUT_EVAL_PATH}/misclassified_samples.xlsx", index=False)
    
    print("\n" + "="*50)
    print("评估完成！输出文件：")
    print(f"1. {OUTPUT_EVAL_PATH}/eval_metrics.json：评估指标")
    print(f"2. {OUTPUT_EVAL_PATH}/confusion_matrix.xlsx：混淆矩阵")
    print(f"3. {OUTPUT_EVAL_PATH}/misclassified_samples.xlsx：误判样本（用于词典优化）")
    
    # 6. 词典优化建议
    print("\n词典优化建议：")
    if mae > 1.0:
        print("- 得分误差较大，建议检查高频误判词汇的权重（如查看误判样本中反复出现的词汇）")
    if len(misclassified_df) > len(annotated_df)*0.2:
        print("- 误判率超过20%，建议扩展情绪词典（补充未匹配的金融情绪词汇）")

if __name__ == "__main__":
    evaluate_emotion_dict()
4. 词典优化流程（基于评估结果）
查看misclassified_samples.xlsx，统计高频误判词汇（如 “计提减值” 被误判为中性，实际为负面）；
调整词典权重：打开finance_emotion_dict_final.xlsx，修正误判词汇的权重（如 “计提减值” 从 0 改为 - 4）；
补充词汇：若误判原因是词典未包含该词汇（如 “AI 算力突破”），添加到核心情绪词典并标注权重（+5）；
重新执行评估脚本，直到 MAE<1.0、误判率 < 20%。
二、情绪词典调用代码（规则引擎完整实现）
以下代码是情绪量化模块的核心，基于生成的金融情绪词典，实现文本情绪得分计算、分类、事件关联，可直接集成到你的舆情系统中。
1. 规则引擎完整代码（sentiment_quantify_engine.py）
python
运行
import json
import jieba
import re
import pandas as pd
from datetime import datetime

# ---------------------- 配置加载 ----------------------
class SentimentQuantifyEngine:
    def __init__(self, emotion_dict_path, stopwords_path="./dict/cn_stopwords.txt"):
        """初始化：加载词典、停用词、事件库"""
        # 1. 加载情绪词典
        self.emotion_dict = self._load_emotion_dict(emotion_dict_path)
        self.core_emotion = self.emotion_dict["核心情绪词典"]
        self.positive_modifiers = self.emotion_dict["修饰词词典"]["正面修饰"]
        self.negative_modifiers = self.emotion_dict["修饰词词典"]["负面修饰"]
        self.neg_words = self.emotion_dict["修饰词词典"]["否定词"]
        
        # 2. 加载停用词
        self.stopwords = self._load_stopwords(stopwords_path)
        
        # 3. 加载股票核心事件库（用于事件关联）
        self.event_lib = self._load_event_lib()
        
        # 4. 初始化jieba分词（加载金融词典）
        jieba.load_userdict("/mnt/truenas/stock/nlp/emotion_dict/finance_emotion_dict_final.xlsx")
    
    def _load_emotion_dict(self, dict_path):
        """加载情绪词典"""
        try:
            with open(dict_path, "r", encoding="utf-8") as f:
                return json.load(f)
        except Exception as e:
            raise Exception(f"情绪词典加载失败：{e}")
    
    def _load_stopwords(self, stopwords_path):
        """加载停用词"""
        stopwords = set()
        try:
            with open(stopwords_path, "r", encoding="utf-8") as f:
                for line in f:
                    stopwords.add(line.strip())
            return stopwords
        except Exception as e:
            raise Exception(f"停用词加载失败：{e}")
    
    def _load_event_lib(self):
        """加载股票核心事件库（可扩展）"""
        return {
            "业绩相关": {
                "keywords": ["净利润", "营收", "增长", "下滑", "暴增", "亏损", "盈利"],
                "sentiment_map": {"增长": "正面", "下滑": "负面", "暴增": "正面", "亏损": "负面"}
            },
            "法律合规相关": {
                "keywords": ["违规", "被罚", "涉诉", "立案", "执行", "冻结", "处罚"],
                "sentiment_map": {"违规": "负面", "被罚": "负面", "涉诉": "负面", "立案": "负面"}
            },
            "经营相关": {
                "keywords": ["重大合同", "产能扩张", "研发突破", "裁员", "停产", "新品"],
                "sentiment_map": {"重大合同": "正面", "研发突破": "正面", "裁员": "负面", "停产": "负面"}
            },
            "资金相关": {
                "keywords": ["增持", "减持", "回购", "北向资金", "大宗交易", "质押"],
                "sentiment_map": {"增持": "正面", "回购": "正面", "减持": "负面", "质押": "负面"}
            },
            "政策相关": {
                "keywords": ["扶持", "补贴", "限制", "监管", "改革", "政策"],
                "sentiment_map": {"扶持": "正面", "补贴": "正面", "限制": "负面", "监管": "负面"}
            }
        }
    
    # ---------------------- 核心处理函数 ----------------------
    def _clean_and_cut_text(self, text):
        """文本清洗+分词"""
        # 1. 清洗
        text = re.sub(r'<.*?>', '', text)  # 去除HTML标签
        text = re.sub(r'[^\u4e00-\u9fa5a-zA-Z0-9]', ' ', text)  # 保留有效字符
        text = re.sub(r'\s+', ' ', text).strip()  # 去重空格
        if len(text) < 5:
            return []
        
        # 2. 分词+过滤停用词
        words = jieba.cut(text, cut_all=False)
        return [word for word in words if len(word)>=2 and word not in self.stopwords]
    
    def _match_event(self, text, words):
        """事件关联：匹配股票核心事件"""
        matched_event = {
            "event_type": "无关联事件",
            "event_detail": "",
            "event_sentiment": "中性"
        }
        
        for event_type, event_info in self.event_lib.items():
            keywords = event_info["keywords"]
            # 匹配事件关键词
            matched_keywords = [kw for kw in keywords if kw in text]
            if not matched_keywords:
                continue
            
            # 提取事件详情（基于正则）
            event_detail = ""
            for kw in matched_keywords:
                # 匹配“关键词+数值”（如“净利润增长50%”）
                pattern = re.compile(f'{kw}([^，。！？；]{0,10})')
                match = pattern.search(text)
                if match:
                    event_detail = f"{kw}{match.group(1)}"
                    break
            
            # 匹配事件情绪
            event_sentiment = "中性"
            for kw in matched_keywords:
                if kw in event_info["sentiment_map"]:
                    event_sentiment = event_info["sentiment_map"][kw]
                    break
            
            matched_event = {
                "event_type": event_type,
                "event_detail": event_detail if event_detail else kw,
                "event_sentiment": event_sentiment
            }
            break  # 匹配到第一个事件即可（可扩展为多事件）
        
        return matched_event
    
    def calculate_sentiment(self, stock_code, stock_name, text, source, publish_time=None):
        """
        核心函数：计算单条文本的情绪得分
        :param stock_code: 股票代码
        :param stock_name: 股票名称
        :param text: 舆情文本内容
        :param source: 舆情来源（如“证券时报”）
        :param publish_time: 发布时间（格式：YYYY-MM-DD HH:MM:SS）
        :return: 结构化情绪量化结果
        """
        if not publish_time:
            publish_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        
        # 1. 文本预处理
        words = self._clean_and_cut_text(text)
        if not words:
            return {
                "text_id": f"{stock_code}_{datetime.now().strftime('%Y%m%d%H%M%S%f')}",
                "stock_code": stock_code,
                "stock_name": stock_name,
                "text_content": text,
                "source": source,
                "publish_time": publish_time,
                "sentiment_score": 5.0,
                "sentiment_category": "中性",
                "event_relation": {"event_type": "无关联事件", "event_detail": "", "event_sentiment": "中性"},
                "confidence": 0.5,
                "process_time": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            }
        
        # 2. 权重计算（核心逻辑）
        total_weight = 0
        neg_flag = False  # 否定词标记
        matched_emotion_words = []  # 匹配到的情绪词（用于计算置信度）
        
        for i, word in enumerate(words):
            # 检测否定词（下一个词为情绪词时标记）
            if word in self.neg_words and (i+1 < len(words)) and words[i+1] in self.core_emotion:
                neg_flag = True
                continue
            
            # 匹配核心情绪词并计算权重
            if word in self.core_emotion:
                weight = self.core_emotion[word]
                matched_emotion_words.append(word)
                
                # 否定词修正
                if neg_flag:
                    weight = -weight
                    neg_flag = False
                
                # 修饰词修正（前一个词为修饰词）
                if i > 0:
                    prev_word = words[i-1]
                    if prev_word in self.positive_modifiers:
                        weight *= self.positive_modifiers[prev_word]
                    elif prev_word in self.negative_modifiers:
                        weight *= self.negative_modifiers[prev_word]
                
                total_weight += weight
        
        # 3. 得分归一化（0-10分）
        total_weight = max(min(total_weight, 5), -5)  # 限制权重范围
        sentiment_score = round((total_weight + 5) / 10 * 10, 1)  # 保留1位小数
        
        # 4. 情绪分类
        if sentiment_score >=7:
            sentiment_category = "正面"
        elif sentiment_score <=3:
            sentiment_category = "负面"
        else:
            sentiment_category = "中性"
        
        # 5. 事件关联
        event_relation = self._match_event(text, words)
        
        # 6. 置信度计算（匹配到的情绪词越多，置信度越高）
        confidence = min(0.5 + len(matched_emotion_words) * 0.1, 1.0)  # 0.5~1.0
        
        # 7. 返回结构化结果
        return {
            "text_id": f"{stock_code}_{datetime.now().strftime('%Y%m%d%H%M%S%f')}",
            "stock_code": stock_code,
            "stock_name": stock_name,
            "text_content": text,
            "source": source,
            "publish_time": publish_time,
            "sentiment_score": sentiment_score,
            "sentiment_category": sentiment_category,
            "event_relation": event_relation,
            "confidence": round(confidence, 2),
            "process_time": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        }
    
    def batch_calculate_sentiment(self, text_list):
        """
        批量计算情绪得分
        :param text_list: 文本列表，格式：
            [
                {"stock_code": "600036", "stock_name": "招商银行", "text": "xxx", "source": "xxx", "publish_time": "xxx"},
                ...
            ]
        :return: 批量结果列表
        """
        results = []
        for item in text_list:
            result = self.calculate_sentiment(
                stock_code=item["stock_code"],
                stock_name=item["stock_name"],
                text=item["text"],
                source=item["source"],
                publish_time=item.get("publish_time")
            )
            results.append(result)
        return results

# ---------------------- 调用示例 ----------------------
if __name__ == "__main__":
    # 初始化引擎
    engine = SentimentQuantifyEngine(
        emotion_dict_path="/mnt/truenas/stock/nlp/emotion_dict/finance_emotion_dict.json"
    )
    
    # 1. 单条文本调用
    single_text = {
        "stock_code": "600036",
        "stock_name": "招商银行",
        "text": "公司2024年一季度净利润同比增长50%，新增海外订单10亿元",
        "source": "证券时报",
        "publish_time": "2024-10-20 09:30:00"
    }
    single_result = engine.calculate_sentiment(**single_text)
    print("单条文本情绪量化结果：")
    print(json.dumps(single_result, ensure_ascii=False, indent=2))
    
    # 2. 批量文本调用
    batch_texts = [
        {
            "stock_code": "600036",
            "stock_name": "招商银行",
            "text": "公司因信息披露违规被证监会罚款300万元",
            "source": "证监会官网",
            "publish_time": "2024-10-19 14:00:00"
        },
        {
            "stock_code": "600036",
            "stock_name": "招商银行",
            "text": "公司发布2024年一季度财报，营收10亿元",
            "source": "巨潮资讯网",
            "publish_time": "2024-10-20 10:00:00"
        }
    ]
    batch_results = engine.batch_calculate_sentiment(batch_texts)
    print("\n批量文本情绪量化结果：")
    print(json.dumps(batch_results, ensure_ascii=False, indent=2))
    
    # 3. 保存批量结果到Excel
    batch_df = pd.DataFrame(batch_results)
    batch_df.to_excel("./sentiment_batch_result.xlsx", index=False)
    print("\n批量结果已保存至：sentiment_batch_result.xlsx")
2. 集成到多 Agent 系统的调用代码
将规则引擎封装为 API 接口（基于 FastAPI），适配多 Agent 的舆情分析师调用：
python
运行
# sentiment_api.py
from fastapi import FastAPI, HTTPException
import uvicorn
import json
from sentiment_quantify_engine import SentimentQuantifyEngine

app = FastAPI(title="金融舆情情绪量化API")

# 初始化情绪量化引擎
engine = SentimentQuantifyEngine(
    emotion_dict_path="/mnt/truenas/stock/nlp/emotion_dict/finance_emotion_dict.json"
)

# 单条文本情绪量化接口
@app.post("/api/v1/sentiment/single")
async def single_sentiment(data: dict):
    try:
        # 校验参数
        required_params = ["stock_code", "stock_name", "text", "source"]
        for param in required_params:
            if param not in data:
                raise HTTPException(status_code=400, detail=f"缺少参数：{param}")
        
        result = engine.calculate_sentiment(
            stock_code=data["stock_code"],
            stock_name=data["stock_name"],
            text=data["text"],
            source=data["source"],
            publish_time=data.get("publish_time")
        )
        return {"code": 200, "msg": "success", "data": result}
    except Exception as e:
        return {"code": 500, "msg": f"处理失败：{str(e)}", "data": None}

# 批量文本情绪量化接口
@app.post("/api/v1/sentiment/batch")
async def batch_sentiment(data: dict):
    try:
        if "text_list" not in data or not isinstance(data["text_list"], list):
            raise HTTPException(status_code=400, detail="参数错误：text_list应为列表")
        
        results = engine.batch_calculate_sentiment(data["text_list"])
        return {"code": 200, "msg": "success", "data": {"results": results, "total": len(results)}}
    except Exception as e:
        return {"code": 500, "msg": f"处理失败：{str(e)}", "data": None}

if __name__ == "__main__":
    # 启动API服务（适配Docker部署）
    uvicorn.run(app, host="0.0.0.0", port=8001)
三、全流程闭环总结
词典构建：通过脚本生成金融专属情绪词典；
词典评估：基于标注数据计算准确率、MAE，优化词典；
情绪量化：通过规则引擎实现文本情绪得分计算；
接口集成：封装为 API，对接多 Agent 舆情分析师；
迭代优化：基于实际使用中的误判样本，持续更新词典和规则。