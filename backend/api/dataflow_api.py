"""
æ•°æ®æµç›‘æ§API
æä¾›è‚¡ç¥¨æ•°æ®æµç›‘æ§ã€æ–°é—»èˆ†æƒ…åˆ†æã€é£é™©é¢„è­¦ç­‰åŠŸèƒ½
"""

from fastapi import APIRouter, HTTPException, BackgroundTasks
from pydantic import BaseModel, Field
from typing import List, Dict, Any, Optional
from datetime import datetime, timedelta
import asyncio
import os

from backend.utils.logging_config import get_logger
from backend.utils.tool_logging import log_api_call
import math
from datetime import date as date_type


def sanitize_for_json(obj):
    """
    é€’å½’æ¸…ç†æ•°æ®ä¸­æ— æ³•JSONåºåˆ—åŒ–çš„å€¼ï¼š
    - inf, -inf, nan -> None
    - date, datetime -> ISOæ ¼å¼å­—ç¬¦ä¸²
    """
    if isinstance(obj, dict):
        return {k: sanitize_for_json(v) for k, v in obj.items()}
    elif isinstance(obj, list):
        return [sanitize_for_json(item) for item in obj]
    elif isinstance(obj, float):
        if math.isnan(obj) or math.isinf(obj):
            return None
        return obj
    elif isinstance(obj, (datetime, date_type)):
        return obj.isoformat()
    else:
        return obj


# ä¿ç•™æ—§å‡½æ•°åä½œä¸ºåˆ«åï¼Œä¿æŒå‘åå…¼å®¹
sanitize_float_values = sanitize_for_json

# å¯¼å…¥é£é™©ç›‘æ§æ¨¡å—
from backend.dataflows.risk import (
    check_suspend_status,
    is_st_stock,
    get_stock_realtime_quote,
    analyze_stock_risk
)

# å¯¼å…¥æ–°é—»å’Œæƒ…ç»ªåˆ†ææ¨¡å—
from backend.dataflows.news.multi_source_news_aggregator import get_news_aggregator
from backend.dataflows.news.sentiment_engine import get_sentiment_engine

# å¯¼å…¥ç»¼åˆæ•°æ®æœåŠ¡
from backend.dataflows.comprehensive_stock_data import get_comprehensive_service

logger = get_logger("api.dataflow")
router = APIRouter(prefix="/api/dataflow", tags=["Data Flow"])


# ==================== æ•°æ®æ¨¡å‹ ====================

class MonitorStockRequest(BaseModel):
    """æ·»åŠ ç›‘æ§è‚¡ç¥¨è¯·æ±‚"""
    code: str = Field(..., description="è‚¡ç¥¨ä»£ç ï¼Œå¦‚600519.SH")
    frequency: str = Field("1h", description="æ›´æ–°é¢‘ç‡ï¼š5m/15m/30m/1h/1d")
    items: Dict[str, bool] = Field(
        default_factory=lambda: {
            "news": True,
            "risk": True,
            "sentiment": True,
            "suspend": False
        },
        description="ç›‘æ§é¡¹ç›®"
    )


class RemoveMonitorRequest(BaseModel):
    """ç§»é™¤ç›‘æ§è¯·æ±‚"""
    code: str = Field(..., description="è‚¡ç¥¨ä»£ç ")


class UpdateMonitorRequest(BaseModel):
    """ç«‹å³æ›´æ–°è¯·æ±‚"""
    code: str = Field(..., description="è‚¡ç¥¨ä»£ç ")


# ==================== å…¨å±€çŠ¶æ€ ====================

# å¯¼å…¥æ•°æ®åº“æœåŠ¡
from backend.database.database import get_db_context
from backend.database.services import (
    MonitoredStockService,
    StockDataService,
    StockNewsService,
    DataFlowStatsService
)

# ç›‘æ§çš„è‚¡ç¥¨åˆ—è¡¨ï¼ˆä½¿ç”¨æ•°æ®åº“å­˜å‚¨ï¼‰
def _load_monitored_stocks():
    """ä»æ•°æ®åº“åŠ è½½ç›‘æ§è‚¡ç¥¨ï¼ŒåŒ…æ‹¬æƒ…ç»ªå’Œé£é™©æ•°æ®"""
    try:
        with get_db_context() as db:
            stocks = MonitoredStockService.get_all_active(db)
            result = {}
            for stock in stocks:
                # å°è¯•ä»æ•°æ®åº“åŠ è½½ç»¼åˆæ•°æ®ä»¥è·å–æƒ…ç»ªå’Œé£é™©ä¿¡æ¯
                sentiment_score = 50
                risk_level = "low"
                risk_score = 0
                latest_news = ""

                try:
                    record = StockDataService.get_latest(db, stock.ts_code, 'comprehensive')
                    if record and record.data:
                        data = record.data
                        # è·å–æƒ…ç»ªè¯„åˆ†
                        if 'overall_score' in data:
                            sentiment_score = data.get('overall_score', 50)
                        # è·å–é£é™©ä¿¡æ¯
                        if 'risk' in data and isinstance(data['risk'], dict):
                            risk_level = data['risk'].get('risk_level', 'low')
                            risk_score = data['risk'].get('risk_score', 0)
                        # è·å–æœ€æ–°æ–°é—»
                        if 'news' in data and isinstance(data['news'], list) and data['news']:
                            latest_news = data['news'][0].get('title', '') if data['news'] else ''
                except Exception as e:
                    logger.warning(f"åŠ è½½{stock.ts_code}çš„ç»¼åˆæ•°æ®å¤±è´¥: {e}")

                result[stock.ts_code] = {
                    "name": stock.name,
                    "code": stock.ts_code,
                    "frequency": stock.frequency,
                    "items": stock.items or {},
                    "sentimentScore": sentiment_score,
                    "riskLevel": risk_level,
                    "riskScore": risk_score,
                    "latestNews": latest_news,
                    "lastUpdate": stock.last_update.isoformat() if stock.last_update else None,
                    "pendingTasks": 0
                }
            return result
    except Exception as e:
        logger.error(f"åŠ è½½ç›‘æ§è‚¡ç¥¨å¤±è´¥: {e}")
        return {}

def _save_monitored_stocks():
    """ä¿å­˜ç›‘æ§è‚¡ç¥¨åˆ°æ•°æ®åº“ï¼ˆå·²åœ¨å„æ“ä½œä¸­ç›´æ¥ä¿å­˜ï¼Œæ­¤å‡½æ•°ä¿ç•™å…¼å®¹æ€§ï¼‰"""
    pass  # æ•°æ®åº“æ“ä½œå·²åœ¨å„APIä¸­ç›´æ¥æ‰§è¡Œ

# åˆå§‹åŒ–æ—¶ä»æ•°æ®åº“åŠ è½½
monitored_stocks = _load_monitored_stocks()

# æ•°æ®ç¼“å­˜ - å¯åŠ¨æ—¶ä»æ•°æ®åº“åŠ è½½
def _load_comprehensive_cache_from_db():
    """ä»æ•°æ®åº“åŠ è½½ç»¼åˆæ•°æ®ç¼“å­˜"""
    result = {}
    try:
        with get_db_context() as db:
            stocks = MonitoredStockService.get_all_active(db)
            for stock in stocks:
                record = StockDataService.get_latest(db, stock.ts_code, 'comprehensive')
                if record and record.data:
                    result[f"comprehensive_{stock.ts_code}"] = {
                        'cached_at': record.fetch_time.isoformat() if record.fetch_time else None,
                        'data': record.data
                    }
        logger.info(f"âœ… å¯åŠ¨æ—¶ä»æ•°æ®åº“åŠ è½½ç»¼åˆæ•°æ®ç¼“å­˜: {len(result)}ä¸ª")
    except Exception as e:
        logger.error(f"åŠ è½½ç»¼åˆæ•°æ®ç¼“å­˜å¤±è´¥: {e}")
    return result

data_cache = _load_comprehensive_cache_from_db()
data_sources_status = {
    "tushare": {
        "id": "tushare",
        "name": "Tushare",
        "type": "è‚¡ç¥¨æ•°æ®/æ–°é—»",
        "status": "offline",
        "todayCalls": 0,
        "lastUpdate": None,
        "error": None
    },
    "akshare": {
        "id": "akshare",
        "name": "AKShare",
        "type": "è‚¡ç¥¨æ•°æ®/èµ„è®¯",
        "status": "offline",
        "todayCalls": 0,
        "lastUpdate": None,
        "error": None
    },
    "eastmoney": {
        "id": "eastmoney",
        "name": "ä¸œæ–¹è´¢å¯Œ",
        "type": "æ–°é—»/èµ„è®¯",
        "status": "offline",
        "todayCalls": 0,
        "lastUpdate": None,
        "error": None
    },
    "juhe": {
        "id": "juhe",
        "name": "èšåˆæ•°æ®",
        "type": "æ–°é—»/èˆ†æƒ…",
        "status": "offline",
        "todayCalls": 0,
        "lastUpdate": None,
        "error": None
    },
    "cninfo": {
        "id": "cninfo",
        "name": "å·¨æ½®èµ„è®¯",
        "type": "å…¬å‘Š/ç ”æŠ¥",
        "status": "offline",
        "todayCalls": 0,
        "lastUpdate": None,
        "error": None
    }
}

# æ–°é—»åˆ—è¡¨
news_list = []


# ==================== APIæ¥å£ ====================

@router.get("/daily-stats")
@log_api_call("è·å–æ¯æ—¥ç»Ÿè®¡æ•°æ®")
async def get_daily_stats():
    """
    è·å–æ¯æ—¥ç»Ÿè®¡æ•°æ®
    åŒ…æ‹¬ï¼šç›‘æ§è‚¡ç¥¨æ•°ã€ä»Šæ—¥æ–°é—»æ•°ã€é£é™©é¢„è­¦æ•°ã€åˆ†æä»»åŠ¡æ•°
    """
    try:
        # ç»Ÿè®¡ç›‘æ§è‚¡ç¥¨æ•°
        monitored_count = len(monitored_stocks)

        # ç»Ÿè®¡ä»Šæ—¥æ–°é—»æ•°
        today = datetime.now().date()
        today_news_count = 0
        for news in news_list:
            try:
                news_time = news.get('publishTime') or news.get('pub_time', '')
                if news_time:
                    news_date = datetime.fromisoformat(news_time.replace('Z', '+00:00')).date()
                    if news_date == today:
                        today_news_count += 1
            except:
                pass

        # ç»Ÿè®¡é£é™©é¢„è­¦æ•°ï¼ˆé«˜é£é™©è‚¡ç¥¨æ•°ï¼‰
        risk_alert_count = sum(
            1 for stock in monitored_stocks.values()
            if stock.get('riskLevel') in ['high', 'medium']
        )

        # ç»Ÿè®¡åˆ†æä»»åŠ¡æ•°ï¼ˆå¾…å¤„ç†ä»»åŠ¡ï¼‰
        analysis_task_count = sum(
            stock.get('pendingTasks', 0)
            for stock in monitored_stocks.values()
        )

        # APIè°ƒç”¨ç»Ÿè®¡
        api_calls = {}
        for source_id, source_data in data_sources_status.items():
            api_calls[source_id] = source_data.get('todayCalls', 0)

        return {
            "success": True,
            "stats": {
                "monitoredStocks": monitored_count,
                "todayNews": today_news_count or len(news_list),  # å¦‚æœä»Šæ—¥æ–°é—»ä¸º0ï¼Œè¿”å›æ€»æ–°é—»æ•°
                "riskAlerts": risk_alert_count,
                "analysisTasks": analysis_task_count,
                "apiCalls": api_calls
            }
        }

    except Exception as e:
        logger.error(f"è·å–æ¯æ—¥ç»Ÿè®¡å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/stock/comprehensive/{ts_code}/from-db")
@log_api_call("ä»æ•°æ®åº“è·å–è‚¡ç¥¨ç»¼åˆæ•°æ®")
async def get_stock_comprehensive_from_db(ts_code: str):
    """
    ä»æ•°æ®åº“è·å–è‚¡ç¥¨çš„ç»¼åˆæ•°æ®ï¼ˆåªè¯»å–ï¼Œä¸è§¦å‘ä»»ä½•æ›´æ–°ï¼‰

    æ•°æ®æ›´æ–°åªåœ¨ä»¥ä¸‹æƒ…å†µå‘ç”Ÿï¼š
    1. é¦–æ¬¡æ·»åŠ ç›‘æ§è‚¡ç¥¨å
    2. å®šæ—¶å™¨åˆ°è¾¾æ—¶é—´
    3. æ‰‹åŠ¨ç‚¹å‡»ç«‹å³æ›´æ–°æŒ‰é’®

    Args:
        ts_code: è‚¡ç¥¨ä»£ç 
    """
    try:
        logger.info(f"ğŸ“Š ä»æ•°æ®åº“è·å– {ts_code} çš„ç»¼åˆæ•°æ®...")

        # 1. ä¼˜å…ˆä»æ•°æ®åº“è·å–
        with get_db_context() as db:
            record = StockDataService.get_latest(db, ts_code, 'comprehensive')
            if record and record.data:
                logger.info(f"âœ… ä»æ•°æ®åº“è·å–æ•°æ®æˆåŠŸ")
                # æ¸…ç†éæ³•floatå€¼ï¼ˆinf, -inf, nanï¼‰
                sanitized_data = sanitize_for_json(record.data)
                # åŒæ—¶æ›´æ–°å†…å­˜ç¼“å­˜
                cache_key = f"comprehensive_{ts_code}"
                data_cache[cache_key] = {
                    'cached_at': record.fetch_time.isoformat() if record.fetch_time else None,
                    'data': sanitized_data
                }
                return {
                    "success": True,
                    "has_data": True,
                    "data": sanitized_data,
                    "loaded_at": record.fetch_time.isoformat() if record.fetch_time else None,
                    "from_database": True
                }

        # 2. æ•°æ®åº“æ²¡æœ‰æ•°æ®ï¼Œæ£€æŸ¥å†…å­˜ç¼“å­˜ï¼ˆå…¼å®¹æ—§æ•°æ®ï¼‰
        cache_key = f"comprehensive_{ts_code}"
        if cache_key in data_cache:
            cached_data = data_cache[cache_key]
            logger.info(f"âœ… ä»å†…å­˜ç¼“å­˜è·å–æ•°æ®æˆåŠŸ")
            sanitized_data = sanitize_for_json(cached_data.get('data', {}))
            return {
                "success": True,
                "has_data": True,
                "data": sanitized_data,
                "loaded_at": cached_data.get('cached_at'),
                "from_database": False
            }

        # 3. æ²¡æœ‰æ•°æ®
        logger.info(f"â„¹ï¸ æ²¡æœ‰æ‰¾åˆ° {ts_code} çš„æ•°æ®ï¼Œè¯·ç‚¹å‡»åˆ·æ–°æŒ‰é’®è·å–")
        return {
            "success": True,
            "has_data": False,
            "data": None,
            "message": "æš‚æ— æ•°æ®ï¼Œè¯·ç‚¹å‡»åˆ·æ–°æŒ‰é’®è·å–"
        }

    except Exception as e:
        logger.error(f"ä»æ•°æ®åº“è·å–ç»¼åˆæ•°æ®å¤±è´¥: {e}")
        import traceback
        logger.error(traceback.format_exc())
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/stock/cached/{ts_code}")
@log_api_call("è·å–è‚¡ç¥¨ç¼“å­˜æ•°æ®")
async def get_stock_cached(ts_code: str):
    """
    è·å–è‚¡ç¥¨çš„ç¼“å­˜æ•°æ®ï¼ˆç”¨äºå‰ç«¯å¿«é€ŸåŠ è½½ï¼‰

    Args:
        ts_code: è‚¡ç¥¨ä»£ç 
    """
    try:
        cache_key = f"comprehensive_{ts_code}"

        if cache_key in data_cache:
            cached_data = data_cache[cache_key]
            return {
                "success": True,
                "has_data": True,
                "comprehensive": cached_data.get('data', {}),
                "news": cached_data.get('data', {}).get('news', []),
                "cached_at": cached_data.get('cached_at')
            }

        return {
            "success": True,
            "has_data": False,
            "message": "æ— ç¼“å­˜æ•°æ®"
        }

    except Exception as e:
        logger.error(f"è·å–ç¼“å­˜æ•°æ®å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/monitored-stocks")
@log_api_call("è·å–ç›‘æ§è‚¡ç¥¨åˆ—è¡¨")
async def get_monitored_stocks():
    """
    è·å–å½“å‰ç›‘æ§çš„è‚¡ç¥¨åˆ—è¡¨
    """
    try:
        stocks = []
        for code, data in monitored_stocks.items():
            stocks.append({
                "code": code,
                "name": data.get("name", "æœªçŸ¥"),
                "sentimentScore": data.get("sentimentScore", 50),
                "riskLevel": data.get("riskLevel", "low"),
                "riskScore": data.get("riskScore", 0),
                "latestNews": data.get("latestNews", ""),
                "updateFrequency": data.get("frequency", "1h"),
                "lastUpdate": data.get("lastUpdate"),
                "pendingTasks": data.get("pendingTasks", 0)
            })

        return {
            "success": True,
            "stocks": stocks
        }

    except Exception as e:
        logger.error(f"è·å–ç›‘æ§è‚¡ç¥¨å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/sources/status")
@log_api_call("è·å–æ•°æ®æºçŠ¶æ€")
async def get_data_sources_status():
    """
    è·å–æ‰€æœ‰æ•°æ®æºçš„çŠ¶æ€ï¼ˆä½¿ç”¨ç¼“å­˜ï¼Œä¸è‡ªåŠ¨æ£€æµ‹ï¼‰
    è‡ªåŠ¨æ£€æµ‹ä¼šå¾ˆæ…¢ï¼Œæ”¹ä¸ºåªåœ¨ç”¨æˆ·ç‚¹å‡»"æ£€æµ‹è¿æ¥"æ—¶æ‰æ£€æµ‹
    """
    try:
        sources = list(data_sources_status.values())
        return {
            "success": True,
            "sources": sources
        }

    except Exception as e:
        logger.error(f"è·å–æ•°æ®æºçŠ¶æ€å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))


async def _check_all_data_sources():
    """æ£€æµ‹æ‰€æœ‰æ•°æ®æºçŠ¶æ€ï¼ˆä½¿ç”¨è½»é‡çº§APIï¼‰"""
    try:
        current_time = datetime.now().isoformat()

        # æ£€æµ‹AKShare - ä½¿ç”¨è½»é‡çº§API
        try:
            import akshare as ak
            # ä½¿ç”¨äº¤æ˜“æ—¥å†APIï¼Œæ¯”å…¨å¸‚åœºè¡Œæƒ…å¿«å¾ˆå¤š
            df = ak.tool_trade_date_hist_sina()
            if df is not None and not df.empty:
                data_sources_status["akshare"]["status"] = "online"
                data_sources_status["akshare"]["error"] = None
            else:
                data_sources_status["akshare"]["status"] = "error"
                data_sources_status["akshare"]["error"] = "æ— æ³•è·å–æ•°æ®"
        except Exception as e:
            data_sources_status["akshare"]["status"] = "error"
            data_sources_status["akshare"]["error"] = str(e)[:100]
        data_sources_status["akshare"]["lastUpdate"] = current_time

        # æ£€æµ‹Tushare
        try:
            import tushare as ts
            # æ£€æŸ¥æ˜¯å¦æœ‰token
            token = os.getenv('TUSHARE_TOKEN')
            if token:
                ts.set_token(token)
                pro = ts.pro_api()
                df = pro.daily(ts_code='000001.SZ', start_date='20250101', end_date='20250102')
                if df is not None and not df.empty:
                    data_sources_status["tushare"]["status"] = "online"
                    data_sources_status["tushare"]["error"] = None
                else:
                    data_sources_status["tushare"]["status"] = "error"
                    data_sources_status["tushare"]["error"] = "æ— æ³•è·å–æ•°æ®"
            else:
                data_sources_status["tushare"]["status"] = "offline"
                data_sources_status["tushare"]["error"] = "æœªé…ç½®TUSHARE_TOKEN"
        except Exception as e:
            data_sources_status["tushare"]["status"] = "error"
            data_sources_status["tushare"]["error"] = str(e)[:100]
        data_sources_status["tushare"]["lastUpdate"] = current_time

        # æ£€æµ‹ä¸œæ–¹è´¢å¯Œ
        try:
            import requests
            response = requests.get("https://push2.eastmoney.com/api/qt/stock/get", timeout=5)
            if response.status_code == 200:
                data_sources_status["eastmoney"]["status"] = "online"
                data_sources_status["eastmoney"]["error"] = None
            else:
                data_sources_status["eastmoney"]["status"] = "error"
                data_sources_status["eastmoney"]["error"] = f"HTTP {response.status_code}"
        except Exception as e:
            data_sources_status["eastmoney"]["status"] = "error"
            data_sources_status["eastmoney"]["error"] = str(e)[:100]
        data_sources_status["eastmoney"]["lastUpdate"] = current_time

        # æ£€æµ‹èšåˆæ•°æ®
        try:
            import requests
            juhe_key = os.getenv('JUHE_API_KEY')
            if juhe_key:
                response = requests.get(f"http://web.juhe.cn/finance/stock/hs?gid=sh601006&key={juhe_key}", timeout=5)
                if response.status_code == 200:
                    data = response.json()
                    if data.get("error_code") == 0:
                        data_sources_status["juhe"]["status"] = "online"
                        data_sources_status["juhe"]["error"] = None
                    else:
                        data_sources_status["juhe"]["status"] = "error"
                        data_sources_status["juhe"]["error"] = data.get("reason", "APIé”™è¯¯")
                else:
                    data_sources_status["juhe"]["status"] = "error"
                    data_sources_status["juhe"]["error"] = f"HTTP {response.status_code}"
            else:
                data_sources_status["juhe"]["status"] = "offline"
                data_sources_status["juhe"]["error"] = "æœªé…ç½®JUHE_API_KEY"
        except Exception as e:
            data_sources_status["juhe"]["status"] = "error"
            data_sources_status["juhe"]["error"] = str(e)[:100]
        data_sources_status["juhe"]["lastUpdate"] = current_time

        # æ£€æµ‹å·¨æ½®èµ„è®¯
        try:
            from backend.dataflows.announcement.cninfo_api import CninfoConfig
            if CninfoConfig.is_configured():
                # å°è¯•è°ƒç”¨ä¸€ä¸ªç®€å•çš„APIæ¥æ£€æµ‹è¿æ¥
                import requests
                response = requests.get("https://webapi.cninfo.com.cn/", timeout=5)
                if response.status_code == 200:
                    data_sources_status["cninfo"]["status"] = "online"
                    data_sources_status["cninfo"]["error"] = None
                else:
                    data_sources_status["cninfo"]["status"] = "error"
                    data_sources_status["cninfo"]["error"] = f"HTTP {response.status_code}"
            else:
                data_sources_status["cninfo"]["status"] = "offline"
                data_sources_status["cninfo"]["error"] = "æœªé…ç½®CNINFO_ACCESS_KEY"
        except Exception as e:
            data_sources_status["cninfo"]["status"] = "error"
            data_sources_status["cninfo"]["error"] = str(e)[:100]
        data_sources_status["cninfo"]["lastUpdate"] = current_time

    except Exception as e:
        logger.error(f"æ£€æµ‹æ•°æ®æºå¤±è´¥: {e}")


@router.post("/sources/check")
@log_api_call("æ£€æµ‹æ•°æ®æºè¿æ¥")
async def check_data_sources():
    """
    æ£€æµ‹æ‰€æœ‰æ•°æ®æºçš„è¿æ¥çŠ¶æ€
    """
    try:
        # TODO: å®ç°çœŸå®çš„æ•°æ®æºè¿æ¥æ£€æµ‹
        # è¿™é‡Œå…ˆç”¨æ¨¡æ‹Ÿæ•°æ®
        
        # Tushareæ£€æµ‹
        try:
            import tushare as ts
            data_sources_status["tushare"]["status"] = "online"
            data_sources_status["tushare"]["lastUpdate"] = datetime.now().isoformat()
            data_sources_status["tushare"]["error"] = None
        except Exception as e:
            data_sources_status["tushare"]["status"] = "error"
            data_sources_status["tushare"]["error"] = str(e)
        
        # AKShareæ£€æµ‹
        try:
            import akshare as ak
            data_sources_status["akshare"]["status"] = "online"
            data_sources_status["akshare"]["lastUpdate"] = datetime.now().isoformat()
            data_sources_status["akshare"]["error"] = None
        except Exception as e:
            data_sources_status["akshare"]["status"] = "error"
            data_sources_status["akshare"]["error"] = str(e)
        
        # å…¶ä»–æ•°æ®æºè®¾ç½®ä¸ºåœ¨çº¿ï¼ˆç®€åŒ–ï¼‰
        for source_id in ["eastmoney", "juhe"]:
            data_sources_status[source_id]["status"] = "online"
            data_sources_status[source_id]["lastUpdate"] = datetime.now().isoformat()
        
        return {
            "success": True,
            "message": "æ•°æ®æºæ£€æµ‹å®Œæˆ"
        }

    except Exception as e:
        logger.error(f"æ£€æµ‹æ•°æ®æºå¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/sources/check-single")
@log_api_call("æ£€æµ‹å•ä¸ªæ•°æ®æº")
async def check_single_data_source(request: Dict[str, Any]):
    """
    æ£€æµ‹å•ä¸ªæ•°æ®æºçš„è¿æ¥çŠ¶æ€
    """
    try:
        source_id = request.get("source_id")
        if not source_id or source_id not in data_sources_status:
            return {"success": False, "error": "æ— æ•ˆçš„æ•°æ®æºID"}

        current_time = datetime.now().isoformat()
        status = "offline"
        error = None

        if source_id == "akshare":
            try:
                import akshare as ak
                df = ak.tool_trade_date_hist_sina()
                if df is not None and not df.empty:
                    status = "online"
                else:
                    status = "error"
                    error = "æ— æ³•è·å–æ•°æ®"
            except Exception as e:
                status = "error"
                error = str(e)[:100]

        elif source_id == "tushare":
            try:
                import tushare as ts
                token = os.getenv('TUSHARE_TOKEN')
                if token:
                    ts.set_token(token)
                    pro = ts.pro_api()
                    df = pro.daily(ts_code='000001.SZ', start_date='20250101', end_date='20250102')
                    if df is not None:
                        status = "online"
                    else:
                        status = "error"
                        error = "æ— æ³•è·å–æ•°æ®"
                else:
                    status = "offline"
                    error = "æœªé…ç½®TUSHARE_TOKEN"
            except Exception as e:
                status = "error"
                error = str(e)[:100]

        elif source_id == "eastmoney":
            try:
                import requests
                response = requests.get("https://push2.eastmoney.com/api/qt/stock/get", timeout=5)
                if response.status_code == 200:
                    status = "online"
                else:
                    status = "error"
                    error = f"HTTP {response.status_code}"
            except Exception as e:
                status = "error"
                error = str(e)[:100]

        elif source_id == "juhe":
            try:
                import requests
                juhe_key = os.getenv('JUHE_API_KEY')
                if juhe_key:
                    response = requests.get(f"http://web.juhe.cn/finance/stock/hs?gid=sh601006&key={juhe_key}", timeout=5)
                    if response.status_code == 200:
                        data = response.json()
                        if data.get("error_code") == 0:
                            status = "online"
                        else:
                            status = "error"
                            error = data.get("reason", "APIé”™è¯¯")
                    else:
                        status = "error"
                        error = f"HTTP {response.status_code}"
                else:
                    status = "offline"
                    error = "æœªé…ç½®JUHE_API_KEY"
            except Exception as e:
                status = "error"
                error = str(e)[:100]

        elif source_id == "cninfo":
            try:
                from backend.dataflows.announcement.cninfo_api import CninfoConfig
                if CninfoConfig.is_configured():
                    import requests
                    response = requests.get("https://webapi.cninfo.com.cn/", timeout=5)
                    if response.status_code == 200:
                        status = "online"
                    else:
                        status = "error"
                        error = f"HTTP {response.status_code}"
                else:
                    status = "offline"
                    error = "æœªé…ç½®CNINFO_ACCESS_KEY"
            except Exception as e:
                status = "error"
                error = str(e)[:100]

        # æ›´æ–°çŠ¶æ€
        data_sources_status[source_id]["status"] = status
        data_sources_status[source_id]["lastUpdate"] = current_time
        data_sources_status[source_id]["error"] = error

        return {
            "success": True,
            "source_id": source_id,
            "status": status,
            "error": error,
            "success_rate": 100 if status == "online" else 0
        }

    except Exception as e:
        logger.error(f"æ£€æµ‹å•ä¸ªæ•°æ®æºå¤±è´¥: {e}")
        return {"success": False, "error": str(e)}


@router.get("/news")
@log_api_call("è·å–æ–°é—»åˆ—è¡¨")
async def get_news(source: Optional[str] = None, limit: int = 50):
    """
    è·å–æ–°é—»åˆ—è¡¨ - ä½¿ç”¨ç»Ÿä¸€æ–°é—»ç›‘æ§ä¸­å¿ƒ

    Args:
        source: æ•°æ®æºç­›é€‰ï¼ˆå¯é€‰ï¼‰
        limit: è¿”å›æ•°é‡é™åˆ¶
    """
    try:
        # ä¼˜å…ˆä½¿ç”¨ç»Ÿä¸€æ–°é—»ç›‘æ§ä¸­å¿ƒ
        try:
            from backend.services.news_center import get_news_monitor_center
            monitor = get_news_monitor_center()

            # ä»ç›‘æ§ä¸­å¿ƒè·å–æ–°é—»
            news_data = monitor.get_latest_news(limit=limit)

            if news_data:
                # æŒ‰æ¥æºç­›é€‰
                if source and source != "all":
                    news_data = [n for n in news_data if n.get("source") == source]

                # è½¬æ¢ä¸ºå‰ç«¯æœŸæœ›çš„æ ¼å¼
                formatted_news = []
                for n in news_data:
                    formatted_news.append({
                        'id': n.get('id', ''),
                        'title': n.get('title', ''),
                        'summary': n.get('content', '')[:200] if n.get('content') else '',
                        'content': n.get('content', ''),
                        'publishTime': n.get('pub_time', ''),
                        'pub_time': n.get('pub_time', ''),
                        'source': n.get('source', ''),
                        'sentiment': n.get('sentiment', 'neutral'),
                        'sentiment_score': n.get('sentiment_score', 50),
                        'url': n.get('url', ''),
                        'keywords': n.get('keywords', []),
                        'urgency': n.get('urgency', 'low'),
                        'related_stocks': n.get('related_stocks', []),
                        'impact_score': n.get('impact_score', 0)
                    })

                # è®¡ç®—æƒ…ç»ªç»Ÿè®¡
                sentiment_stats = {
                    'positive': sum(1 for n in formatted_news if n.get('sentiment') == 'positive'),
                    'negative': sum(1 for n in formatted_news if n.get('sentiment') == 'negative'),
                    'neutral': sum(1 for n in formatted_news if n.get('sentiment') == 'neutral')
                }

                # è·å–ç›‘æ§ä¸­å¿ƒç»Ÿè®¡
                monitor_stats = monitor.get_stats()

                logger.info(f"ğŸ“° ä»ç»Ÿä¸€æ–°é—»ä¸­å¿ƒè·å–æ–°é—»: {len(formatted_news)}æ¡")
                return {
                    "success": True,
                    "news": formatted_news,
                    "total": len(formatted_news),
                    "total_fetched": monitor_stats.get('total_fetched', len(formatted_news)),
                    "sentiment_stats": sentiment_stats,
                    "source": "news_monitor_center"
                }
        except Exception as e:
            logger.warning(f"ç»Ÿä¸€æ–°é—»ä¸­å¿ƒä¸å¯ç”¨ï¼Œå›é€€åˆ°æ—§é€»è¾‘: {e}")

        # å›é€€åˆ°æ—§é€»è¾‘
        global news_list

        # å¦‚æœæ–°é—»åˆ—è¡¨ä¸ºç©ºï¼Œä¸»åŠ¨è·å–å¸‚åœºæ–°é—»
        if not news_list:
            logger.info("ğŸ“° æ–°é—»åˆ—è¡¨ä¸ºç©ºï¼Œæ­£åœ¨è·å–å¸‚åœºæ–°é—»...")
            try:
                import akshare as ak
                from backend.dataflows.stock.akshare_utils import get_stock_news_em

                # å°è¯•å¤šä¸ªæ–°é—»æº
                news_sources = [
                    ('stock_info_global_em', 'ä¸œæ–¹è´¢å¯Œ'),
                    ('stock_news_em', 'ä¸œæ–¹è´¢å¯Œä¸ªè‚¡'),
                ]

                for api_name, source_name in news_sources:
                    if news_list:  # å¦‚æœå·²ç»è·å–åˆ°æ–°é—»ï¼Œè·³è¿‡
                        break
                    try:
                        if api_name == 'stock_info_global_em':
                            df = ak.stock_info_global_em()
                        elif api_name == 'stock_news_em':
                            # ä½¿ç”¨ä¿®å¤ç‰ˆå‡½æ•°è·å–ä¸ªè‚¡æ–°é—»
                            df = get_stock_news_em(symbol="000001", max_news=50)
                        else:
                            continue

                        if df is not None and not df.empty:
                            for _, row in df.head(50).iterrows():
                                # æ ¹æ®ä¸åŒAPIè°ƒæ•´å­—æ®µå
                                if api_name == 'stock_info_global_em':
                                    title = str(row.get('æ ‡é¢˜', ''))
                                    content = str(row.get('å†…å®¹', ''))
                                    pub_time = str(row.get('å‘å¸ƒæ—¶é—´', ''))
                                else:
                                    title = str(row.get('æ–°é—»æ ‡é¢˜', row.get('æ ‡é¢˜', '')))
                                    content = str(row.get('æ–°é—»å†…å®¹', row.get('å†…å®¹', '')))
                                    pub_time = str(row.get('å‘å¸ƒæ—¶é—´', row.get('æ—¶é—´', '')))

                                if not title:
                                    continue

                                # ç®€å•æƒ…ç»ªåˆ†æ
                                sentiment = 'neutral'
                                positive_keywords = ['æ¶¨', 'ä¸Šæ¶¨', 'å¤§æ¶¨', 'æš´æ¶¨', 'åˆ©å¥½', 'çªç ´', 'æ–°é«˜', 'å¢é•¿', 'ç›ˆåˆ©', 'è¶…é¢„æœŸ', 'ä¸Šè°ƒ', 'å¢æŒ']
                                negative_keywords = ['è·Œ', 'ä¸‹è·Œ', 'å¤§è·Œ', 'æš´è·Œ', 'åˆ©ç©º', 'ä¸‹æ»‘', 'æ–°ä½', 'äºæŸ', 'ä¸‹é™', 'ä¸åŠé¢„æœŸ', 'ä¸‹è°ƒ', 'å‡æŒ']

                                for kw in positive_keywords:
                                    if kw in title or kw in content:
                                        sentiment = 'positive'
                                        break
                                if sentiment == 'neutral':
                                    for kw in negative_keywords:
                                        if kw in title or kw in content:
                                            sentiment = 'negative'
                                            break

                                news_list.append({
                                    'id': f"em_{pub_time}_{len(news_list)}",
                                    'title': title,
                                    'summary': content[:200] if content else '',
                                    'content': content,
                                    'publishTime': pub_time,
                                    'pub_time': pub_time,
                                    'source': source_name,
                                    'sentiment': sentiment,
                                    'sentiment_score': 75 if sentiment == 'positive' else (25 if sentiment == 'negative' else 50),
                                    'url': '',
                                    'relatedStocks': []
                                })

                            logger.info(f"âœ… ä»{source_name}è·å–å¸‚åœºæ–°é—»æˆåŠŸ: {len(news_list)}æ¡")
                    except Exception as e:
                        logger.warning(f"âš ï¸ ä»{source_name}è·å–æ–°é—»å¤±è´¥: {e}")
                        continue

            except Exception as e:
                logger.warning(f"âš ï¸ è·å–å¸‚åœºæ–°é—»å¤±è´¥: {e}")
                import traceback
                logger.warning(traceback.format_exc())

        filtered_news = news_list

        if source and source != "all":
            filtered_news = [n for n in news_list if n.get("source") == source]

        # è®°å½•å»é‡å‰çš„æ•°é‡
        total_before_limit = len(filtered_news)
        filtered_news = filtered_news[:limit]

        # è®¡ç®—æƒ…ç»ªç»Ÿè®¡
        sentiment_stats = {
            'positive': sum(1 for n in filtered_news if n.get('sentiment') == 'positive'),
            'negative': sum(1 for n in filtered_news if n.get('sentiment') == 'negative'),
            'neutral': sum(1 for n in filtered_news if n.get('sentiment') == 'neutral')
        }

        return {
            "success": True,
            "news": filtered_news,
            "total": len(filtered_news),
            "total_fetched": len(news_list),  # åŸå§‹è·å–æ•°é‡
            "total_after_filter": total_before_limit,  # ç­›é€‰åæ•°é‡
            "sentiment_stats": sentiment_stats
        }

    except Exception as e:
        logger.error(f"è·å–æ–°é—»å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/stock/news/{ts_code}")
@log_api_call("è·å–è‚¡ç¥¨æ–°é—»")
async def get_stock_news(ts_code: str, limit: int = 20):
    """
    è·å–æŒ‡å®šè‚¡ç¥¨çš„æ–°é—» - ä¼˜å…ˆä½¿ç”¨ç»Ÿä¸€æ–°é—»ç›‘æ§ä¸­å¿ƒ
    """
    try:
        logger.info(f"è·å–{ts_code}çš„æ–°é—»...")

        # ä¼˜å…ˆä½¿ç”¨ç»Ÿä¸€æ–°é—»ç›‘æ§ä¸­å¿ƒ
        try:
            from backend.services.news_center import get_news_monitor_center
            monitor = get_news_monitor_center()

            # ä»ç›‘æ§ä¸­å¿ƒè·å–è¯¥è‚¡ç¥¨ç›¸å…³æ–°é—»
            news_data = monitor.get_news_for_stock(ts_code, limit=limit)

            if news_data:
                # è½¬æ¢ä¸ºå‰ç«¯æœŸæœ›çš„æ ¼å¼
                formatted_news = []
                for n in news_data:
                    formatted_news.append({
                        'title': n.get('title', ''),
                        'content': n.get('content', ''),
                        'pub_time': n.get('pub_time', ''),
                        'source': n.get('source', ''),
                        'sentiment': n.get('sentiment', 'neutral'),
                        'score': n.get('sentiment_score', 50),
                        'url': n.get('url', ''),
                        'keywords': n.get('keywords', []),
                        'urgency': n.get('urgency', 'low'),
                        'report_type': n.get('report_type', 'news'),
                        'impact_score': n.get('impact_score', 0)
                    })

                # è®¡ç®—æƒ…ç»ªç»Ÿè®¡
                overall_score = sum(n.get('score', 50) for n in formatted_news) / len(formatted_news) if formatted_news else 50
                sentiment_summary = {
                    'positive': sum(1 for n in formatted_news if n.get('sentiment') == 'positive'),
                    'negative': sum(1 for n in formatted_news if n.get('sentiment') == 'negative'),
                    'neutral': sum(1 for n in formatted_news if n.get('sentiment') == 'neutral')
                }

                logger.info(f"âœ… ä»ç»Ÿä¸€æ–°é—»ä¸­å¿ƒè·å–{ts_code}æ–°é—»: {len(formatted_news)}æ¡")
                return {
                    "success": True,
                    "news": formatted_news,
                    "total": len(formatted_news),
                    "overall_score": overall_score,
                    "sentiment_summary": sentiment_summary,
                    "source": "news_monitor_center"
                }
        except Exception as e:
            logger.warning(f"ç»Ÿä¸€æ–°é—»ä¸­å¿ƒä¸å¯ç”¨ï¼Œå›é€€åˆ°æ—§é€»è¾‘: {e}")

        # å›é€€åˆ°æ—§é€»è¾‘
        aggregator = get_news_aggregator()

        # ä¿®å¤ï¼šä½¿ç”¨æ­£ç¡®çš„å‚æ•°å limit_per_source
        result = aggregator.aggregate_news(
            ts_code=ts_code,
            limit_per_source=limit,  # ä¿®å¤å‚æ•°å
            include_tushare=False,  # Tushareéœ€è¦5000ç§¯åˆ†ï¼Œé»˜è®¤ä¸å¼€å¯
            include_akshare=True,
            include_market_news=False
        )

        # ä¿®å¤ï¼šä½¿ç”¨æ­£ç¡®çš„å­—æ®µå merged_news
        news_list = result.get('merged_news', [])

        logger.info(f"âœ… è¿”å›{len(news_list)}æ¡æ–°é—»")

        return {
            "success": True,
            "news": news_list,
            "total": result.get('total_count', 0),
            "sources": result.get('sources', {})
        }

    except Exception as e:
        logger.error(f"è·å–è‚¡ç¥¨æ–°é—»å¤±è´¥: {e}")
        import traceback
        logger.error(traceback.format_exc())
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/stock/sentiment/{ts_code}")
@log_api_call("è·å–è‚¡ç¥¨æƒ…ç»ªåˆ†æ")
async def get_stock_sentiment(ts_code: str, limit: int = 20):
    """
    è·å–æŒ‡å®šè‚¡ç¥¨çš„æƒ…ç»ªåˆ†æ
    """
    try:
        logger.info(f"è·å–{ts_code}çš„æƒ…ç»ªåˆ†æ...")
        
        # å…ˆè·å–æ–°é—»
        aggregator = get_news_aggregator()
        news_result = aggregator.aggregate_news(
            ts_code=ts_code,
            limit_per_source=limit,  # ä¿®å¤å‚æ•°å
            include_tushare=False,
            include_akshare=True,
            include_market_news=False
        )
        
        news_list = news_result.get('merged_news', [])  # ä¿®å¤å­—æ®µå
        
        # æƒ…ç»ªåˆ†æ
        engine = get_sentiment_engine()
        sentiment_result = engine.analyze_news_list(news_list)
        
        return {
            "success": True,
            **sentiment_result
        }
        
    except Exception as e:
        logger.error(f"æƒ…ç»ªåˆ†æå¤±è´¥: {e}")
        import traceback
        logger.error(traceback.format_exc())
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/stock/risk/{ts_code}")
@log_api_call("è·å–è‚¡ç¥¨é£é™©åˆ†æ")
async def get_stock_risk(ts_code: str):
    """
    è·å–æŒ‡å®šè‚¡ç¥¨çš„é£é™©åˆ†æ
    """
    try:
        logger.info(f"è·å–{ts_code}çš„é£é™©åˆ†æ...")
        
        # è°ƒç”¨é£é™©åˆ†æå‡½æ•°
        risk_result = analyze_stock_risk(ts_code)
        
        return {
            "success": True,
            **risk_result
        }
        
    except Exception as e:
        logger.error(f"é£é™©åˆ†æå¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/stock/comprehensive/{ts_code}")
@log_api_call("è·å–è‚¡ç¥¨ç»¼åˆæ•°æ®")
async def get_stock_comprehensive(ts_code: str, force_update: bool = False):
    """
    è·å–è‚¡ç¥¨çš„æ‰€æœ‰ç»¼åˆæ•°æ®ï¼ˆæ­¤æ¥å£ä¼šè§¦å‘æ•°æ®æ›´æ–°ï¼‰
    åŒ…æ‹¬ï¼šå®æ—¶è¡Œæƒ…ã€åœå¤ç‰Œã€STçŠ¶æ€ã€è´¢åŠ¡æ•°æ®ã€å®¡è®¡æ„è§ã€
          ä¸šç»©é¢„å‘Šã€åˆ†çº¢é€è‚¡ã€é™å”®è§£ç¦ã€è‚¡æƒè´¨æŠ¼ã€
          è‚¡ä¸œå¢å‡æŒã€é¾™è™æ¦œã€æ–°é—»ç­‰

    æ³¨æ„ï¼šæ­¤æ¥å£ä¼šè§¦å‘æ•°æ®æ›´æ–°å¹¶ä¿å­˜åˆ°æ•°æ®åº“
    å‰ç«¯è¯¦æƒ…æ¨¡æ€æ¡†åº”ä½¿ç”¨ /from-db æ¥å£åªè¯»å–æ•°æ®

    Args:
        ts_code: è‚¡ç¥¨ä»£ç 
        force_update: æ˜¯å¦å¼ºåˆ¶æ›´æ–°ï¼ˆå¿½ç•¥ç¼“å­˜ï¼‰
    """
    try:
        logger.info(f"ğŸ“Š å¼€å§‹è·å– {ts_code} çš„ç»¼åˆæ•°æ®...")

        # æ£€æŸ¥ç¼“å­˜
        cache_key = f"comprehensive_{ts_code}"
        current_time = datetime.now()

        # å¦‚æœç¼“å­˜å­˜åœ¨ä¸”ä¸è¶…è¿‡5åˆ†é’Ÿï¼Œç›´æ¥è¿”å›
        if not force_update and cache_key in data_cache:
            cached_data = data_cache[cache_key]
            cache_time = datetime.fromisoformat(cached_data.get('cached_at', '1970-01-01'))
            if (current_time - cache_time).total_seconds() < 300:  # 5åˆ†é’Ÿç¼“å­˜
                logger.info(f"ğŸ“¦ ä½¿ç”¨ç¼“å­˜æ•°æ® ({(current_time - cache_time).total_seconds():.1f}så‰)")
                # æ¸…ç†éæ³•floatå€¼ï¼ˆinf, -inf, nanï¼‰
                sanitized_data = sanitize_for_json(cached_data['data'])
                return {
                    "success": True,
                    "cached": True,
                    **sanitized_data
                }

        # è·å–æ–°æ•°æ®
        logger.info(f"ğŸ”„ è·å–æ–°æ•°æ®...")
        service = get_comprehensive_service()
        result = service.get_all_stock_data(ts_code)

        # æ¸…ç†éæ³•floatå€¼ï¼ˆinf, -inf, nanï¼‰
        result = sanitize_for_json(result)

        # ä¿å­˜åˆ°å†…å­˜ç¼“å­˜
        data_cache[cache_key] = {
            'cached_at': current_time.isoformat(),
            'data': result
        }

        # ä¿å­˜åˆ°æ•°æ®åº“
        with get_db_context() as db:
            StockDataService.save_or_update(
                db=db,
                ts_code=ts_code,
                data_type='comprehensive',
                data=result,
                source='mixed'
            )
            # æ›´æ–°ç›‘æ§è‚¡ç¥¨çš„æœ€åæ›´æ–°æ—¶é—´
            MonitoredStockService.update_last_update(db, ts_code)
        logger.info(f"âœ… ç»¼åˆæ•°æ®å·²ä¿å­˜åˆ°æ•°æ®åº“: {ts_code}")

        # æ›´æ–°å†…å­˜ä¸­çš„ç›‘æ§è‚¡ç¥¨ä¿¡æ¯
        if ts_code in monitored_stocks:
            monitored_stocks[ts_code]["lastUpdate"] = current_time.isoformat()
            # åŒæ—¶æ›´æ–°æƒ…ç»ªå’Œé£é™©æ•°æ®
            if 'overall_score' in result:
                monitored_stocks[ts_code]["sentimentScore"] = result.get('overall_score', 50)
            if 'risk' in result and isinstance(result['risk'], dict):
                monitored_stocks[ts_code]["riskLevel"] = result['risk'].get('risk_level', 'low')
                monitored_stocks[ts_code]["riskScore"] = result['risk'].get('risk_score', 0)
            if 'news' in result and isinstance(result['news'], list) and result['news']:
                monitored_stocks[ts_code]["latestNews"] = result['news'][0].get('title', '') if result['news'] else ''

        return {
            "success": True,
            "cached": False,
            **result
        }

    except Exception as e:
        logger.error(f"ç»¼åˆæ•°æ®è·å–å¤±è´¥: {e}")
        import traceback
        logger.error(traceback.format_exc())
        raise HTTPException(status_code=500, detail=str(e))


from fastapi.responses import StreamingResponse
import json

@router.get("/stock/comprehensive/{ts_code}/stream")
async def get_stock_comprehensive_stream(ts_code: str):
    """
    æµå¼è·å–è‚¡ç¥¨ç»¼åˆæ•°æ®ï¼ˆSSEï¼‰
    å‰ç«¯å¯ä»¥è¾¹è·å–è¾¹æ¸²æŸ“ï¼Œæå‡ç”¨æˆ·ä½“éªŒ
    """
    async def generate():
        try:
            # å‘é€å¼€å§‹ä¿¡å·
            yield f"data: {json.dumps({'type': 'start', 'ts_code': ts_code})}\n\n"

            # è·å–ç»¼åˆæ•°æ®æœåŠ¡
            service = get_comprehensive_service()

            # å®šä¹‰æ•°æ®åˆ†ç±»
            categories = {
                'basic': {'name': 'åŸºç¡€ä¿¡æ¯', 'fields': ['realtime', 'st_status', 'suspend']},
                'financial': {'name': 'è´¢åŠ¡æ•°æ®', 'fields': ['financial', 'forecast', 'dividend', 'audit']},
                'risk': {'name': 'é£é™©æ•°æ®', 'fields': ['pledge', 'restricted', 'holder_trade']},
                'market': {'name': 'å¸‚åœºæ•°æ®', 'fields': ['dragon_tiger', 'block_trade', 'margin']},
                'news': {'name': 'æ–°é—»èˆ†æƒ…', 'fields': ['news_sina', 'announcements', 'news']},
                'company': {'name': 'å…¬å¸ä¿¡æ¯', 'fields': ['company_info', 'managers', 'main_business']},
            }

            # è·å–å®Œæ•´æ•°æ®
            logger.info(f"ğŸ“Š å¼€å§‹æµå¼è·å– {ts_code} çš„ç»¼åˆæ•°æ®...")
            result = service.get_all_stock_data(ts_code)

            # æ¸…ç†éæ³•floatå€¼ï¼ˆinf, -inf, nanï¼‰
            result = sanitize_float_values(result)

            # æŒ‰åˆ†ç±»å‘é€æ•°æ®
            success_count = 0
            total_count = 0

            for category_key, category_info in categories.items():
                category_data = {}
                category_success = 0
                category_total = 0

                for field in category_info['fields']:
                    if field in result:
                        category_data[field] = result[field]
                        category_total += 1
                        if isinstance(result[field], dict) and result[field].get('status') in ['success', 'has_suspend', 'normal']:
                            category_success += 1

                total_count += category_total
                success_count += category_success

                # å‘é€åˆ†ç±»æ•°æ®ï¼ˆæ•°æ®å·²ç»è¢«sanitizeè¿‡ï¼‰
                yield f"data: {json.dumps({'type': 'category', 'category': category_key, 'data': {'name': category_info['name'], 'data': category_data, 'success_count': category_success, 'total_count': category_total}}, ensure_ascii=False)}\n\n"

                # çŸ­æš‚å»¶è¿Ÿï¼Œè®©å‰ç«¯æœ‰æ—¶é—´å¤„ç†
                await asyncio.sleep(0.1)

            # ä¿å­˜åˆ°å†…å­˜ç¼“å­˜ï¼ˆå·²æ¸…ç†çš„æ•°æ®ï¼‰
            cache_key = f"comprehensive_{ts_code}"
            current_time = datetime.now()
            data_cache[cache_key] = {
                'cached_at': current_time.isoformat(),
                'data': result
            }

            # ä¿å­˜åˆ°æ•°æ®åº“
            with get_db_context() as db:
                StockDataService.save_or_update(
                    db=db,
                    ts_code=ts_code,
                    data_type='comprehensive',
                    data=result,
                    source='mixed'
                )
                # æ›´æ–°ç›‘æ§è‚¡ç¥¨çš„æœ€åæ›´æ–°æ—¶é—´
                MonitoredStockService.update_last_update(db, ts_code)
            logger.info(f"âœ… ç»¼åˆæ•°æ®å·²ä¿å­˜åˆ°æ•°æ®åº“: {ts_code}")

            # æ›´æ–°å†…å­˜ä¸­çš„ç›‘æ§è‚¡ç¥¨ä¿¡æ¯
            if ts_code in monitored_stocks:
                monitored_stocks[ts_code]["lastUpdate"] = current_time.isoformat()
                # åŒæ—¶æ›´æ–°æƒ…ç»ªå’Œé£é™©æ•°æ®
                if 'overall_score' in result:
                    monitored_stocks[ts_code]["sentimentScore"] = result.get('overall_score', 50)
                if 'risk' in result and isinstance(result['risk'], dict):
                    monitored_stocks[ts_code]["riskLevel"] = result['risk'].get('risk_level', 'low')
                    monitored_stocks[ts_code]["riskScore"] = result['risk'].get('risk_score', 0)
                if 'news' in result and isinstance(result['news'], list) and result['news']:
                    monitored_stocks[ts_code]["latestNews"] = result['news'][0].get('title', '') if result['news'] else ''

            # å‘é€å®Œæˆä¿¡å·ï¼ŒåŒ…å« interface_status
            complete_data = {
                'type': 'complete',
                'success_count': success_count,
                'total_count': total_count,
                'success_rate': f'{success_count/total_count*100:.1f}%' if total_count > 0 else '0%',
                'total_time': 0,
                'interface_status': result.get('interface_status', {}),
                'alerts': result.get('alerts', [])
            }
            yield f"data: {json.dumps(complete_data, ensure_ascii=False)}\n\n"

        except Exception as e:
            logger.error(f"æµå¼è·å–æ•°æ®å¤±è´¥: {e}")
            import traceback
            logger.error(traceback.format_exc())
            yield f"data: {json.dumps({'type': 'error', 'error': str(e)})}\n\n"

    return StreamingResponse(
        generate(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "Access-Control-Allow-Origin": "*",
            "Access-Control-Allow-Headers": "*",
        }
    )


@router.post("/monitor/add")
@log_api_call("æ·»åŠ ç›‘æ§è‚¡ç¥¨")
async def add_monitor(request: MonitorStockRequest, background_tasks: BackgroundTasks):
    """
    æ·»åŠ è‚¡ç¥¨ç›‘æ§
    é¦–æ¬¡æ·»åŠ åä¼šç«‹å³æ‰§è¡Œä¸€æ¬¡æ•°æ®æ›´æ–°
    """
    try:
        code = request.code

        # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
        if code in monitored_stocks:
            raise HTTPException(status_code=400, detail="è¯¥è‚¡ç¥¨å·²åœ¨ç›‘æ§åˆ—è¡¨ä¸­")

        # è·å–è‚¡ç¥¨åç§°ï¼ˆä½¿ç”¨æ•°æ®æºç®¡ç†å™¨è·å–çœŸå®åç§°ï¼‰
        stock_name = code.split('.')[0]  # é»˜è®¤ä½¿ç”¨ä»£ç 
        try:
            from backend.dataflows.data_source_manager import get_data_source_manager
            manager = get_data_source_manager()
            stock_info = manager.get_stock_info(code)
            if stock_info and stock_info.get('name'):
                stock_name = stock_info['name']
                logger.info(f"âœ… è·å–è‚¡ç¥¨åç§°æˆåŠŸ: {code} -> {stock_name}")
        except Exception as e:
            logger.warning(f"âš ï¸ è·å–è‚¡ç¥¨åç§°å¤±è´¥ï¼Œä½¿ç”¨ä»£ç ä½œä¸ºåç§°: {e}")

        # ä¿å­˜åˆ°æ•°æ®åº“
        with get_db_context() as db:
            MonitoredStockService.add_stock(
                db=db,
                ts_code=code,
                name=stock_name,
                frequency=request.frequency,
                items=request.items
            )

        # æ·»åŠ åˆ°å†…å­˜ç›‘æ§åˆ—è¡¨
        monitored_stocks[code] = {
            "name": stock_name,
            "code": code,
            "frequency": request.frequency,
            "items": request.items,
            "sentimentScore": 50,
            "riskLevel": "low",
            "latestNews": "",
            "lastUpdate": datetime.now().isoformat(),
            "pendingTasks": 0
        }

        # æ·»åŠ åå°ä»»åŠ¡ï¼šé¦–æ¬¡æ·»åŠ åç«‹å³æ‰§è¡Œä¸€æ¬¡æ•°æ®æ›´æ–°
        background_tasks.add_task(update_stock_data, code)

        logger.info(f"æ·»åŠ ç›‘æ§è‚¡ç¥¨: {code} ({stock_name})")

        return {
            "success": True,
            "message": f"å·²æ·»åŠ ç›‘æ§: {stock_name}({code})",
            "code": code,
            "name": stock_name
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"æ·»åŠ ç›‘æ§å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/monitor/remove")
@log_api_call("ç§»é™¤ç›‘æ§è‚¡ç¥¨")
async def remove_monitor(request: RemoveMonitorRequest):
    """
    ç§»é™¤è‚¡ç¥¨ç›‘æ§
    """
    try:
        code = request.code

        if code not in monitored_stocks:
            raise HTTPException(status_code=404, detail="è¯¥è‚¡ç¥¨ä¸åœ¨ç›‘æ§åˆ—è¡¨ä¸­")

        # ä»æ•°æ®åº“åˆ é™¤
        with get_db_context() as db:
            MonitoredStockService.delete_stock(db, code)

        # ä»å†…å­˜åˆ é™¤
        del monitored_stocks[code]

        logger.info(f"ç§»é™¤ç›‘æ§è‚¡ç¥¨: {code}")
        
        return {
            "success": True,
            "message": f"å·²ç§»é™¤ç›‘æ§: {code}"
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"ç§»é™¤ç›‘æ§å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/monitor/update")
@log_api_call("ç«‹å³æ›´æ–°è‚¡ç¥¨æ•°æ®")
async def update_monitor(request: UpdateMonitorRequest, background_tasks: BackgroundTasks):
    """
    ç«‹å³æ›´æ–°æŒ‡å®šè‚¡ç¥¨çš„æ•°æ®
    """
    try:
        code = request.code
        
        if code not in monitored_stocks:
            raise HTTPException(status_code=404, detail="è¯¥è‚¡ç¥¨ä¸åœ¨ç›‘æ§åˆ—è¡¨ä¸­")
        
        # æ·»åŠ åå°ä»»åŠ¡
        background_tasks.add_task(update_stock_data, code)
        
        return {
            "success": True,
            "message": f"æ›´æ–°ä»»åŠ¡å·²æäº¤: {code}"
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"æ›´æ–°å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/stock/realtime/{ts_code}")
@log_api_call("è·å–è‚¡ç¥¨å®æ—¶æ•°æ®")
async def get_stock_realtime(ts_code: str):
    """
    è·å–è‚¡ç¥¨å®æ—¶æ•°æ®
    """
    try:
        realtime_data = get_stock_realtime_quote(ts_code)

        if realtime_data:
            return {
                "success": True,
                "data": realtime_data
            }
        else:
            return {
                "success": False,
                "error": "æœªèƒ½è·å–å®æ—¶æ•°æ®"
            }

    except Exception as e:
        logger.error(f"è·å–å®æ—¶æ•°æ®å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/stock/suspend/{ts_code}")
@log_api_call("æ£€æŸ¥è‚¡ç¥¨åœå¤ç‰ŒçŠ¶æ€")
async def get_stock_suspend(ts_code: str):
    """
    æ£€æŸ¥è‚¡ç¥¨åœå¤ç‰ŒçŠ¶æ€
    """
    try:
        suspend_status = check_suspend_status(ts_code)

        return {
            "success": True,
            "data": suspend_status
        }

    except Exception as e:
        logger.error(f"æ£€æŸ¥åœå¤ç‰Œå¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))


# ==================== åå°ä»»åŠ¡ ====================

async def update_stock_data(code: str):
    """
    æ›´æ–°è‚¡ç¥¨æ•°æ®ï¼ˆåå°ä»»åŠ¡ï¼‰
    
    åŒ…æ‹¬ï¼š
    1. è·å–æœ€æ–°æ–°é—»
    2. è¿›è¡Œæƒ…ç»ªåˆ†æ
    3. è¿›è¡Œé£é™©è¯„ä¼°
    4. æ›´æ–°ç›‘æ§çŠ¶æ€
    """
    try:
        logger.info(f"å¼€å§‹æ›´æ–°è‚¡ç¥¨æ•°æ®: {code}")
        
        if code not in monitored_stocks:
            return
        
        stock_data = monitored_stocks[code]
        items = stock_data.get("items", {})
        
        # 1. è·å–æ–°é—»ï¼ˆä½¿ç”¨çœŸå®çš„å¤šæºæ–°é—»èšåˆå™¨ï¼‰
        news_list_local = []
        if items.get("news", False):
            try:
                logger.info(f"ğŸ“° è·å–{code}çš„æ–°é—»...")
                news_aggregator = get_news_aggregator()
                news_data = news_aggregator.aggregate_news(
                    code,
                    include_tushare=False,  # Tushareæ–°é—»éœ€è¦5000ç§¯åˆ†
                    include_akshare=True,
                    limit_per_source=10
                )
                
                news_list_local = news_data.get('merged_news', [])
                stock_data["newsCount"] = news_data.get('total_count', 0)
                stock_data["latestNews"] = news_list_local[0]['title'] if news_list_local else ""
                
                # å…³é”®ä¿®å¤ï¼šå°†æ–°é—»æ·»åŠ åˆ°å…¨å±€news_listä¸­
                global news_list
                for news_item in news_list_local:
                    # æ·»åŠ ç›¸å…³è‚¡ç¥¨ä¿¡æ¯
                    news_with_stock = {
                        'id': f"{code}_{news_item.get('pub_time', '')}",
                        'title': news_item.get('title', ''),
                        'summary': news_item.get('content', '')[:200] if news_item.get('content') else '',
                        'publishTime': news_item.get('pub_time', ''),
                        'source': news_item.get('source', ''),
                        'relatedStocks': [code],
                        'sentiment': 0  # å°†åœ¨æƒ…ç»ªåˆ†æåæ›´æ–°
                    }
                    # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨ï¼ˆé¿å…é‡å¤ï¼‰
                    if not any(n.get('id') == news_with_stock['id'] for n in news_list):
                        news_list.append(news_with_stock)
                
                # ä¿æŒæœ€è¿‘100æ¡æ–°é—»
                if len(news_list) > 100:
                    news_list = news_list[-100:]
                
                logger.info(f"âœ… è·å–æ–°é—»æˆåŠŸ: {len(news_list_local)}æ¡ï¼Œå…¨å±€æ–°é—»æ€»æ•°: {len(news_list)}")
                
            except Exception as e:
                logger.error(f"âŒ è·å–æ–°é—»å¤±è´¥: {e}")
                await asyncio.sleep(0.1)  # ä¿æŒå¼‚æ­¥
        
        # 2. æƒ…ç»ªåˆ†æï¼ˆä½¿ç”¨çœŸå®çš„æƒ…ç»ªåˆ†æå¼•æ“ï¼‰
        sentiment_score = 50  # é»˜è®¤ä¸­æ€§
        if items.get("sentiment", False) and news_list_local:
            try:
                logger.info(f"ğŸ’­ åˆ†æ{code}çš„æƒ…ç»ª...")
                sentiment_engine = get_sentiment_engine()
                sentiment_result = sentiment_engine.analyze_news_list(news_list_local)
                
                sentiment_score = sentiment_result.get('overall_score', 50)
                stock_data["sentimentScore"] = sentiment_score
                stock_data["sentimentDetail"] = {
                    'overall': sentiment_result.get('overall_sentiment', 'neutral'),
                    'positive': sentiment_result.get('positive_count', 0),
                    'negative': sentiment_result.get('negative_count', 0),
                    'neutral': sentiment_result.get('neutral_count', 0)
                }
                
                # æ›´æ–°å…¨å±€news_listä¸­çš„æƒ…ç»ªåˆ†æ•°
                news_sentiments = sentiment_result.get('news_sentiments', [])
                for i, news_sentiment in enumerate(news_sentiments):
                    news_id = f"{code}_{news_list_local[i].get('pub_time', '')}"
                    for news_item in news_list:
                        if news_item.get('id') == news_id:
                            news_item['sentiment'] = news_sentiment.get('score', 50)
                            break
                
                logger.info(f"âœ… æƒ…ç»ªåˆ†æå®Œæˆ: {sentiment_score:.2f}åˆ† ({sentiment_result.get('overall_sentiment')})")
                logger.info(f"   æ­£é¢:{sentiment_result.get('positive_count')} ä¸­æ€§:{sentiment_result.get('neutral_count')} è´Ÿé¢:{sentiment_result.get('negative_count')}")
                
            except Exception as e:
                logger.error(f"âŒ æƒ…ç»ªåˆ†æå¤±è´¥: {e}")
                sentiment_score = 50
        elif items.get("sentiment", False):
            logger.warning(f"âš ï¸ æ— æ–°é—»æ•°æ®ï¼Œè·³è¿‡æƒ…ç»ªåˆ†æ")
            stock_data["sentimentScore"] = sentiment_score
        
        # 3. é£é™©åˆ†æï¼ˆä½¿ç”¨çœŸå®çš„é£é™©åˆ†æå¼•æ“ï¼‰
        if items.get("risk", False):
            try:
                logger.info(f"ğŸ” å¼€å§‹åˆ†æ{code}çš„é£é™©...")
                risk_result = analyze_stock_risk(
                    code, 
                    sentiment_score=sentiment_score
                )
                
                stock_data["riskLevel"] = risk_result.get("risk_level", "low")
                stock_data["riskScore"] = risk_result.get("risk_score", 0)
                stock_data["riskFactors"] = risk_result.get("risk_factors", {})
                stock_data["warnings"] = risk_result.get("warnings", [])
                
                logger.info(f"âœ… {code} é£é™©åˆ†æå®Œæˆ: {stock_data['riskLevel']} (å¾—åˆ†:{stock_data['riskScore']})")
                
            except Exception as e:
                logger.error(f"âŒ é£é™©åˆ†æå¤±è´¥ {code}: {e}")
                stock_data["riskLevel"] = "unknown"
        
        # 4. åœå¤ç‰Œç›‘æ§
        if items.get("suspend", False):
            try:
                logger.info(f"æ£€æŸ¥{code}çš„åœå¤ç‰ŒçŠ¶æ€...")
                suspend_status = check_suspend_status(code)
                
                stock_data["isSuspended"] = suspend_status.get("is_suspended", False)
                stock_data["suspendInfo"] = suspend_status
                
                if suspend_status.get("is_suspended"):
                    logger.warning(f"âš ï¸ {code} å½“å‰å¤„äºåœç‰ŒçŠ¶æ€")
                    
            except Exception as e:
                logger.error(f"âŒ åœå¤ç‰Œæ£€æŸ¥å¤±è´¥ {code}: {e}")
        
        # æ›´æ–°æ—¶é—´
        stock_data["lastUpdate"] = datetime.now().isoformat()

        # è·å–ç»¼åˆæ•°æ®å¹¶ä¿å­˜åˆ°æ•°æ®åº“
        try:
            logger.info(f"ğŸ“Š è·å–å¹¶ä¿å­˜ {code} çš„ç»¼åˆæ•°æ®åˆ°æ•°æ®åº“...")
            service = get_comprehensive_service()
            comprehensive_result = service.get_all_stock_data(code)

            # æ¸…ç†éæ³•floatå€¼
            comprehensive_result = sanitize_for_json(comprehensive_result)

            # å°†æƒ…ç»ªåˆ†æå’Œé£é™©åˆ†æç»“æœæ·»åŠ åˆ°ç»¼åˆæ•°æ®ä¸­
            comprehensive_result['overall_score'] = stock_data.get('sentimentScore', 50)
            comprehensive_result['sentiment_detail'] = stock_data.get('sentimentDetail', {})
            comprehensive_result['risk'] = {
                'risk_level': stock_data.get('riskLevel', 'low'),
                'risk_score': stock_data.get('riskScore', 0),
                'risk_factors': stock_data.get('riskFactors', {}),
                'warnings': stock_data.get('warnings', [])
            }

            # ä¿å­˜åˆ°å†…å­˜ç¼“å­˜
            cache_key = f"comprehensive_{code}"
            data_cache[cache_key] = {
                'cached_at': datetime.now().isoformat(),
                'data': comprehensive_result
            }

            # ä¿å­˜åˆ°æ•°æ®åº“
            with get_db_context() as db:
                StockDataService.save_or_update(
                    db=db,
                    ts_code=code,
                    data_type='comprehensive',
                    data=comprehensive_result,
                    source='mixed'
                )
                # æ›´æ–°ç›‘æ§è‚¡ç¥¨çš„æœ€åæ›´æ–°æ—¶é—´
                MonitoredStockService.update_last_update(db, code)
            logger.info(f"âœ… ç»¼åˆæ•°æ®å·²ä¿å­˜åˆ°æ•°æ®åº“: {code}")

        except Exception as e:
            logger.error(f"âŒ ä¿å­˜ç»¼åˆæ•°æ®å¤±è´¥ {code}: {e}")

        logger.info(f"âœ… å®Œæˆæ›´æ–°è‚¡ç¥¨æ•°æ®: {code}")
        
    except Exception as e:
        logger.error(f"âŒ æ›´æ–°è‚¡ç¥¨æ•°æ®å¤±è´¥ {code}: {e}")


# ==================== å®šæ—¶ä»»åŠ¡ï¼ˆç®€åŒ–ç¤ºä¾‹ï¼‰ ====================

async def scheduled_update_task():
    """
    å®šæ—¶æ›´æ–°ä»»åŠ¡ï¼ˆåº”è¯¥åœ¨åå°è¿è¡Œï¼‰
    """
    while True:
        try:
            current_time = datetime.now()
            
            for code, data in monitored_stocks.items():
                frequency = data.get("frequency", "1h")
                last_update = data.get("lastUpdate")
                
                if not last_update:
                    continue
                
                last_time = datetime.fromisoformat(last_update)
                
                # æ ¹æ®é¢‘ç‡è®¡ç®—æ˜¯å¦éœ€è¦æ›´æ–°
                intervals = {
                    "5m": 5,
                    "15m": 15,
                    "30m": 30,
                    "1h": 60,
                    "1d": 1440
                }
                
                interval_minutes = intervals.get(frequency, 60)

                if (current_time - last_time).total_seconds() >= interval_minutes * 60:
                    await update_stock_data(code)

            # æ¯åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡
            await asyncio.sleep(60)

        except Exception as e:
            logger.error(f"å®šæ—¶ä»»åŠ¡å‡ºé”™: {e}")
            await asyncio.sleep(60)


# ==================== æ¥å£æµ‹è¯• ====================

@router.get("/interfaces/test/stream")
async def test_interfaces_stream():
    """
    æµå¼æµ‹è¯•æ‰€æœ‰æ•°æ®æ¥å£ï¼ˆSSEï¼‰
    é€ä¸ªæµ‹è¯•å„æ•°æ®æºçš„æ¥å£ï¼Œå®æ—¶è¿”å›æµ‹è¯•ç»“æœ
    """
    import time

    async def generate():
        try:
            # å®šä¹‰è¦æµ‹è¯•çš„æ¥å£
            interfaces = {
                'tushare': {
                    'name': 'Tushare',
                    'icon': 'ğŸ“Š',
                    'interfaces': [
                        {'id': 'tushare_daily', 'name': 'æ—¥çº¿æ•°æ®', 'category': 'è¡Œæƒ…æ•°æ®', 'test_func': 'test_tushare_daily'},
                        {'id': 'tushare_income', 'name': 'åˆ©æ¶¦è¡¨', 'category': 'è´¢åŠ¡æ•°æ®', 'test_func': 'test_tushare_income'},
                        {'id': 'tushare_suspend', 'name': 'åœå¤ç‰Œ', 'category': 'å…¬å‘Šæ•°æ®', 'test_func': 'test_tushare_suspend'},
                        {'id': 'tushare_pledge', 'name': 'è‚¡æƒè´¨æŠ¼', 'category': 'é£é™©æ•°æ®', 'test_func': 'test_tushare_pledge'},
                    ]
                },
                'akshare': {
                    'name': 'AKShare',
                    'icon': 'ğŸ”—',
                    'interfaces': [
                        {'id': 'akshare_spot', 'name': 'å®æ—¶è¡Œæƒ…', 'category': 'è¡Œæƒ…æ•°æ®', 'test_func': 'test_akshare_spot'},
                        {'id': 'akshare_news', 'name': 'ä¸ªè‚¡æ–°é—»', 'category': 'æ–°é—»æ•°æ®', 'test_func': 'test_akshare_news'},
                        {'id': 'akshare_st', 'name': 'STè‚¡ç¥¨', 'category': 'é£é™©æ•°æ®', 'test_func': 'test_akshare_st'},
                        {'id': 'akshare_block', 'name': 'å¤§å®—äº¤æ˜“', 'category': 'äº¤æ˜“æ•°æ®', 'test_func': 'test_akshare_block'},
                    ]
                },
                'eastmoney': {
                    'name': 'ä¸œæ–¹è´¢å¯Œ',
                    'icon': 'ğŸ’°',
                    'interfaces': [
                        {'id': 'em_realtime', 'name': 'å®æ—¶è¡Œæƒ…', 'category': 'è¡Œæƒ…æ•°æ®', 'test_func': 'test_em_realtime'},
                        {'id': 'em_news', 'name': 'è´¢ç»æ–°é—»', 'category': 'æ–°é—»æ•°æ®', 'test_func': 'test_em_news'},
                    ]
                }
            }

            # è®¡ç®—æ€»æ¥å£æ•°
            total = sum(len(source['interfaces']) for source in interfaces.values())

            # å‘é€å¼€å§‹ä¿¡å·
            yield f"data: {json.dumps({'type': 'start', 'total': total, 'sources': list(interfaces.keys())})}\n\n"

            progress = 0
            success_count = 0

            # æµ‹è¯•å‡½æ•°æ˜ å°„
            async def test_tushare_daily():
                import os
                token = os.getenv('TUSHARE_TOKEN')
                if not token:
                    return False, 'TUSHARE_TOKENæœªé…ç½®'
                import tushare as ts
                ts.set_token(token)
                pro = ts.pro_api()
                df = pro.daily(ts_code='000001.SZ', start_date='20250101', end_date='20250102')
                return df is not None and not df.empty, f'{len(df)}æ¡æ•°æ®' if df is not None else 'æ— æ•°æ®'

            async def test_tushare_income():
                import os
                token = os.getenv('TUSHARE_TOKEN')
                if not token:
                    return False, 'TUSHARE_TOKENæœªé…ç½®'
                import tushare as ts
                ts.set_token(token)
                pro = ts.pro_api()
                df = pro.income(ts_code='000001.SZ')
                return df is not None and not df.empty, f'{len(df)}æ¡æ•°æ®' if df is not None else 'æ— æ•°æ®'

            async def test_tushare_suspend():
                import os
                token = os.getenv('TUSHARE_TOKEN')
                if not token:
                    return False, 'TUSHARE_TOKENæœªé…ç½®'
                import tushare as ts
                ts.set_token(token)
                pro = ts.pro_api()
                df = pro.suspend_d(ts_code='000001.SZ')
                return True, 'æ¥å£å¯ç”¨'

            async def test_tushare_pledge():
                import os
                token = os.getenv('TUSHARE_TOKEN')
                if not token:
                    return False, 'TUSHARE_TOKENæœªé…ç½®'
                import tushare as ts
                ts.set_token(token)
                pro = ts.pro_api()
                df = pro.pledge_stat(ts_code='000001.SZ')
                return df is not None and not df.empty, f'{len(df)}æ¡æ•°æ®' if df is not None else 'æ— æ•°æ®'

            async def test_akshare_spot():
                import akshare as ak
                df = ak.stock_zh_a_spot_em()
                return df is not None and not df.empty, f'{len(df)}åªè‚¡ç¥¨'

            async def test_akshare_news():
                import akshare as ak
                df = ak.stock_news_em(symbol='000001')
                return df is not None and not df.empty, f'{len(df)}æ¡æ–°é—»'

            async def test_akshare_st():
                import akshare as ak
                df = ak.stock_zh_a_st_em()
                return df is not None and not df.empty, f'{len(df)}åªSTè‚¡ç¥¨'

            async def test_akshare_block():
                import akshare as ak
                df = ak.stock_dzjy_sctj()
                return df is not None and not df.empty, f'{len(df)}æ¡è®°å½•'

            async def test_em_realtime():
                import akshare as ak
                df = ak.stock_zh_a_spot_em()
                return df is not None and not df.empty, f'{len(df)}åªè‚¡ç¥¨'

            async def test_em_news():
                import akshare as ak
                df = ak.stock_info_global_em()
                return df is not None and not df.empty, f'{len(df)}æ¡æ–°é—»'

            test_funcs = {
                'test_tushare_daily': test_tushare_daily,
                'test_tushare_income': test_tushare_income,
                'test_tushare_suspend': test_tushare_suspend,
                'test_tushare_pledge': test_tushare_pledge,
                'test_akshare_spot': test_akshare_spot,
                'test_akshare_news': test_akshare_news,
                'test_akshare_st': test_akshare_st,
                'test_akshare_block': test_akshare_block,
                'test_em_realtime': test_em_realtime,
                'test_em_news': test_em_news,
            }

            # é€ä¸ªæ•°æ®æºæµ‹è¯•
            for source_key, source_info in interfaces.items():
                # å‘é€æ•°æ®æºå¼€å§‹ä¿¡å·
                yield f"data: {json.dumps({'type': 'source_start', 'source': source_key, 'name': source_info['name'], 'icon': source_info['icon'], 'count': len(source_info['interfaces'])})}\n\n"

                source_success = 0
                source_fail = 0

                for iface in source_info['interfaces']:
                    # å‘é€æµ‹è¯•å¼€å§‹ä¿¡å·
                    yield f"data: {json.dumps({'type': 'test_start', 'source': source_key, 'interface_id': iface['id'], 'name': iface['name'], 'category': iface['category']})}\n\n"

                    start_time = time.time()
                    try:
                        test_func = test_funcs.get(iface['test_func'])
                        if test_func:
                            success, message = await test_func()
                            elapsed = round(time.time() - start_time, 2)
                            status = 'success' if success else 'error'
                            if success:
                                source_success += 1
                                success_count += 1
                            else:
                                source_fail += 1
                        else:
                            elapsed = 0
                            status = 'not_implemented'
                            message = 'æµ‹è¯•å‡½æ•°æœªå®ç°'
                            source_fail += 1
                    except Exception as e:
                        elapsed = round(time.time() - start_time, 2)
                        status = 'error'
                        message = str(e)[:100]
                        source_fail += 1

                    progress += 1

                    # å‘é€æµ‹è¯•ç»“æœ
                    yield f"data: {json.dumps({'type': 'test_result', 'source': source_key, 'interface_id': iface['id'], 'status': status, 'elapsed': elapsed, 'message': message, 'progress': progress})}\n\n"

                    # çŸ­æš‚å»¶è¿Ÿé¿å…è¿‡å¿«
                    await asyncio.sleep(0.1)

                # å‘é€æ•°æ®æºå®Œæˆä¿¡å·
                yield f"data: {json.dumps({'type': 'source_complete', 'source': source_key, 'name': source_info['name'], 'success': source_success, 'fail': source_fail})}\n\n"

            # å‘é€å®Œæˆä¿¡å·
            success_rate = round(success_count / total * 100, 1) if total > 0 else 0
            yield f"data: {json.dumps({'type': 'complete', 'total': total, 'success': success_count, 'fail': total - success_count, 'success_rate': success_rate})}\n\n"

        except Exception as e:
            logger.error(f"æ¥å£æµ‹è¯•å¤±è´¥: {e}")
            import traceback
            logger.error(traceback.format_exc())
            yield f"data: {json.dumps({'type': 'error', 'error': str(e)})}\n\n"

    return StreamingResponse(
        generate(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "Access-Control-Allow-Origin": "*",
            "Access-Control-Allow-Headers": "*",
        }
    )
