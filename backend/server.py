"""
IcySaint AI - Python åç«¯æœåŠ¡å™¨
ä½¿ç”¨ FastAPI æ¡†æ¶æ›¿ä»£ Vercel Serverless Functions
"""

import os
import sys
# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, project_root)

import json
import asyncio
from typing import Optional, Dict, Any, List
from contextlib import asynccontextmanager
from fastapi import FastAPI, HTTPException, BackgroundTasks, Query
from fastapi.middleware.cors import CORSMiddleware
from fastapi.staticfiles import StaticFiles
from fastapi.responses import FileResponse, JSONResponse
from pydantic import BaseModel, ConfigDict, Field
from datetime import datetime, date
import httpx
from dotenv import load_dotenv
import uvicorn
from asyncio import Semaphore

# å¯¼å…¥é™çº§å¤„ç†å™¨
from backend.utils.llm_fallback_handler import get_fallback_handler

# å…¨å±€å¹¶å‘æ§åˆ¶å™¨ - é™åˆ¶åŒæ—¶å‘é€åˆ°SiliconFlowçš„è¯·æ±‚æ•°
# å¢åŠ åˆ°20ä¸ªå¹¶å‘ï¼Œé¿å…åˆ†ææ—¶é˜»å¡å…¶ä»–åŠŸèƒ½
siliconflow_semaphore = Semaphore(20)  # æœ€å¤š20ä¸ªå¹¶å‘è¯·æ±‚

# æ¨¡å‹èƒ½åŠ›ç”»åƒä¸é™é»˜å‹æµ‹çš„å…¨å±€çŠ¶æ€ä¸ç»“æœæ–‡ä»¶
CALIBRATION_RESULTS_FILE = os.path.join(os.path.dirname(__file__), "model_calibration.json")
calibration_state = {
    "status": "idle",       # idle / running / completed / error
    "lastRunAt": None,
    "error": None,
    "results": {},           # {model_name: {provider, channel, tests: [...]}}
    "settings": None         # æœ€è¿‘ä¸€æ¬¡å‹æµ‹ä½¿ç”¨çš„é…ç½®å¿«ç…§
}


def save_calibration_state():
    """å°†å½“å‰å‹æµ‹çŠ¶æ€ä¿å­˜åˆ°æ–‡ä»¶ï¼Œæ–¹ä¾¿å‰ç«¯æˆ–é‡å¯åæŸ¥çœ‹"""
    try:
        with open(CALIBRATION_RESULTS_FILE, "w", encoding="utf-8") as f:
            json.dump(calibration_state, f, ensure_ascii=False, indent=2)
    except Exception as e:
        print(f"[Calibration] ä¿å­˜ç»“æœå¤±è´¥: {str(e)}")


def load_calibration_state_from_file():
    """ä»æ–‡ä»¶åŠ è½½æœ€è¿‘ä¸€æ¬¡å‹æµ‹çŠ¶æ€ï¼ˆå¦‚æœå­˜åœ¨ï¼‰"""
    global calibration_state
    try:
        if os.path.exists(CALIBRATION_RESULTS_FILE):
            with open(CALIBRATION_RESULTS_FILE, "r", encoding="utf-8") as f:
                data = json.load(f)
            if isinstance(data, dict):
                calibration_state.update(data)
    except Exception as e:
        print(f"[Calibration] åŠ è½½ç»“æœå¤±è´¥: {str(e)}")


# æ¨¡å—åŠ è½½æ—¶å°è¯•æ¢å¤ä¸€æ¬¡å†å²å‹æµ‹ç»“æœ
load_calibration_state_from_file()

# åŠ è½½ç¯å¢ƒå˜é‡ - æ˜ç¡®æŒ‡å®š.envæ–‡ä»¶è·¯å¾„
from pathlib import Path
env_file = Path(__file__).parent.parent / '.env'
if env_file.exists():
    load_dotenv(env_file)
    print(f"âœ… åŠ è½½ç¯å¢ƒå˜é‡æ–‡ä»¶: {env_file}")
else:
    load_dotenv()  # å°è¯•é»˜è®¤åŠ è½½
    print("âš ï¸ ä½¿ç”¨é»˜è®¤ç¯å¢ƒå˜é‡åŠ è½½")

# å¯¼å…¥APIè·¯ç”±
from backend.api.news_api import router as news_router
from backend.api.debate_api import router as debate_router
from backend.api.trading_api import router as trading_router
from backend.api.verification_api import router as verification_router
from backend.api.agents_api import router as agents_router
from backend.api.agent_config_api import router as agent_config_router
from backend.api.unified_news_api_endpoint import router as unified_news_router
from backend.api.documents_api import router as documents_router
from backend.api.akshare_data_api import router as akshare_router
from backend.api.agent_logs_api import router as agent_logs_router
from backend.api.analysis_session_api import router as analysis_session_router  # åˆ†æä¼šè¯ API
from backend.api.analysis_session_db_api import router as analysis_session_db_router  # æ•°æ®åº“ç‰ˆä¼šè¯ API
from backend.api.backtest_api import router as backtest_router  # å›æµ‹API
from backend.api.strategy_api import router as strategy_router  # ç­–ç•¥API
from backend.api.llm_config_api import router as llm_config_router  # LLMé…ç½®APIï¼ˆæ™ºèƒ½åˆ†æï¼‰
from backend.api.trading_llm_config_api import router as trading_llm_config_router  # äº¤æ˜“LLMé…ç½®APIï¼ˆæ–°åŠŸèƒ½ï¼‰
from backend.api.strategy_selection_api import router as strategy_selection_router  # æ™ºèƒ½ç­–ç•¥é€‰æ‹©API
from backend.api.auto_trading_api import router as auto_trading_router  # è‡ªåŠ¨äº¤æ˜“API
from backend.api.tracking_api import router as tracking_router  # æŒç»­è·Ÿè¸ªAPI
from backend.api.verification_api import router as verification_router  # éªŒè¯æŠ¥å‘ŠAPI
from backend.api.kline_api import router as kline_router  # Kçº¿API
from backend.api.scheduler_api import router as scheduler_router  # è°ƒåº¦å™¨API
from backend.api.data_source_health_api import router as data_source_health_router  # æ•°æ®æºå¥åº·æ£€æŸ¥API
from backend.api.dataflow_api import router as dataflow_router  # æ•°æ®æµç›‘æ§API
from backend.api.notification_api import router as notification_router  # é€šçŸ¥æœåŠ¡API
from backend.api.sse_api import router as sse_router  # SSEå®æ—¶æ¨é€API
from backend.api.async_analysis_api import router as async_analysis_router  # å¼‚æ­¥åˆ†æAPI
from backend.api.websocket_api import router as websocket_router  # WebSocketå®æ—¶é€šçŸ¥API
from backend.api.providers_api import router as providers_router  # æ•°æ®æºProvider API (TDX/Wencai/TA-Lib)
from backend.api.report_api import router as report_router  # PDFæŠ¥å‘Šç”ŸæˆAPI
from backend.api.longhubang_api import router as longhubang_router  # é¾™è™æ¦œåˆ†æAPI
from backend.api.wencai_api import router as wencai_router  # é—®è´¢æ™ºèƒ½é€‰è‚¡API
from backend.api.sector_rotation_api import router as sector_rotation_router  # æ¿å—è½®åŠ¨åˆ†æAPI
from backend.api.sentiment_api import router as sentiment_router  # å¸‚åœºæƒ…ç»ªåˆ†æAPI
from backend.api.realtime_monitor_api import router as realtime_monitor_router  # å®æ—¶ç›¯ç›˜ç›‘æ§API
from backend.api.market_data_api import router as market_data_router  # å¸‚åœºæ•°æ®APIï¼ˆç›˜å£ã€æ’è¡Œæ¦œç­‰ï¼‰
from backend.api.news_center_api import router as news_center_router  # ç»Ÿä¸€æ–°é—»ç›‘æ§ä¸­å¿ƒAPI
from backend.api.cninfo_api import router as cninfo_router  # å·¨æ½®èµ„è®¯ç½‘å®˜æ–¹API
from backend.api.system_api import router as system_router  # ç³»ç»Ÿè®¾ç½®API
from backend.api.api_monitor_api import router as api_monitor_router  # APIç›‘æ§API
from backend.api.datasource_api import router as datasource_router  # æ•°æ®æºè°ƒåº¦å™¨API

# ==================== é…ç½® ====================

# API Keys ä»ç¯å¢ƒå˜é‡è¯»å–
API_KEYS = {
    "gemini": os.getenv("GEMINI_API_KEY", ""),
    "deepseek": os.getenv("DEEPSEEK_API_KEY", ""),
    "qwen": os.getenv("DASHSCOPE_API_KEY", "") or os.getenv("QWEN_API_KEY", ""),  # æ”¯æŒä¸¤ç§ç¯å¢ƒå˜é‡å
    "siliconflow": os.getenv("SILICONFLOW_API_KEY", ""),
    "juhe": os.getenv("JUHE_API_KEY", ""),
    "finnhub": os.getenv("FINNHUB_API_KEY", ""),
    "tushare": os.getenv("TUSHARE_TOKEN", ""),
    "cninfo_access_key": os.getenv("CNINFO_ACCESS_KEY", ""),
    "cninfo_access_secret": os.getenv("CNINFO_ACCESS_SECRET", ""),
    "cninfo_access_token": os.getenv("CNINFO_ACCESS_TOKEN", "")
}

# API ç«¯ç‚¹
API_ENDPOINTS = {
    "gemini": "https://generativelanguage.googleapis.com/v1beta/models",
    "deepseek": "https://api.deepseek.com/chat/completions",
    "qwen": "https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions",
    "siliconflow": "https://api.siliconflow.cn/v1/chat/completions",
    "siliconflow_models": "https://api.siliconflow.cn/v1/models",
    "juhe": "http://web.juhe.cn/finance/stock/hs"
}

# ==================== HTTPè¿æ¥æ±  ====================
# å…¨å±€HTTPå®¢æˆ·ç«¯è¿æ¥æ± ï¼Œé¿å…é‡å¤åˆ›å»ºè¿æ¥
http_clients = {}

@asynccontextmanager
async def lifespan(app: FastAPI):
    """åº”ç”¨ç”Ÿå‘½å‘¨æœŸç®¡ç†"""
    # å¯åŠ¨æ—¶åˆå§‹åŒ–è¿æ¥æ± 
    global http_clients
    
    # é€šç”¨è¿æ¥é™åˆ¶é…ç½®
    limits = httpx.Limits(
        max_keepalive_connections=20,  # ä¿æŒæ´»åŠ¨è¿æ¥æ•°
        max_connections=50,            # æœ€å¤§è¿æ¥æ•°
        keepalive_expiry=30            # è¿æ¥ä¿æŒæ—¶é—´ï¼ˆç§’ï¼‰
    )
    
    # AI APIçš„è¶…æ—¶é…ç½®ï¼ˆéœ€è¦é•¿æ—¶é—´ï¼‰
    # æ³¨æ„ï¼šhttpxä¸æ”¯æŒtotalå‚æ•°ï¼Œä½¿ç”¨timeoutå‚æ•°ä»£æ›¿
    ai_timeout = httpx.Timeout(
        connect=5.0,      # è¿æ¥è¶…æ—¶ï¼šå»ºç«‹TCPè¿æ¥çš„æ—¶é—´
        read=180.0,       # è¯»å–è¶…æ—¶ï¼š3åˆ†é’Ÿï¼Œé€‚åº”AIé•¿å“åº”
        write=10.0,       # å†™å…¥è¶…æ—¶ï¼šå‘é€è¯·æ±‚çš„æ—¶é—´
        pool=5.0          # è¿æ¥æ± è¶…æ—¶ï¼šè·å–è¿æ¥çš„ç­‰å¾…æ—¶é—´
    )
    
    # æ™®é€šAPIçš„è¶…æ—¶é…ç½®ï¼ˆè‚¡ç¥¨æ•°æ®ç­‰ï¼‰
    normal_timeout = httpx.Timeout(
        connect=5.0,      
        read=30.0,        # æ™®é€šAPI 30ç§’è¶³å¤Ÿ
        write=10.0,       
        pool=5.0         
    )
    
    # ä¸ºæ¯ä¸ªAPIæœåŠ¡åˆ›å»ºä¸“ç”¨å®¢æˆ·ç«¯
    http_clients['gemini'] = httpx.AsyncClient(
        limits=limits,
        timeout=ai_timeout,  # AI APIä½¿ç”¨é•¿è¶…æ—¶
        verify=True
    )
    
    http_clients['deepseek'] = httpx.AsyncClient(
        limits=limits,
        timeout=ai_timeout,  # AI APIä½¿ç”¨é•¿è¶…æ—¶
        verify=True
    )
    
    http_clients['qwen'] = httpx.AsyncClient(
        limits=limits,
        timeout=ai_timeout,  # AI APIä½¿ç”¨é•¿è¶…æ—¶
        verify=True
    )
    
    http_clients['siliconflow'] = httpx.AsyncClient(
        limits=limits,
        timeout=ai_timeout,  # AI APIä½¿ç”¨é•¿è¶…æ—¶
        verify=True
    )
    
    # è‚¡ç¥¨APIä¸“ç”¨å®¢æˆ·ç«¯ï¼ˆä½¿ç”¨æ™®é€šè¶…æ—¶ï¼‰
    http_clients['juhe'] = httpx.AsyncClient(
        limits=limits,
        timeout=normal_timeout,  # è‚¡ç¥¨APIä½¿ç”¨çŸ­è¶…æ—¶
        verify=True
    )
    
    # é€šç”¨å®¢æˆ·ç«¯ï¼ˆç”¨äºå…¶ä»–è¯·æ±‚ï¼‰
    http_clients['default'] = httpx.AsyncClient(
        limits=limits,
        timeout=ai_timeout,  # é»˜è®¤ä½¿ç”¨AIè¶…æ—¶é…ç½®
        verify=True
    )
    
    print("âœ… HTTPè¿æ¥æ± åˆå§‹åŒ–æˆåŠŸ")

    # åˆå§‹åŒ– Redis è¿æ¥ï¼ˆç”¨äºå¼‚æ­¥ä»»åŠ¡å’ŒSSEï¼‰
    try:
        from backend.services.async_task.redis_client import redis_client
        await redis_client.connect()
        if redis_client.is_redis_available:
            print("âœ… Redis è¿æ¥æˆåŠŸ")
        else:
            print("âš ï¸ Redis ä¸å¯ç”¨ï¼Œä½¿ç”¨å†…å­˜é™çº§æ¨¡å¼")
    except Exception as e:
        print(f"âš ï¸ Redis åˆå§‹åŒ–å¤±è´¥: {e}ï¼Œä½¿ç”¨å†…å­˜é™çº§æ¨¡å¼")

    # å¯åŠ¨äº¤æ˜“è°ƒåº¦å™¨ï¼ˆå¯é€‰ï¼Œæ ¹æ®ç¯å¢ƒå˜é‡æ§åˆ¶ï¼‰
    import os
    if os.getenv("ENABLE_SCHEDULER", "false").lower() == "true":
        try:
            from backend.services.scheduler_service import start_scheduler
            start_scheduler()
            print("âœ… äº¤æ˜“è°ƒåº¦å™¨å·²å¯åŠ¨")
        except Exception as e:
            print(f"âš ï¸ äº¤æ˜“è°ƒåº¦å™¨å¯åŠ¨å¤±è´¥: {e}")
    
    # å¯åŠ¨æ•°æ®æ¸…ç†è°ƒåº¦å™¨ï¼ˆé»˜è®¤å¯åŠ¨ï¼‰
    if os.getenv("ENABLE_DATA_CLEANUP", "true").lower() == "true":
        try:
            from backend.dataflows.data_cleanup_scheduler import start_cleanup_scheduler
            start_cleanup_scheduler()
            print("âœ… æ•°æ®æ¸…ç†è°ƒåº¦å™¨å·²å¯åŠ¨")
        except Exception as e:
            print(f"âš ï¸ æ•°æ®æ¸…ç†è°ƒåº¦å™¨å¯åŠ¨å¤±è´¥: {e}")

    # å¯åŠ¨åå°æ–°é—»æœåŠ¡ï¼ˆé»˜è®¤å¯åŠ¨ï¼‰
    # è¯¥æœåŠ¡ä½¿ç”¨ç‹¬ç«‹è¿›ç¨‹æ± å¤„ç†æ–°é—»è¯·æ±‚ï¼Œå®Œå…¨ä¸é˜»å¡FastAPIä¸»äº‹ä»¶å¾ªç¯
    if os.getenv("ENABLE_BACKGROUND_NEWS", "true").lower() == "true":
        try:
            from backend.services.background_news_service import background_news_service
            await background_news_service.start()
            print("âœ… åå°æ–°é—»æœåŠ¡å·²å¯åŠ¨ï¼ˆç‹¬ç«‹è¿›ç¨‹æ± æ¨¡å¼ï¼‰")
        except Exception as e:
            print(f"âš ï¸ åå°æ–°é—»æœåŠ¡å¯åŠ¨å¤±è´¥: {e}")

    # å¯åŠ¨ç»Ÿä¸€åå°æ•°æ®æ›´æ–°æœåŠ¡ï¼ˆé»˜è®¤ç¦ç”¨ï¼‰
    # è¯¥æœåŠ¡ä¼šè‡ªåŠ¨ä¸ºæ‰€æœ‰ç›‘æ§è‚¡ç¥¨è°ƒç”¨48ä¸ªæ¥å£ï¼Œéå¸¸è€—æ—¶
    # å¦‚éœ€å¯ç”¨ï¼Œè¯·è®¾ç½®ç¯å¢ƒå˜é‡ ENABLE_DATA_UPDATE_SERVICE=true
    if os.getenv("ENABLE_DATA_UPDATE_SERVICE", "false").lower() == "true":
        try:
            from backend.services.unified_data_update_service import start_background_update_service
            start_background_update_service()
            print("âœ… ç»Ÿä¸€åå°æ•°æ®æ›´æ–°æœåŠ¡å·²å¯åŠ¨")
        except Exception as e:
            print(f"âš ï¸ ç»Ÿä¸€åå°æ•°æ®æ›´æ–°æœåŠ¡å¯åŠ¨å¤±è´¥: {e}")

    # å¯åŠ¨å®æ—¶ç›¯ç›˜ç›‘æ§æœåŠ¡ï¼ˆé»˜è®¤ç¦ç”¨ï¼‰
    # è¯¥æœåŠ¡æä¾›å®æ—¶è‚¡ç¥¨ç›‘æ§å’ŒAIå†³ç­–åŠŸèƒ½
    # å¦‚éœ€å¯ç”¨ï¼Œè¯·è®¾ç½®ç¯å¢ƒå˜é‡ ENABLE_REALTIME_MONITOR=true
    if os.getenv("ENABLE_REALTIME_MONITOR", "false").lower() == "true":
        try:
            from backend.services.realtime_monitor_service import get_realtime_monitor_service
            realtime_monitor = get_realtime_monitor_service()
            # å°è¯•åŠ è½½ä¿å­˜çš„é…ç½®å¹¶è‡ªåŠ¨å¯åŠ¨
            realtime_monitor.load_config()
            if realtime_monitor.config.get("auto_start", False):
                realtime_monitor.start_monitoring()
                print("âœ… å®æ—¶ç›¯ç›˜ç›‘æ§æœåŠ¡å·²å¯åŠ¨ï¼ˆè‡ªåŠ¨å¯åŠ¨æ¨¡å¼ï¼‰")
            else:
                print("âœ… å®æ—¶ç›¯ç›˜ç›‘æ§æœåŠ¡å·²åˆå§‹åŒ–ï¼ˆç­‰å¾…æ‰‹åŠ¨å¯åŠ¨ï¼‰")
        except Exception as e:
            print(f"âš ï¸ å®æ—¶ç›¯ç›˜ç›‘æ§æœåŠ¡å¯åŠ¨å¤±è´¥: {e}")

    # å¯åŠ¨ç»Ÿä¸€æ–°é—»ç›‘æ§ä¸­å¿ƒï¼ˆé»˜è®¤å¯åŠ¨ï¼‰
    # è¯¥æœåŠ¡æ•´åˆæ‰€æœ‰æ–°é—»æ•°æ®æºï¼Œæä¾›ç»Ÿä¸€ç¼“å­˜å’Œå®æ—¶æ¨é€
    if os.getenv("ENABLE_NEWS_MONITOR_CENTER", "true").lower() == "true":
        try:
            from backend.services.news_center import get_news_monitor_center
            news_monitor = get_news_monitor_center()
            await news_monitor.start()
            print("âœ… ç»Ÿä¸€æ–°é—»ç›‘æ§ä¸­å¿ƒå·²å¯åŠ¨")
        except Exception as e:
            print(f"âš ï¸ ç»Ÿä¸€æ–°é—»ç›‘æ§ä¸­å¿ƒå¯åŠ¨å¤±è´¥: {e}")

    # å¯åŠ¨TDXæ•°æ®ç¼“å­˜æœåŠ¡ï¼ˆé»˜è®¤å¯åŠ¨ï¼‰
    # è¯¥æœåŠ¡åœ¨åå°å®šæ—¶è·å–TDXæ•°æ®ï¼Œç¼“å­˜åˆ°æœåŠ¡å™¨ç«¯æ–‡ä»¶
    # æ‰€æœ‰APIè¯·æ±‚ç›´æ¥è¯»å–ç¼“å­˜ï¼Œä¸é˜»å¡ç”¨æˆ·è¯·æ±‚
    if os.getenv("ENABLE_TDX_CACHE_SERVICE", "true").lower() == "true":
        try:
            from backend.services.tdx_cache_service import get_tdx_cache_service
            tdx_cache = get_tdx_cache_service()
            tdx_cache.start()
            print("âœ… TDXæ•°æ®ç¼“å­˜æœåŠ¡å·²å¯åŠ¨")
        except Exception as e:
            print(f"âš ï¸ TDXæ•°æ®ç¼“å­˜æœåŠ¡å¯åŠ¨å¤±è´¥: {e}")

    # yield æ§åˆ¶æƒç»™åº”ç”¨
    yield

    # åœæ­¢è°ƒåº¦å™¨
    try:
        from backend.services.scheduler_service import stop_scheduler
        stop_scheduler()
        print("âœ… äº¤æ˜“è°ƒåº¦å™¨å·²åœæ­¢")
    except:
        pass

    # åœæ­¢æ•°æ®æ¸…ç†è°ƒåº¦å™¨
    try:
        from backend.dataflows.data_cleanup_scheduler import stop_cleanup_scheduler
        stop_cleanup_scheduler()
        print("âœ… æ•°æ®æ¸…ç†è°ƒåº¦å™¨å·²åœæ­¢")
    except:
        pass

    # åœæ­¢åå°æ–°é—»æœåŠ¡
    try:
        from backend.services.background_news_service import background_news_service
        await background_news_service.stop()
        print("âœ… åå°æ–°é—»æœåŠ¡å·²åœæ­¢")
    except:
        pass

    # åœæ­¢ç»Ÿä¸€åå°æ•°æ®æ›´æ–°æœåŠ¡
    try:
        from backend.services.unified_data_update_service import stop_background_update_service
        stop_background_update_service()
        print("âœ… ç»Ÿä¸€åå°æ•°æ®æ›´æ–°æœåŠ¡å·²åœæ­¢")
    except:
        pass

    # åœæ­¢å®æ—¶ç›¯ç›˜ç›‘æ§æœåŠ¡
    try:
        from backend.services.realtime_monitor_service import get_realtime_monitor_service
        realtime_monitor = get_realtime_monitor_service()
        if realtime_monitor.is_running:
            realtime_monitor.stop_monitoring()
            print("âœ… å®æ—¶ç›¯ç›˜ç›‘æ§æœåŠ¡å·²åœæ­¢")
    except:
        pass

    # åœæ­¢ç»Ÿä¸€æ–°é—»ç›‘æ§ä¸­å¿ƒ
    try:
        from backend.services.news_center import get_news_monitor_center
        news_monitor = get_news_monitor_center()
        await news_monitor.stop()
        print("âœ… ç»Ÿä¸€æ–°é—»ç›‘æ§ä¸­å¿ƒå·²åœæ­¢")
    except:
        pass

    # åœæ­¢TDXæ•°æ®ç¼“å­˜æœåŠ¡
    try:
        from backend.services.tdx_cache_service import get_tdx_cache_service
        tdx_cache = get_tdx_cache_service()
        tdx_cache.stop()
        print("âœ… TDXæ•°æ®ç¼“å­˜æœåŠ¡å·²åœæ­¢")
    except:
        pass

    # å…³é—­ Redis è¿æ¥
    try:
        from backend.services.async_task.redis_client import redis_client
        await redis_client.disconnect()
        print("âœ… Redis è¿æ¥å·²å…³é—­")
    except:
        pass

    # å…³é—­æ—¶æ¸…ç†è¿æ¥æ± 
    for name, client in http_clients.items():
        await client.aclose()
        print(f"âœ… å…³é—­ {name} è¿æ¥æ± ")

    http_clients.clear()
    print("âœ… æ‰€æœ‰HTTPè¿æ¥æ± å·²å…³é—­")

# åˆ›å»º FastAPI åº”ç”¨ï¼ˆä½¿ç”¨æ–°çš„lifespanï¼‰
app = FastAPI(
    title="IcySaint AI Backend",
    version="1.0.0",
    lifespan=lifespan
)

# é…ç½® CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # å…è®¸æ‰€æœ‰æºï¼ŒåŒ…æ‹¬Vueå¼€å‘æœåŠ¡å™¨
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# æ³¨å†ŒAPIè·¯ç”±
app.include_router(news_router)
app.include_router(debate_router)
app.include_router(trading_router)
app.include_router(verification_router)
app.include_router(agents_router)
app.include_router(agent_config_router)  # æ™ºèƒ½ä½“é…ç½®API
app.include_router(unified_news_router)  # ç»Ÿä¸€æ–°é—»API
app.include_router(documents_router)  # æ–‡æ¡£API
app.include_router(akshare_router)  # AKShareæ•°æ® API
app.include_router(agent_logs_router)  # æ™ºèƒ½ä½“æ—¥å¿—æµAPI
app.include_router(analysis_session_router)  # åˆ†æä¼šè¯ API
app.include_router(analysis_session_db_router)  # æ•°æ®åº“ç‰ˆä¼šè¯ API
app.include_router(backtest_router)  # å›æµ‹API
app.include_router(strategy_router)  # ç­–ç•¥API
app.include_router(llm_config_router)  # LLMé…ç½®APIï¼ˆæ™ºèƒ½åˆ†æï¼‰
app.include_router(trading_llm_config_router)  # äº¤æ˜“LLMé…ç½®APIï¼ˆæ–°åŠŸèƒ½ï¼‰
app.include_router(strategy_selection_router)  # æ™ºèƒ½ç­–ç•¥é€‰æ‹©API
app.include_router(auto_trading_router)  # è‡ªåŠ¨äº¤æ˜“API
app.include_router(tracking_router)  # æŒç»­è·Ÿè¸ªAPI
app.include_router(verification_router)  # éªŒè¯æŠ¥å‘ŠAPI
app.include_router(kline_router)  # Kçº¿API
app.include_router(scheduler_router)  # è°ƒåº¦å™¨API
app.include_router(data_source_health_router)  # æ•°æ®æºå¥åº·æ£€æŸ¥API
app.include_router(dataflow_router)  # æ•°æ®æµç›‘æ§API
app.include_router(notification_router)  # é€šçŸ¥æœåŠ¡API
app.include_router(sse_router)  # SSEå®æ—¶æ¨é€API
app.include_router(async_analysis_router)  # å¼‚æ­¥åˆ†æAPI
app.include_router(websocket_router)  # WebSocketå®æ—¶é€šçŸ¥API
app.include_router(providers_router)  # æ•°æ®æºProvider API (TDX/Wencai/TA-Lib)
app.include_router(report_router)  # PDFæŠ¥å‘Šç”ŸæˆAPI
app.include_router(longhubang_router)  # é¾™è™æ¦œåˆ†æAPI
app.include_router(wencai_router)  # é—®è´¢æ™ºèƒ½é€‰è‚¡API
app.include_router(sector_rotation_router)  # æ¿å—è½®åŠ¨åˆ†æAPI
app.include_router(sentiment_router)  # å¸‚åœºæƒ…ç»ªåˆ†æAPI
app.include_router(realtime_monitor_router)  # å®æ—¶ç›¯ç›˜ç›‘æ§API
app.include_router(market_data_router)  # å¸‚åœºæ•°æ®APIï¼ˆç›˜å£ã€æ’è¡Œæ¦œç­‰ï¼‰
app.include_router(news_center_router)  # ç»Ÿä¸€æ–°é—»ç›‘æ§ä¸­å¿ƒAPI
app.include_router(cninfo_router)  # å·¨æ½®èµ„è®¯ç½‘å®˜æ–¹API
app.include_router(system_router)  # ç³»ç»Ÿè®¾ç½®API
app.include_router(api_monitor_router)  # APIç›‘æ§API
app.include_router(datasource_router)  # æ•°æ®æºè°ƒåº¦å™¨API


# ==================== æ•°æ®æ¨¡å‹ ====================

class GeminiRequest(BaseModel):
    model: str = "gemini-2.5-flash"
    prompt: str
    temperature: float = 0.7
    tools: Optional[List[Dict]] = None
    apiKey: Optional[str] = None

class DeepSeekRequest(BaseModel):
    model: str = "deepseek-chat"
    systemPrompt: str
    prompt: str
    temperature: float = 0.7
    apiKey: Optional[str] = None

class QwenRequest(BaseModel):
    model: str = "qwen-plus"
    systemPrompt: str
    prompt: str
    temperature: float = 0.7
    apiKey: Optional[str] = None

class SiliconFlowRequest(BaseModel):
    model: str = "Qwen/Qwen2.5-7B-Instruct"
    systemPrompt: str
    prompt: str
    temperature: float = 0.7
    apiKey: Optional[str] = None
    # ä»…ç”¨äºèƒ½åŠ›ç”»åƒ/å‹æµ‹ç­‰é«˜çº§ç”¨æ³•ï¼šè¦†ç›–é»˜è®¤max_tokensä¸æ˜¯å¦å¼€å¯thinkingèƒ½åŠ›
    maxTokens: Optional[int] = None
    enableThinking: Optional[bool] = None
    # æ™ºèƒ½ä½“è§’è‰²ï¼ˆç”¨äºé™çº§ç­–ç•¥ï¼‰
    agentRole: Optional[str] = None

class StockRequest(BaseModel):
    symbol: str
    apiKey: Optional[str] = None

class AnalyzeRequest(BaseModel):
    agent_id: str
    stock_code: str
    stock_data: Optional[Dict[str, Any]] = {}
    previous_outputs: Optional[Dict[str, Any]] = {}
    custom_instruction: Optional[str] = None


class CalibrationRunRequest(BaseModel):
    """è§¦å‘æ¨¡å‹èƒ½åŠ›ç”»åƒä¸é™é»˜å‹æµ‹çš„è¯·æ±‚ä½“"""
    # å¦‚æœä¸æŒ‡å®šï¼Œåˆ™é»˜è®¤ä½¿ç”¨ agent_configs.json ä¸­çš„ selectedModels + summarizerModel
    models: Optional[List[str]] = None
    # å¯é€‰è¦†ç›–é…ç½®ï¼›å¦‚æœä¸ºç©ºåˆ™ä½¿ç”¨ agent_configs.json ä¸­çš„ calibrationSettings
    calibrationSettings: Optional[Dict[str, Any]] = None

# ==================== AI API ç«¯ç‚¹ ====================

@app.post("/api/ai/gemini")
async def gemini_api(request: GeminiRequest):
    """Google Gemini API ä»£ç†"""
    try:
        api_key = request.apiKey or API_KEYS["gemini"]
        if not api_key:
            raise HTTPException(status_code=500, detail="æœªé…ç½® Gemini API Key")
        
        # ä½¿ç”¨å…¨å±€è¿æ¥æ± å®¢æˆ·ç«¯
        client = http_clients.get('gemini', http_clients['default'])
        
        # ç®€åŒ–çš„å®ç°ï¼Œå®é™…éœ€è¦æŒ‰ç…§ Google API æ ¼å¼
        headers = {"x-api-key": api_key}
        data = {
            "contents": [{"parts": [{"text": request.prompt}]}],
            "generationConfig": {"temperature": request.temperature}
        }
        
        response = await client.post(
            f"{API_ENDPOINTS['gemini']}/{request.model}:generateContent",
            headers=headers,
            json=data
        )
        
        if response.status_code != 200:
            raise HTTPException(status_code=response.status_code, detail="Gemini API é”™è¯¯")
        
        result = response.json()
        text = result.get("candidates", [{}])[0].get("content", {}).get("parts", [{}])[0].get("text", "")
        
        return {"success": True, "text": text}
    
    except HTTPException as e:
        import traceback
        error_detail = f"HTTP {e.status_code}: {e.detail}"
        print(f"[Gemini] HTTPé”™è¯¯: {error_detail}")
        print(traceback.format_exc())
        return {"success": False, "error": error_detail}
    except Exception as e:
        import traceback
        error_msg = f"{type(e).__name__}: {str(e)}"
        print(f"[Gemini] é”™è¯¯: {error_msg}")
        print(f"[Gemini] è¯¦ç»†ä¿¡æ¯:")
        print(traceback.format_exc())
        return {"success": False, "error": error_msg}

@app.post("/api/ai/deepseek")
async def deepseek_api(request: DeepSeekRequest):
    """DeepSeek API ä»£ç†"""
    try:
        api_key = request.apiKey or API_KEYS["deepseek"]
        if not api_key:
            raise HTTPException(status_code=500, detail="æœªé…ç½® DeepSeek API Key")
        
        # ä½¿ç”¨å…¨å±€è¿æ¥æ± å®¢æˆ·ç«¯
        client = http_clients.get('deepseek', http_clients['default'])
        
        headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {api_key}"
        }
        data = {
            "model": request.model,
            "messages": [
                {"role": "system", "content": request.systemPrompt},
                {"role": "user", "content": request.prompt}
            ],
            "temperature": request.temperature,
            "stream": False
        }
        
        # é‡è¯•æœºåˆ¶
        max_retries = 2
        for attempt in range(max_retries):
            try:
                response = await client.post(
                    API_ENDPOINTS["deepseek"],
                    headers=headers,
                    json=data,
                    timeout=httpx.Timeout(180.0, connect=60.0)
                )
                break
            except httpx.ReadTimeout:
                if attempt < max_retries - 1:
                    print(f"[DeepSeek] è¶…æ—¶ï¼Œæ­£åœ¨é‡è¯•... (å°è¯• {attempt + 2}/{max_retries})")
                    await asyncio.sleep(2)
                else:
                    print(f"[DeepSeek] æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥")
                    raise
        
        if response.status_code == 402:
            # 402æ˜¯ä½™é¢ä¸è¶³
            print(f"[DeepSeek] ä½™é¢ä¸è¶³ï¼Œè¿”å›é™çº§å“åº”")
            return {
                "success": True,
                "text": f"âš ï¸ DeepSeek API ä½™é¢ä¸è¶³ã€‚å»ºè®®ï¼š\n1. æ£€æŸ¥ API ä½™é¢\n2. åˆ‡æ¢åˆ° SiliconFlow æˆ–å…¶ä»–æ¨¡å‹\n3. å……å€¼åé‡è¯•",
                "usage": {"prompt_tokens": 0, "completion_tokens": 0, "total_tokens": 0},
                "quota_exceeded": True
            }
        elif response.status_code != 200:
            raise HTTPException(status_code=response.status_code, detail="DeepSeek API é”™è¯¯")
        
        result = response.json()
        text = result.get("choices", [{}])[0].get("message", {}).get("content", "")
        
        return {"success": True, "text": text}
    
    except HTTPException as e:
        import traceback
        error_detail = f"HTTP {e.status_code}: {e.detail}"
        print(f"[DeepSeek] HTTPé”™è¯¯: {error_detail}")
        print(traceback.format_exc())
        return {"success": False, "error": error_detail}
    except Exception as e:
        import traceback
        error_msg = f"{type(e).__name__}: {str(e)}"
        print(f"[DeepSeek] é”™è¯¯: {error_msg}")
        print(f"[DeepSeek] è¯¦ç»†ä¿¡æ¯:")
        print(traceback.format_exc())
        return {"success": False, "error": error_msg}

@app.post("/api/ai/qwen")
async def qwen_api(request: QwenRequest):
    """é€šä¹‰åƒé—® API ä»£ç†"""
    try:
        api_key = request.apiKey or API_KEYS["qwen"]
        if not api_key:
            raise HTTPException(status_code=500, detail="æœªé…ç½® Qwen API Key")
        
        # ä½¿ç”¨å…¨å±€è¿æ¥æ± å®¢æˆ·ç«¯
        client = http_clients.get('qwen', http_clients['default'])
        
        headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {api_key}"
        }
        data = {
            "model": request.model,
            "messages": [
                {"role": "system", "content": request.systemPrompt},
                {"role": "user", "content": request.prompt}
            ],
            "temperature": request.temperature,
            "stream": False
        }
        
        # é‡è¯•æœºåˆ¶
        max_retries = 2
        for attempt in range(max_retries):
            try:
                response = await client.post(
                    API_ENDPOINTS["qwen"],
                    headers=headers,
                    json=data,
                    timeout=httpx.Timeout(180.0, connect=60.0)
                )
                break
            except httpx.ReadTimeout:
                if attempt < max_retries - 1:
                    print(f"[Qwen] è¶…æ—¶ï¼Œæ­£åœ¨é‡è¯•... (å°è¯• {attempt + 2}/{max_retries})")
                    await asyncio.sleep(2)
                else:
                    print(f"[Qwen] æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥")
                    raise
        
        if response.status_code != 200:
            raise HTTPException(status_code=response.status_code, detail="Qwen API é”™è¯¯")
        
        result = response.json()
        text = result.get("choices", [{}])[0].get("message", {}).get("content", "")
        
        return {"success": True, "text": text}
    
    except HTTPException as e:
        import traceback
        error_detail = f"HTTP {e.status_code}: {e.detail}"
        print(f"[Qwen] HTTPé”™è¯¯: {error_detail}")
        print(traceback.format_exc())
        return {"success": False, "error": error_detail}
    except Exception as e:
        import traceback
        error_msg = f"{type(e).__name__}: {str(e)}"
        print(f"[Qwen] é”™è¯¯: {error_msg}")
        print(f"[Qwen] è¯¦ç»†ä¿¡æ¯:")
        print(traceback.format_exc())
        return {"success": False, "error": error_msg}

@app.post("/api/ai/siliconflow")
async def siliconflow_api(request: SiliconFlowRequest):
    """ç¡…åŸºæµåŠ¨ API ä»£ç†"""
    # ä½¿ç”¨å…¨å±€å¹¶å‘æ§åˆ¶å™¨é™åˆ¶å¹¶å‘è¯·æ±‚
    import datetime
    import time
    req_time = datetime.datetime.now().strftime("%H:%M:%S")
    request._start_time = time.time()  # è®°å½•å¼€å§‹æ—¶é—´
    
    # è®°å½•ç­‰å¾…è·å–é”çš„æ—¶é—´
    lock_wait_start = time.time()
    async with siliconflow_semaphore:
        lock_wait_time = time.time() - lock_wait_start
        concurrent_count = 10 - siliconflow_semaphore._value
        print(f"[SiliconFlow] [{req_time}] è·å–å¹¶å‘é”")
        print(f"  - ç­‰å¾…é”è€—æ—¶: {lock_wait_time:.1f}ç§’")
        print(f"  - å½“å‰å¹¶å‘æ•°: {concurrent_count}/10")
        
        client = None
        try:
            api_key = request.apiKey or API_KEYS["siliconflow"]
            if not api_key:
                raise HTTPException(status_code=500, detail="æœªé…ç½® SiliconFlow API Key")

            # âœ… åŠ¨æ€è¶…æ—¶é…ç½®ï¼šæ ¹æ®æ™ºèƒ½ä½“ç±»å‹è°ƒæ•´
            # å¤æ‚æ™ºèƒ½ä½“ï¼ˆnews_analyst, fundamentalï¼‰éœ€è¦æ›´é•¿æ—¶é—´
            agent_role = request.agentRole if hasattr(request, 'agentRole') else None
            complex_agents = ['NEWS', 'FUNDAMENTAL', 'TECHNICAL', 'MACRO', 'INDUSTRY']

            if agent_role in complex_agents:
                read_timeout = 60.0  # å¤æ‚æ™ºèƒ½ä½“ 60ç§’
                total_timeout = 90.0
            else:
                read_timeout = 45.0  # æ™®é€šæ™ºèƒ½ä½“ 45ç§’
                total_timeout = 60.0

            # ä¸ºæ¯ä¸ªè¯·æ±‚åˆ›å»ºç‹¬ç«‹çš„å®¢æˆ·ç«¯ï¼Œé¿å…è¿æ¥æ± æ­»é”
            client = httpx.AsyncClient(
                timeout=httpx.Timeout(
                    timeout=total_timeout,
                    connect=15.0,
                    read=read_timeout,
                    write=15.0,
                    pool=15.0
                ),
                limits=httpx.Limits(
                    max_connections=10,        # ä¿å®ˆè®¾ç½®ï¼Œé¿å…è¿‡å¤šè¿æ¥
                    max_keepalive_connections=5  # ä¿å®ˆè®¾ç½®
                )
            )
            
            headers = {
                "Content-Type": "application/json",
                "Authorization": f"Bearer {api_key}"
            }
            is_qwen3_model = isinstance(request.model, str) and "qwen3" in request.model.lower()
            # é»˜è®¤è¾“å‡ºé•¿åº¦æ§åˆ¶ï¼šæ”¯æŒé€šè¿‡è¯·æ±‚è¦†ç›–
            # å¢åŠ é»˜è®¤å€¼ä»¥æ”¯æŒæ›´å®Œæ•´çš„åˆ†ææŠ¥å‘Š
            max_tokens = 4096
            if request.maxTokens is not None:
                try:
                    max_tokens = int(request.maxTokens)
                except Exception:
                    max_tokens = 4096
            elif is_qwen3_model:
                # Qwen3 é»˜è®¤ä¿å®ˆä¸€äº›ï¼Œé¿å…é•¿è¾“å‡ºå¯¼è‡´è¶…æ—¶
                max_tokens = 2048
            data = {
                "model": request.model,
                "messages": [
                    {"role": "system", "content": request.systemPrompt},
                    {"role": "user", "content": request.prompt}
                ],
                "temperature": request.temperature,
                "max_tokens": max_tokens,
                "stream": False
            }
            # enable_thinking ä»…åœ¨è¯·æ±‚æ˜¾å¼æŒ‡å®šæˆ–é’ˆå¯¹ Qwen3 æ—¶è®¾ç½®
            enable_thinking = None
            if request.enableThinking is not None:
                enable_thinking = bool(request.enableThinking)
            elif is_qwen3_model:
                # é»˜è®¤ä¸å¼€å¯ Qwen3 çš„ thinkingï¼Œé¿å…å“åº”è¿‡æ…¢
                enable_thinking = False
            if enable_thinking is not None:
                data["enable_thinking"] = enable_thinking
            
            # ä½¿ç”¨é™çº§å¤„ç†å™¨æ‰§è¡Œè¯·æ±‚
            fallback_handler = get_fallback_handler()
            
            # æ£€æŸ¥æ˜¯å¦å¯ç”¨é™çº§ï¼ˆå¯é€šè¿‡ç¯å¢ƒå˜é‡æ§åˆ¶ï¼‰
            use_fallback = os.getenv("USE_FALLBACK", "true").lower() == "true"
            
            if use_fallback and agent_role:
                try:
                    result, metrics = await fallback_handler.execute_with_fallback(
                        client=client,
                        url=API_ENDPOINTS["siliconflow"],
                        headers=headers,
                        data=data,
                        agent_role=agent_role,
                        max_retries=4
                    )
                    
                    # è®°å½•æŒ‡æ ‡
                    total_time = time.time() - request._start_time
                    print(f"[SiliconFlow] [{req_time}] ğŸ è¯·æ±‚å®Œæˆ")
                    print(f"  - æ€»è€—æ—¶: {total_time:.1f}ç§’")
                    print(f"  - æœ€ç»ˆçŠ¶æ€: {metrics.final_status}")
                    print(f"  - å°è¯•æ¬¡æ•°: {len(metrics.attempt_times)}")
                    
                    # ä»…åœ¨å®é™…å‘ç”Ÿé™çº§ï¼ˆå‹ç¼©æˆ–é»˜è®¤å“åº”ï¼‰æ—¶æ‰“å°æç¤ºæ—¥å¿—
                    fallback_level = result.get("fallback_level", 0)
                    if fallback_level and fallback_level > 0:
                        level_name = {
                            1: "è½»åº¦å‹ç¼©",
                            2: "æ·±åº¦å‹ç¼©",
                            3: "æœ€å°åŒ–",
                            99: "é»˜è®¤å“åº”"
                        }.get(fallback_level, f"çº§åˆ«{fallback_level}")
                        print(f"[SiliconFlow] ä½¿ç”¨é™çº§å¤„ç†å™¨ (è§’è‰²: {agent_role}, çº§åˆ«: {fallback_level}, æ¨¡å¼: {level_name})")
                    
                    if metrics.final_status.startswith("success"):
                        print(f"  - âœ… æˆåŠŸ")
                    elif "cached" in metrics.final_status:
                        print(f"  - âš¡ ä½¿ç”¨ç¼“å­˜")
                    elif "default" in metrics.final_status:
                        print(f"  - âš ï¸ ä½¿ç”¨é»˜è®¤å“åº”")
                        
                    # æå–å“åº”æ–‡æœ¬
                    text = result.get("choices", [{}])[0].get("message", {}).get("content", "")
                    usage = result.get("usage", {})
                    
                    return {
                        "success": True,
                        "text": text,
                        "usage": usage,
                        "fallback_level": result.get("fallback_level", 0),
                        "metrics": {
                            "total_time": total_time,
                            "attempts": len(metrics.attempt_times),
                            "final_status": metrics.final_status
                        }
                    }
                    
                except Exception as e:
                    print(f"[SiliconFlow] é™çº§å¤„ç†å™¨é”™è¯¯: {e}")
                    # é™çº§å¤„ç†å™¨å¤±è´¥ï¼Œä½¿ç”¨åŸæœ‰é€»è¾‘
                    pass
            
            # åŸæœ‰é‡è¯•é€»è¾‘ï¼ˆä½œä¸ºåå¤‡ï¼‰
            if not agent_role:
                # ä»è¯·æ±‚ä¸­å°è¯•æ¨æ–­è§’è‰²ï¼ˆanalyzeè¯·æ±‚å¯èƒ½ä¼ é€’äº†agent_idï¼‰
                agent_role = "GENERAL"  # é€šç”¨è¯·æ±‚
                print(f"[SiliconFlow] é€šç”¨è¯·æ±‚ï¼ˆæœªæŒ‡å®šè§’è‰²ï¼‰")
            
            print(f"[SiliconFlow] ä½¿ç”¨åŸæœ‰é‡è¯•é€»è¾‘ï¼ˆagent_role={agent_role}ï¼‰")
            max_retries = 2
            response = None
            
            for attempt in range(max_retries):
                try:
                    print(f"[SiliconFlow] {request.model} å°è¯• {attempt+1}/{max_retries} [æç¤ºè¯é•¿åº¦: {len(request.prompt)}å­—ç¬¦]")
                    
                    # å¦‚æœæ˜¯é‡è¯•ï¼Œé‡æ–°åˆ›å»ºå®¢æˆ·ç«¯
                    if attempt > 0:
                        if client:
                            await client.aclose()
                        print(f"[SiliconFlow] é‡æ–°å»ºç«‹è¿æ¥...")
                        client = httpx.AsyncClient(
                            timeout=httpx.Timeout(
                                timeout=total_timeout,
                                connect=15.0,
                                read=read_timeout,
                                write=15.0,
                                pool=15.0
                            ),
                            limits=httpx.Limits(
                                max_connections=10,
                                max_keepalive_connections=5
                            )
                        )
                    
                    # æµ‹è¯•è¿æ¥ï¼ˆå°è¯·æ±‚éªŒè¯ï¼‰
                    if attempt > 0:
                        print(f"[SiliconFlow] æµ‹è¯•è¿æ¥...")
                        try:
                            test_response = await client.post(
                                API_ENDPOINTS["siliconflow"],
                                headers=headers,
                                json={
                                    "model": request.model,
                                    "messages": [{"role": "user", "content": "test"}],
                                    "max_tokens": 1,
                                    "stream": False
                                },
                                timeout=5.0  # 5ç§’å¿«é€Ÿæµ‹è¯•
                            )
                            if test_response.status_code == 200:
                                print(f"[SiliconFlow] è¿æ¥æµ‹è¯•æˆåŠŸ")
                            else:
                                print(f"[SiliconFlow] è¿æ¥æµ‹è¯•å¤±è´¥: HTTP {test_response.status_code}")
                        except Exception as test_e:
                            print(f"[SiliconFlow] è¿æ¥æµ‹è¯•å¼‚å¸¸: {type(test_e).__name__}")
                    
                    # å‘é€å®é™…è¯·æ±‚
                    import time
                    start_time = time.time()
                    request_size_kb = len(str(data)) / 1024
                    prompt_tokens_est = len(request.prompt) / 2  # ç²—ç•¥ä¼°ç®—tokenæ•°
                    print(f"[SiliconFlow] [{time.strftime('%H:%M:%S')}] å‘é€è¯·æ±‚")
                    print(f"  - æç¤ºè¯: {len(request.prompt)} å­—ç¬¦ (~{prompt_tokens_est:.0f} tokens)")
                    print(f"  - è¯·æ±‚ä½“: {request_size_kb:.1f} KB")
                    print(f"  - æ¨¡å‹: {request.model}")
                    
                    response = await asyncio.wait_for(
                        client.post(
                            API_ENDPOINTS["siliconflow"],
                            headers=headers,
                            json=data
                        ),
                        timeout=120.0  # å•æ¬¡è°ƒç”¨æ•´ä½“è¶…æ—¶120ç§’ï¼ˆåŸ45ç§’ï¼‰ï¼Œç»™è¶³æ—¶é—´
                    )
                    
                    elapsed = time.time() - start_time
                    print(f"[SiliconFlow] [{time.strftime('%H:%M:%S')}] âœ… å“åº”æˆåŠŸ")
                    print(f"  - APIå“åº”æ—¶é—´: {elapsed:.2f}ç§’")
                    print(f"  - é€Ÿåº¦: {len(request.prompt)/elapsed:.0f} å­—ç¬¦/ç§’")
                    break  # æˆåŠŸåˆ™è·³å‡ºå¾ªç¯
                except (asyncio.TimeoutError, httpx.ReadTimeout, httpx.RemoteProtocolError, httpx.ConnectError, httpx.NetworkError) as e:
                    error_type = type(e).__name__
                    elapsed = time.time() - start_time if 'start_time' in locals() else 0
                    print(f"[SiliconFlow] [{time.strftime('%H:%M:%S')}] {error_type} å‘ç”Ÿåœ¨ {elapsed:.1f}ç§’ é”™è¯¯: {str(e)[:200]}")

                    # å¯¹è¶…æ—¶ç±»é”™è¯¯ä½¿ç”¨é™çº§å¤„ç†å™¨çš„é»˜è®¤å“åº”
                    if error_type in ["ReadTimeout", "TimeoutError"]:
                        print(f"[SiliconFlow] {error_type} è¶…æ—¶ï¼Œä½¿ç”¨é™çº§é»˜è®¤å“åº”")
                        # ä½¿ç”¨é™çº§å¤„ç†å™¨çš„é»˜è®¤å“åº”
                        if not fallback_handler:
                            fallback_handler = get_fallback_handler()
                        
                        default_response = fallback_handler._get_default_response(
                            agent_role, 
                            f"è¶…æ—¶é”™è¯¯: {error_type} (å·²ç­‰å¾…{elapsed:.0f}ç§’)"
                        )
                        
                        # è¿”å›é»˜è®¤å“åº”è€Œä¸æ˜¯æŠ›å‡ºå¼‚å¸¸
                        text = default_response['choices'][0]['message']['content']
                        print(f"[SiliconFlow] è¿”å›é»˜è®¤å“åº”: {text[:100]}...")
                        
                        return {
                            "success": True,
                            "text": text,
                            "usage": {"prompt_tokens": 0, "completion_tokens": 0, "total_tokens": 0},
                            "fallback_level": 99,  # æ ‡è®°ä¸ºé»˜è®¤å“åº”
                            "timeout": True
                        }

                    # ä»…å¯¹è¿æ¥ç±»é”™è¯¯ä¿ç•™ä¸€æ¬¡é‡è¯•
                    if attempt < max_retries - 1:
                        wait_time = 3 + (2 * attempt)  # 3s, 5s
                        print(f"[SiliconFlow] {error_type}ï¼Œç­‰å¾…{wait_time}ç§’åé‡è¯• (å°è¯• {attempt + 2}/{max_retries})")

                        # å…³é—­æ—§è¿æ¥
                        if client:
                            try:
                                await client.aclose()
                                print(f"[SiliconFlow] å·²å…³é—­æ—§è¿æ¥")
                            except:
                                pass
                            client = None

                        await asyncio.sleep(wait_time)
                    else:
                        print(f"[SiliconFlow] æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥ ({error_type})ï¼Œè¿”å›é™çº§å“åº”")
                        return {
                            "success": True,
                            "text": f"âš ï¸ ç”±äºç½‘ç»œæ³¢åŠ¨ï¼Œæœ¬æ¬¡åˆ†ææœªèƒ½å®Œæˆã€‚å»ºè®®ï¼š\n1. æ£€æŸ¥ç½‘ç»œè¿æ¥\n2. å°è¯•ä½¿ç”¨å…¶ä»– AI æ¨¡å‹\n3. ç¨åé‡è¯•",
                            "usage": {"prompt_tokens": 0, "completion_tokens": 0, "total_tokens": 0},
                            "timeout": True
                        }
                except Exception as e:
                    print(f"[SiliconFlow] æœªçŸ¥é”™è¯¯: {type(e).__name__}: {str(e)}")
                    if attempt < max_retries - 1:
                        wait_time = 3
                        print(f"[SiliconFlow] ç­‰å¾…{wait_time}ç§’åé‡è¯• (å°è¯• {attempt + 2}/{max_retries})")
                        
                        # å…³é—­æ—§è¿æ¥
                        if client:
                            try:
                                await client.aclose()
                            except:
                                pass
                            client = None
                        
                        await asyncio.sleep(wait_time)
                    else:
                        print(f"[SiliconFlow] æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥ï¼Œè¿”å›é™çº§å“åº”")
                        return {
                            "success": True,
                            "text": f"âš ï¸ åˆ†æè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯ï¼Œè¯·ç¨åé‡è¯•ã€‚",
                            "usage": {"prompt_tokens": 0, "completion_tokens": 0, "total_tokens": 0},
                            "timeout": True
                        }
            
            # å¦‚æœæ‰€æœ‰é‡è¯•éƒ½å¤±è´¥ï¼Œè¿”å›é™çº§å“åº”
            if response is None:
                retry_summary = f"å°è¯•äº†{max_retries}æ¬¡ï¼Œå‡å¤±è´¥"
                if hasattr(request, '_start_time'):
                    total_elapsed = time.time() - request._start_time
                    retry_summary += f"ï¼Œæ€»è€—æ—¶{total_elapsed:.0f}ç§’"
                    
                print(f"[SiliconFlow] âŒ æ‰€æœ‰é‡è¯•å¤±è´¥: {retry_summary}")
                return {
                    "success": True,
                    "text": f"âš ï¸ AIæœåŠ¡æš‚æ—¶å“åº”ç¼“æ…¢ï¼ˆ{retry_summary}ï¼‰ã€‚å»ºè®®ï¼š\n1. å‡å°‘æç¤ºè¯é•¿åº¦\n2. é¿å…åŒæ—¶åˆ†æå¤šä¸ªæ™ºèƒ½ä½“\n3. ç¨åå†è¯•",
                    "usage": {"prompt_tokens": 0, "completion_tokens": 0, "total_tokens": 0},
                    "timeout": True
                }
            
            if response.status_code != 200:
                error_text = response.text
                print(f"[SiliconFlow] HTTP {response.status_code} é”™è¯¯")
                print(f"[SiliconFlow] å“åº”å†…å®¹: {error_text[:500]}")
                raise HTTPException(status_code=response.status_code, detail=f"SiliconFlow API é”™è¯¯: {error_text[:200]}")
            
            result = response.json()
            text = result.get("choices", [{}])[0].get("message", {}).get("content", "")
            
            # è·å–tokenä½¿ç”¨ä¿¡æ¯
            usage = result.get("usage", {})
            prompt_tokens = usage.get("prompt_tokens", 0)
            completion_tokens = usage.get("completion_tokens", 0)
            total_tokens = usage.get("total_tokens", 0)
            
            # è®¡ç®—æ€»è€—æ—¶
            import time
            if hasattr(request, '_start_time'):
                total_elapsed = time.time() - request._start_time
                print(f"[SiliconFlow] [{time.strftime('%H:%M:%S')}] è¯·æ±‚æ€»è€—æ—¶: {total_elapsed:.2f}ç§’")
            
            print(f"[SiliconFlow] Tokenä½¿ç”¨: {total_tokens} (è¾“å…¥: {prompt_tokens}, è¾“å‡º: {completion_tokens})")
            
            return {
                "success": True, 
                "text": text,
                "usage": {
                    "prompt_tokens": prompt_tokens,
                    "completion_tokens": completion_tokens,
                    "total_tokens": total_tokens
                }
            }
        
        except HTTPException as e:
            import traceback
            error_detail = f"HTTP {e.status_code}: {e.detail}"
            print(f"[SiliconFlow] HTTPé”™è¯¯: {error_detail}")
            print(traceback.format_exc())
            return {"success": False, "error": error_detail}
        except Exception as e:
            import traceback
            print(f"[SiliconFlow] âŒ æœªçŸ¥å¼‚å¸¸: {type(e).__name__}")
            traceback.print_exc()
            # è¿”å›å‹å¥½çš„é™çº§å“åº”è€Œä¸æ˜¯æŠ›å‡ºå¼‚å¸¸
            return {
                "success": True,
                "text": f"âš ï¸ åˆ†ææœåŠ¡æš‚æ—¶ä¸å¯ç”¨ï¼Œè¯·ç¨åé‡è¯•ã€‚é”™è¯¯ç±»å‹: {type(e).__name__}",
                "usage": {"prompt_tokens": 0, "completion_tokens": 0, "total_tokens": 0},
                "timeout": True
            }
        finally:
            # å…³é—­å®¢æˆ·ç«¯
            if client:
                try:
                    await client.aclose()
                    print(f"[SiliconFlow] å·²å…³é—­HTTPå®¢æˆ·ç«¯")
                except:
                    pass
            
            # è®¡ç®—å¹¶è¾“å‡ºæ€»è€—æ—¶
            if hasattr(request, '_start_time'):
                total_time = time.time() - request._start_time
                print(f"[SiliconFlow] [{time.strftime('%H:%M:%S')}] ğŸ è¯·æ±‚ç»“æŸï¼Œæ€»è€—æ—¶: {total_time:.1f}ç§’")
                if total_time > 30:
                    print(f"  âš ï¸ è€—æ—¶è¿‡é•¿ï¼Œå»ºè®®æ£€æŸ¥ç½‘ç»œæˆ–APIçŠ¶æ€")

@app.get("/api/models")
async def get_all_models():
    """è·å–æ‰€æœ‰å¯ç”¨æ¨¡å‹çš„ç»¼åˆåˆ—è¡¨"""
    all_models = []
    
    # 1. è·å–ç¡…åŸºæµåŠ¨æ¨¡å‹
    if API_KEYS.get("siliconflow"):
        try:
            # ä½¿ç”¨å…¨å±€è¿æ¥æ± å®¢æˆ·ç«¯
            client = http_clients.get('siliconflow', http_clients['default'])
            headers = {"Authorization": f"Bearer {API_KEYS['siliconflow']}"}
            response = await client.get(
                API_ENDPOINTS["siliconflow_models"],
                headers=headers
            )
            
            if response.status_code == 200:
                result = response.json()
                for model in result.get("data", []):
                    model_id = model.get("id", "")
                    # è§£æprovider
                    provider = "UNKNOWN"
                    if "qwen" in model_id.lower():
                        provider = "QWEN"
                    elif "llama" in model_id.lower():
                        provider = "LLAMA"
                    elif "deepseek" in model_id.lower():
                        provider = "DEEPSEEK"
                    elif "mistral" in model_id.lower():
                        provider = "MISTRAL"
                    elif "yi-" in model_id.lower() or "/yi" in model_id.lower():
                        provider = "YI"
                    elif "glm" in model_id.lower() or "chatglm" in model_id.lower():
                        provider = "GLM"
                    elif "gemma" in model_id.lower():
                        provider = "GEMMA"
                    elif "baichuan" in model_id.lower():
                        provider = "BAICHUAN"
                    elif "internlm" in model_id.lower():
                        provider = "INTERNLM"
                    elif "phi" in model_id.lower():
                        provider = "PHI"
                    elif model.get("owned_by") == "siliconflow":
                        provider = model_id.split("/")[0].upper() if "/" in model_id else "OTHER"
                    
                    # åˆ¤æ–­æ¨¡å‹ç±»å‹
                    model_type = "llm"  # é»˜è®¤ä¸ºLLM
                    if any(keyword in model_id.lower() for keyword in ["stable-diffusion", "sdxl", "flux", "playground", "dall-e", "midjourney"]):
                        model_type = "vision"
                    elif any(keyword in model_id.lower() for keyword in ["embedding", "bge", "jina-embed", "text-embedding"]):
                        model_type = "embedding"
                    elif any(keyword in model_id.lower() for keyword in ["whisper", "speech", "audio", "voice", "bark"]):
                        model_type = "audio"
                    
                    all_models.append({
                        "provider": provider,
                        "name": model_id,
                        "label": model_id.split("/")[-1] if "/" in model_id else model_id,
                        "type": model_type,
                        "channel": "ç¡…åŸºæµåŠ¨"
                    })
        except Exception as e:
            print(f"[Models] è·å–ç¡…åŸºæµåŠ¨æ¨¡å‹å¤±è´¥: {str(e)}")
    
    # 2. æ·»åŠ é€šä¹‰åƒé—®æ¨¡å‹
    if API_KEYS.get("qwen"):
        qwen_models = [
            {"provider": "QWEN", "name": "qwen-max", "label": "é€šä¹‰åƒé—® Max", "type": "llm", "channel": "é˜¿é‡Œäº‘"},
            {"provider": "QWEN", "name": "qwen-max-longcontext", "label": "é€šä¹‰åƒé—® Max é•¿æ–‡æœ¬", "type": "llm", "channel": "é˜¿é‡Œäº‘"},
            {"provider": "QWEN", "name": "qwen-plus", "label": "é€šä¹‰åƒé—® Plus", "type": "llm", "channel": "é˜¿é‡Œäº‘"},
            {"provider": "QWEN", "name": "qwen-turbo", "label": "é€šä¹‰åƒé—® Turbo", "type": "llm", "channel": "é˜¿é‡Œäº‘"},
            {"provider": "QWEN", "name": "qwen-turbo-latest", "label": "é€šä¹‰åƒé—® Turbo æœ€æ–°", "type": "llm", "channel": "é˜¿é‡Œäº‘"},
            {"provider": "QWEN", "name": "qwen2.5-72b-instruct", "label": "Qwen2.5 72B", "type": "llm", "channel": "é˜¿é‡Œäº‘"},
            {"provider": "QWEN", "name": "qwen2.5-32b-instruct", "label": "Qwen2.5 32B", "type": "llm", "channel": "é˜¿é‡Œäº‘"},
            {"provider": "QWEN", "name": "qwen2.5-14b-instruct", "label": "Qwen2.5 14B", "type": "llm", "channel": "é˜¿é‡Œäº‘"},
            {"provider": "QWEN", "name": "qwen2.5-7b-instruct", "label": "Qwen2.5 7B", "type": "llm", "channel": "é˜¿é‡Œäº‘"},
            {"provider": "QWEN", "name": "qwen2.5-3b-instruct", "label": "Qwen2.5 3B", "type": "llm", "channel": "é˜¿é‡Œäº‘"},
            {"provider": "QWEN", "name": "qwen2.5-coder-32b-instruct", "label": "Qwen2.5 Coder 32B", "type": "llm", "channel": "é˜¿é‡Œäº‘"},
            {"provider": "QWEN", "name": "qwen2.5-coder-7b-instruct", "label": "Qwen2.5 Coder 7B", "type": "llm", "channel": "é˜¿é‡Œäº‘"},
        ]
        all_models.extend(qwen_models)
    
    # 3. æ·»åŠ DeepSeekæ¨¡å‹
    if API_KEYS.get("deepseek"):
        deepseek_models = [
            {"provider": "DEEPSEEK", "name": "deepseek-chat", "label": "DeepSeek Chat", "type": "llm", "channel": "DeepSeek"},
            {"provider": "DEEPSEEK", "name": "deepseek-coder", "label": "DeepSeek Coder", "type": "llm", "channel": "DeepSeek"},
            {"provider": "DEEPSEEK", "name": "deepseek-reasoner", "label": "DeepSeek Reasoner", "type": "llm", "channel": "DeepSeek"},
        ]
        all_models.extend(deepseek_models)
    
    # 4. æ·»åŠ Geminiæ¨¡å‹
    if API_KEYS.get("gemini"):
        gemini_models = [
            {"provider": "GEMINI", "name": "gemini-2.0-flash-exp", "label": "Gemini 2.0 Flash (å®éªŒç‰ˆ)", "type": "llm", "channel": "Google"},
            {"provider": "GEMINI", "name": "gemini-exp-1206", "label": "Gemini å®éªŒç‰ˆ 1206", "type": "llm", "channel": "Google"},
            {"provider": "GEMINI", "name": "gemini-exp-1121", "label": "Gemini å®éªŒç‰ˆ 1121", "type": "llm", "channel": "Google"},
            {"provider": "GEMINI", "name": "gemini-1.5-pro-002", "label": "Gemini 1.5 Pro 002", "type": "llm", "channel": "Google"},
            {"provider": "GEMINI", "name": "gemini-1.5-pro", "label": "Gemini 1.5 Pro", "type": "llm", "channel": "Google"},
            {"provider": "GEMINI", "name": "gemini-1.5-flash", "label": "Gemini 1.5 Flash", "type": "llm", "channel": "Google"},
            {"provider": "GEMINI", "name": "gemini-1.5-flash-8b", "label": "Gemini 1.5 Flash 8B", "type": "llm", "channel": "Google"},
        ]
        all_models.extend(gemini_models)
    
    print(f"[Models] è¿”å› {len(all_models)} ä¸ªæ¨¡å‹")
    return {"success": True, "models": all_models, "total": len(all_models)}

@app.get("/api/ai/siliconflow-models")
async def siliconflow_models(apiKey: Optional[str] = None):
    """è·å–ç¡…åŸºæµåŠ¨å¯ç”¨æ¨¡å‹åˆ—è¡¨"""
    try:
        api_key = apiKey or API_KEYS["siliconflow"]
        if not api_key:
            return {"success": False, "error": "æœªé…ç½® SiliconFlow API Key", "models": []}
        
        # ä½¿ç”¨å…¨å±€è¿æ¥æ± å®¢æˆ·ç«¯
        client = http_clients.get('siliconflow', http_clients['default'])
        headers = {"Authorization": f"Bearer {api_key}"}
        response = await client.get(
            API_ENDPOINTS["siliconflow_models"],
            headers=headers
        )
        
        if response.status_code != 200:
            return {"success": False, "error": "è·å–æ¨¡å‹åˆ—è¡¨å¤±è´¥", "models": []}
        
        result = response.json()
        models = []
        for model in result.get("data", []):
            model_id = model.get("id", "")
            # ä¸è¿‡æ»¤ï¼Œè¿”å›æ‰€æœ‰æ¨¡å‹
            models.append({
                "id": model_id,
                "name": model_id,
                "label": model_id.split("/")[-1] if "/" in model_id else model_id,
                "owned_by": model.get("owned_by", "unknown")
            })
        
        print(f"[SiliconFlow] åŠ è½½äº† {len(models)} ä¸ªæ¨¡å‹")
        return {"success": True, "models": models}
    
    except Exception as e:
        print(f"[SiliconFlow Models] é”™è¯¯: {str(e)}")
        return {"success": False, "error": str(e), "models": []}

# ==================== æ¨¡å‹èƒ½åŠ›ç”»åƒä¸é™é»˜å‹æµ‹ ====================

def get_calibration_settings() -> Dict[str, Any]:
    """ä» agent_configs.json è¯»å–æ¨¡å‹èƒ½åŠ›ç”»åƒä¸é™é»˜å‹æµ‹é…ç½®ï¼Œå¹¶ä¸é»˜è®¤å€¼åˆå¹¶"""
    default_settings = {
        "enabled": False,
        "concurrency": [3, 5],
        "promptLengths": [4000, 6000, 8000],
        "maxTokens": 512,
        "enableThinking": False,
        "timeoutSeconds": 180
    }
    config_file = os.path.join(os.path.dirname(__file__), "agent_configs.json")
    if not os.path.exists(config_file):
        return default_settings
    try:
        with open(config_file, "r", encoding="utf-8") as f:
            config_data = json.load(f)
        custom = config_data.get("calibrationSettings") or {}
        if isinstance(custom, dict):
            merged = default_settings.copy()
            merged.update(custom)
            return merged
    except Exception as e:
        print(f"[Calibration] è¯»å–é…ç½®å¤±è´¥: {str(e)}")
    return default_settings


def build_calibration_models(override_models: Optional[List[str]] = None) -> List[Dict[str, Any]]:
    """æ ¹æ® agent_configs.json ä¸­çš„ selectedModels + summarizerModel æ„å»ºä»…åŒ…å«LLMçš„å‹æµ‹å€™é€‰æ¨¡å‹åˆ—è¡¨ã€‚

    å¦‚æœä¼ å…¥ override_modelsï¼Œåˆ™ä¼˜å…ˆä½¿ç”¨ä¸ selectedModels çš„äº¤é›†ï¼›è‹¥äº¤é›†ä¸ºç©ºï¼Œåˆ™ä½¿ç”¨ override_models æœ¬èº«ã€‚
    """
    config_file = os.path.join(os.path.dirname(__file__), "agent_configs.json")
    model_names: set[str] = set()
    if os.path.exists(config_file):
        try:
            with open(config_file, "r", encoding="utf-8") as f:
                config_data = json.load(f)
            for name in config_data.get("selectedModels", []):
                if isinstance(name, str) and name.strip():
                    model_names.add(name.strip())
            summarizer = config_data.get("summarizerModel")
            if isinstance(summarizer, str) and summarizer.strip():
                model_names.add(summarizer.strip())
        except Exception as e:
            print(f"[Calibration] è¯»å– agent_configs å¤±è´¥: {str(e)}")

    # å¦‚æœè°ƒç”¨æ–¹ä¼ å…¥ modelsï¼Œåˆ™ä¸ç°æœ‰åˆ—è¡¨å–äº¤é›†ï¼›äº¤é›†ä¸ºç©ºæ—¶é€€åŒ–ä¸ºä½¿ç”¨ä¼ å…¥åˆ—è¡¨
    if override_models:
        override_set = {m.strip() for m in override_models if isinstance(m, str) and m.strip()}
        if override_set:
            intersection = model_names & override_set
            model_names = intersection or override_set

    if not model_names:
        return []

    vision_keywords = ["stable-diffusion", "sdxl", "flux", "playground", "dall-e", "midjourney"]
    embed_keywords = ["embedding", "bge", "jina-embed", "text-embedding"]
    audio_keywords = ["whisper", "speech", "audio", "voice", "bark"]

    models: List[Dict[str, Any]] = []
    for name in sorted(model_names):
        if not isinstance(name, str) or not name.strip():
            continue
        lower = name.lower()

        # è¿‡æ»¤æ‰éLLMæ¨¡å‹
        model_type = "llm"
        if any(k in lower for k in vision_keywords):
            model_type = "vision"
        elif any(k in lower for k in embed_keywords):
            model_type = "embedding"
        elif any(k in lower for k in audio_keywords):
            model_type = "audio"
        if model_type != "llm":
            continue

        provider = "UNKNOWN"
        channel = None
        if "/" in name:
            # å¸¦æ–œæ çš„ä¸€å¾‹è§†ä¸ºç¡…åŸºæµåŠ¨æ‰˜ç®¡æ¨¡å‹
            channel = "ç¡…åŸºæµåŠ¨"
            if "qwen" in lower:
                provider = "QWEN"
            elif "llama" in lower:
                provider = "LLAMA"
            elif "deepseek" in lower:
                provider = "DEEPSEEK"
            elif "mistral" in lower:
                provider = "MISTRAL"
            elif "yi-" in lower or "/yi" in lower:
                provider = "YI"
            elif "glm" in lower or "chatglm" in lower:
                provider = "GLM"
            elif "gemma" in lower:
                provider = "GEMMA"
            elif "baichuan" in lower:
                provider = "BAICHUAN"
            elif "internlm" in lower:
                provider = "INTERNLM"
            elif "phi" in lower:
                provider = "PHI"
        else:
            # å®˜æ–¹ç›´è¿æ¨¡å‹
            if name.startswith("gemini"):
                provider = "GEMINI"
                channel = "Google"
            elif name in ["deepseek-chat", "deepseek-coder", "deepseek-reasoner"]:
                provider = "DEEPSEEK"
                channel = "DeepSeek"
            elif name in [
                "qwen-max",
                "qwen-max-longcontext",
                "qwen-plus",
                "qwen-turbo",
                "qwen-turbo-latest",
                "qwen2.5-72b-instruct",
                "qwen2.5-32b-instruct",
                "qwen2.5-14b-instruct",
                "qwen2.5-7b-instruct",
                "qwen2.5-3b-instruct",
                "qwen2.5-coder-32b-instruct",
                "qwen2.5-coder-7b-instruct",
            ]:
                provider = "QWEN"
                channel = "é˜¿é‡Œäº‘"

        models.append({
            "name": name,
            "provider": provider,
            "channel": channel,
            "type": "llm"
        })

    return models


async def _run_single_calibration_test(model: Dict[str, Any], prompt_length: int, settings: Dict[str, Any], timeout_seconds: int, sem: asyncio.Semaphore, concurrency: int):
    """å¯¹å•ä¸ªæ¨¡å‹å’Œå•ä¸ªprompté•¿åº¦æ‰§è¡Œä¸€æ¬¡é™é»˜å‹æµ‹ï¼Œè¿”å›æµ‹è¯•ç»“æœè®°å½•ã€‚

    Args:
        model: æ¨¡å‹ä¿¡æ¯å­—å…¸
        prompt_length: æç¤ºè¯é•¿åº¦ï¼ˆå­—ç¬¦æ•°ï¼‰
        settings: å‹æµ‹é…ç½®ï¼ˆåŒ…å« maxTokens / enableThinking ç­‰ï¼‰
        timeout_seconds: å•æ¬¡è°ƒç”¨è¶…æ—¶æ—¶é—´
        sem: æ§åˆ¶å¹¶å‘çš„ä¿¡å·é‡
        concurrency: æœ¬è½®å‹æµ‹ç›®æ ‡å¹¶å‘æ•°ï¼Œç”¨äºè®°å½•åˆ°ç»“æœä¸­
    """
    name = model.get("name") or ""
    provider = (model.get("provider") or "").upper() or "UNKNOWN"
    channel = model.get("channel")

    # æ„é€ æŒ‡å®šé•¿åº¦çš„æµ‹è¯•æç¤ºè¯
    base = "è¿™æ˜¯ä¸€æ®µç”¨äºæ¨¡å‹èƒ½åŠ›ç”»åƒä¸é™é»˜å‹æµ‹çš„æµ‹è¯•æ–‡æœ¬ã€‚"
    repeat = max(1, int(prompt_length / max(len(base), 1)) + 1)
    user_prompt = (base * repeat)[: max(10, prompt_length)]
    system_prompt = "ä½ æ˜¯ä¸€ä¸ªä¸­æ–‡å¤§è¯­è¨€æ¨¡å‹æ€§èƒ½æµ‹è¯•åŠ©æ‰‹ï¼Œè¯·é’ˆå¯¹ç”¨æˆ·è¾“å…¥ç»™å‡ºç®€çŸ­ã€æœ‰æ„ä¹‰çš„å›ç­”ã€‚"

    max_tokens = int(settings.get("maxTokens", 512) or 512)
    enable_thinking = bool(settings.get("enableThinking", False))

    started_at = datetime.utcnow().isoformat()
    start_time = asyncio.get_event_loop().time()
    success = False
    timeout_flag = False
    error_msg = None
    usage = {}
    conc_value = concurrency if isinstance(concurrency, int) and concurrency > 0 else None

    async with sem:
        try:
            # æ ¹æ®åç§°/æä¾›æ–¹æ¨æ–­å®é™…è°ƒç”¨çš„provider
            effective_provider = "SILICONFLOW"
            if "/" not in name:
                if name.startswith("gemini"):
                    effective_provider = "GEMINI"
                elif name in ["deepseek-chat", "deepseek-coder", "deepseek-reasoner"]:
                    effective_provider = "DEEPSEEK"
                elif name in [
                    "qwen-max",
                    "qwen-max-longcontext",
                    "qwen-plus",
                    "qwen-turbo",
                    "qwen-turbo-latest",
                    "qwen2.5-72b-instruct",
                    "qwen2.5-32b-instruct",
                    "qwen2.5-14b-instruct",
                    "qwen2.5-7b-instruct",
                    "qwen2.5-3b-instruct",
                    "qwen2.5-coder-32b-instruct",
                    "qwen2.5-coder-7b-instruct",
                ]:
                    effective_provider = "QWEN"

            result: Dict[str, Any] = {"success": False}

            if effective_provider == "GEMINI":
                req = GeminiRequest(
                    model=name,
                    prompt=user_prompt,
                    temperature=0.5
                )
                result = await asyncio.wait_for(gemini_api(req), timeout=timeout_seconds)
            elif effective_provider == "DEEPSEEK":
                req = DeepSeekRequest(
                    model=name,
                    systemPrompt=system_prompt,
                    prompt=user_prompt,
                    temperature=0.5
                )
                result = await asyncio.wait_for(deepseek_api(req), timeout=timeout_seconds)
            elif effective_provider == "QWEN":
                req = QwenRequest(
                    model=name,
                    systemPrompt=system_prompt,
                    prompt=user_prompt,
                    temperature=0.5
                )
                result = await asyncio.wait_for(qwen_api(req), timeout=timeout_seconds)
            else:
                # é»˜è®¤é€šè¿‡ç¡…åŸºæµåŠ¨è°ƒç”¨
                req = SiliconFlowRequest(
                    model=name,
                    systemPrompt=system_prompt,
                    prompt=user_prompt,
                    temperature=0.5,
                    maxTokens=max_tokens,
                    enableThinking=enable_thinking,
                    agentRole="BENCHMARK"  # å‹æµ‹è§’è‰²
                )
                result = await asyncio.wait_for(siliconflow_api(req), timeout=timeout_seconds)

            success = bool(result.get("success"))
            usage = result.get("usage") or {}
            timeout_flag = bool(result.get("timeout", False))
        except asyncio.TimeoutError:
            timeout_flag = True
            error_msg = f"Timeout({timeout_seconds}s)"
        except Exception as e:
            error_msg = str(e)

    finished_at = datetime.utcnow().isoformat()
    latency = asyncio.get_event_loop().time() - start_time

    return {
        "modelName": name,
        "provider": provider or "UNKNOWN",
        "channel": channel,
        "promptLength": int(prompt_length),
        "maxTokens": max_tokens,
        "enableThinking": enable_thinking,
        "concurrency": conc_value,
        "startedAt": started_at,
        "finishedAt": finished_at,
        "latencySeconds": round(latency, 2),
        "success": success,
        "timeout": timeout_flag,
        "error": error_msg,
        "usage": usage,
    }


async def run_calibration_once(settings: Dict[str, Any], models: List[Dict[str, Any]]):
    """æ‰§è¡Œä¸€æ¬¡å®Œæ•´çš„æ¨¡å‹èƒ½åŠ›ç”»åƒä¸é™é»˜å‹æµ‹ï¼ˆä»…é’ˆå¯¹LLMï¼‰ã€‚"""
    global calibration_state

    prompt_lengths = settings.get("promptLengths") or [4000, 6000, 8000]
    try:
        prompt_lengths = [int(x) for x in prompt_lengths if int(x) > 0]
    except Exception:
        prompt_lengths = [4000, 6000, 8000]

    raw_concurrency = settings.get("concurrency")
    concurrency_list: List[int] = []
    if isinstance(raw_concurrency, list):
        for v in raw_concurrency:
            try:
                iv = int(v)
                if iv > 0:
                    concurrency_list.append(iv)
            except Exception:
                continue
    elif raw_concurrency is not None:
        try:
            iv = int(raw_concurrency)
            if iv > 0:
                concurrency_list.append(iv)
        except Exception:
            pass

    if not concurrency_list:
        concurrency_list = [3]

    # å»é‡å¹¶é™åˆ¶åœ¨ 1-5 ä¹‹é—´
    concurrency_list = sorted({max(1, min(int(v), 5)) for v in concurrency_list})

    timeout_seconds = int(settings.get("timeoutSeconds", 180) or 180)
    if timeout_seconds <= 0:
        timeout_seconds = 180

    print(f"[Calibration] å¼€å§‹é™é»˜å‹æµ‹: æ¨¡å‹æ•°={len(models)}, å¹¶å‘åˆ—è¡¨={concurrency_list}, prompté•¿åº¦={prompt_lengths}, timeout={timeout_seconds}s")

    # åˆå§‹åŒ–ç»“æœç»“æ„
    results: Dict[str, Any] = {}
    for m in models:
        name = m.get("name")
        if not name:
            continue
        results[name] = {
            "provider": (m.get("provider") or "UNKNOWN").upper(),
            "channel": m.get("channel"),
            "tests": []
        }

    try:
        # æŒ‰ä¸åŒå¹¶å‘å€¼ä¾æ¬¡å‹æµ‹ï¼Œå°†ç»“æœè¿½åŠ åˆ°åŒä¸€ tests åˆ—è¡¨ä¸­
        for conc in concurrency_list:
            sem = asyncio.Semaphore(conc)
            tasks = []
            for m in models:
                name = m.get("name")
                if not name:
                    continue
                for length in prompt_lengths:
                    tasks.append(
                        _run_single_calibration_test(
                            m,
                            length,
                            settings,
                            timeout_seconds,
                            sem,
                            conc,
                        )
                    )

            test_records = await asyncio.gather(*tasks, return_exceptions=True)
            for record in test_records:
                if isinstance(record, Exception) or not isinstance(record, dict):
                    continue
                model_name = record.get("modelName")
                if not model_name or model_name not in results:
                    continue
                results[model_name]["tests"].append(record)

        calibration_state["status"] = "completed"
        calibration_state["error"] = None
        calibration_state["results"] = results
        save_calibration_state()
        print(f"[Calibration] é™é»˜å‹æµ‹å®Œæˆï¼Œå…± {len(results)} ä¸ªæ¨¡å‹")
    except Exception as e:
        calibration_state["status"] = "error"
        calibration_state["error"] = str(e)
        save_calibration_state()
        print(f"[Calibration] é™é»˜å‹æµ‹å¤±è´¥: {str(e)}")


@app.post("/api/models/calibration/run")
async def start_model_calibration(request: CalibrationRunRequest):
    """è§¦å‘ä¸€æ¬¡æ¨¡å‹èƒ½åŠ›ç”»åƒä¸é™é»˜å‹æµ‹ï¼ˆä»…é’ˆå¯¹LLMï¼‰ã€‚"""
    global calibration_state

    if calibration_state.get("status") == "running":
        return {"success": False, "error": "å·²æœ‰å‹æµ‹ä»»åŠ¡åœ¨è¿è¡Œï¼Œè¯·ç¨åå†è¯•"}

    # åˆå¹¶é…ç½®ï¼šä»¥ agent_configs.json ä¸ºåŸºç¡€ï¼Œrequest.calibrationSettings ä¸ºè¦†ç›–
    base_settings = get_calibration_settings()
    override = request.calibrationSettings or {}
    if isinstance(override, dict):
        base_settings.update(override)

    models = build_calibration_models(request.models)
    if not models:
        calibration_state["status"] = "idle"
        calibration_state["error"] = None
        calibration_state["results"] = {}
        save_calibration_state()
        return {"success": False, "error": "æ²¡æœ‰å¯ç”¨äºå‹æµ‹çš„æ¨¡å‹ï¼Œè¯·å…ˆåœ¨æ¨¡å‹ç®¡ç†ä¸­é€‰æ‹©å¤§è¯­è¨€æ¨¡å‹"}

    now_iso = datetime.utcnow().isoformat()
    calibration_state["status"] = "running"
    calibration_state["lastRunAt"] = now_iso
    calibration_state["error"] = None
    calibration_state["results"] = {}
    calibration_state["settings"] = base_settings
    save_calibration_state()

    # å¼‚æ­¥å¯åŠ¨å‹æµ‹ä»»åŠ¡
    asyncio.create_task(run_calibration_once(base_settings, models))

    return {
        "success": True,
        "message": f"å·²å¯åŠ¨æ¨¡å‹èƒ½åŠ›ç”»åƒä¸é™é»˜å‹æµ‹ï¼Œå…± {len(models)} ä¸ªæ¨¡å‹",
        "startedAt": now_iso
    }


@app.get("/api/models/calibration/status")
async def get_model_calibration_status():
    """æŸ¥è¯¢å½“å‰æ¨¡å‹èƒ½åŠ›ç”»åƒä¸é™é»˜å‹æµ‹çš„çŠ¶æ€å’Œæœ€è¿‘ä¸€æ¬¡ç»“æœã€‚"""
    # ç›´æ¥è¿”å›å†…å­˜ä¸­çš„çŠ¶æ€ï¼Œå‰ç«¯å¯æ ¹æ®éœ€è¦è§£æ results/tests
    total_models = len(calibration_state.get("results") or {})
    return {
        "success": True,
        "data": {
            **calibration_state,
            "totalModels": total_models
        }
    }


# ==================== åˆ†æ API ====================

# å…¨å±€ç¼“å­˜é…ç½®
_agent_configs_cache = None
_cache_timestamp = 0

def get_agent_config(agent_id: str):
    """è·å–æ™ºèƒ½ä½“é…ç½®ï¼ˆå¸¦ç¼“å­˜ï¼‰"""
    global _agent_configs_cache, _cache_timestamp
    
    config_file = os.path.join(os.path.dirname(__file__), "agent_configs.json")
    
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦æ›´æ–°ï¼ˆæ¯5ç§’æœ€å¤šè¯»ä¸€æ¬¡ï¼‰
    current_time = asyncio.get_event_loop().time()
    if _agent_configs_cache is None or (current_time - _cache_timestamp) > 5:
        if os.path.exists(config_file):
            with open(config_file, 'r', encoding='utf-8') as f:
                _agent_configs_cache = json.load(f)
                _cache_timestamp = current_time
    
    if _agent_configs_cache:
        for agent in _agent_configs_cache.get('agents', []):
            if agent.get('id') == agent_id:
                return agent
    return None

"""
ä¿®å¤åçš„ analyze_stock å‡½æ•°
è¯·å¤åˆ¶æ­¤å‡½æ•°æ›¿æ¢ server.py ä¸­çš„ analyze_stock å‡½æ•°ï¼ˆä»ç¬¬704è¡Œå¼€å§‹ï¼‰
"""

def get_summarizer_settings():
    config_file = os.path.join(os.path.dirname(__file__), "agent_configs.json")
    default_settings = {
        "modelName": "Qwen/Qwen2.5-7B-Instruct",
        "temperature": 0.2
    }
    if not os.path.exists(config_file):
        return default_settings
    try:
        with open(config_file, 'r', encoding='utf-8') as f:
            config_data = json.load(f)
        model_name = config_data.get("summarizerModel")
        temperature = config_data.get("summarizerTemperature", 0.2)
        if not model_name:
            return default_settings
        return {
            "modelName": model_name,
            "temperature": temperature
        }
    except Exception:
        return default_settings

async def summarize_previous_outputs(agent_id: str, previous_outputs: Optional[Dict[str, Any]], stock_code: str) -> str:
    texts = []
    if not previous_outputs:
        return ""
    for agent_name, output in previous_outputs.items():
        if output:
            role = get_agent_role(agent_name)
            texts.append(f"{role}ï¼ˆ{agent_name}ï¼‰çš„ç»“è®º:\n{output}")
    if not texts:
        return ""
    combined_text = "\n\n".join(texts)
    system_prompt = "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æŠ•ç ”å›¢é˜ŸåŠ©ç†ï¼Œæ“…é•¿é˜…è¯»å¤šä½åˆ†æå¸ˆçš„è§‚ç‚¹å¹¶æç‚¼è¦ç‚¹ã€‚"
    user_prompt = (
        f"ä¸‹é¢æ˜¯å…³äºè‚¡ç¥¨ {stock_code} çš„å¤šä½åˆ†æå¸ˆå®Œæ•´åˆ†æï¼Œè¯·åœ¨ä¿ç•™å…³é”®ä¿¡æ¯çš„å‰æä¸‹è¿›è¡Œå‹ç¼©æ•´ç†ï¼š\n\n"
        "1. ç”¨åˆ†ç‚¹æ–¹å¼å½’çº³å‡ºå…¨å±€æ ¸å¿ƒç»“è®ºï¼ˆæœ€å¤š 6 ç‚¹ï¼‰ã€‚\n"
        "2. çªå‡ºé‡å¤§åˆ©å¥½/åˆ©ç©ºã€å…³é”®é£é™©å’Œä¸ç¡®å®šæ€§ã€‚\n"
        "3. è¾“å‡ºé•¿åº¦æ§åˆ¶åœ¨ 1200-1500 å­—ä»¥å†…ã€‚\n\n"
        "ã€åˆ†æåŸæ–‡ã€‘\n" + combined_text
    )
    settings = get_summarizer_settings()
    model_name = settings.get("modelName", "Qwen/Qwen2.5-7B-Instruct")
    temperature = settings.get("temperature", 0.2)
    provider = "SILICONFLOW"
    # ä»…å½“æ¨¡å‹åä¸åŒ…å«"/"ä¸”æ˜æ˜¾æ˜¯å®˜æ–¹ DeepSeek ç›´è¿å‹å·æ—¶ï¼Œæ‰èµ° deepseek_api
    # å¦åˆ™ï¼ˆåŒ…æ‹¬ SiliconFlow deepseek-* æ¨¡å‹ï¼‰ç»Ÿä¸€é€šè¿‡ SiliconFlow æ¸ é“è°ƒç”¨
    if "/" not in model_name:
        lower_name = model_name.lower()
        if model_name.startswith("gemini"):
            provider = "GEMINI"
        elif lower_name.startswith("deepseek-") or lower_name in ["deepseek-chat", "deepseek-coder", "deepseek-reasoner"]:
            provider = "DEEPSEEK"
    try:
        if provider == "GEMINI":
            req = GeminiRequest(
                model=model_name,
                prompt=system_prompt + "\n\n" + user_prompt,
                temperature=temperature
            )
            result = await gemini_api(req)
        elif provider == "DEEPSEEK":
            req = DeepSeekRequest(
                model=model_name,
                systemPrompt=system_prompt,
                prompt=user_prompt,
                temperature=temperature
            )
            result = await deepseek_api(req)
        else:
            req = SiliconFlowRequest(
                model=model_name,
                systemPrompt=system_prompt,
                prompt=user_prompt,
                temperature=temperature,
                agentRole="SUMMARIZER"  # æ‘˜è¦å™¨è§’è‰²
            )
            result = await siliconflow_api(req)
    except Exception:
        result = {"success": False}
    if result.get("success"):
        text = result.get("text") or ""
        if text:
            return text
    return combined_text[:2000]

@app.post("/api/analyze")
async def analyze_stock(request: AnalyzeRequest):
    """ç»Ÿä¸€çš„æ™ºèƒ½ä½“åˆ†ææ¥å£"""
    try:
        print(f"[åˆ†æ] {request.agent_id} å¼€å§‹åˆ†æ...")
        agent_id = request.agent_id
        stock_code = request.stock_code
        stock_data = request.stock_data
        previous_outputs = request.previous_outputs
        custom_instruction = request.custom_instruction
        
        # ä»ç¼“å­˜è·å–é…ç½®
        agent_config = get_agent_config(agent_id)
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°é…ç½®ï¼Œä½¿ç”¨é»˜è®¤å€¼ï¼ˆä½¿ç”¨SiliconFlowé¿å…ä½™é¢é—®é¢˜ï¼‰
        if not agent_config:
            agent_config = {
                "modelName": "Qwen/Qwen2.5-7B-Instruct",  # é»˜è®¤ä½¿ç”¨SiliconFlowçš„é€šä¹‰åƒé—®
                "modelProvider": "SILICONFLOW",
                "temperature": 0.3
            }
        
        model_name = agent_config.get("modelName", "deepseek-chat")
        temperature = agent_config.get("temperature", 0.3)
        
        # æ ¹æ®æ¨¡å‹åç§°åˆ¤æ–­ä½¿ç”¨å“ªä¸ªAPI
        # ä¼˜å…ˆåˆ¤æ–­ï¼šå¦‚æœåŒ…å«æ–œæ ï¼Œè¯´æ˜æ˜¯å¹³å°æ¨¡å‹ï¼ˆå¦‚ Qwen/Qwen3-8Bï¼‰ï¼Œä½¿ç”¨ç¡…åŸºæµåŠ¨
        api_endpoint = None
        if "/" in model_name:
            # åŒ…å«æ–œæ çš„éƒ½æ˜¯å¹³å°æ¨¡å‹ï¼Œé€šè¿‡ç¡…åŸºæµåŠ¨è®¿é—®
            api_endpoint = "/api/ai/siliconflow"
            provider = "SILICONFLOW"
        elif model_name.startswith("gemini"):
            # Geminiå®˜æ–¹æ¨¡å‹
            api_endpoint = "/api/ai/gemini"
            provider = "GEMINI"
        elif model_name in ["deepseek-chat", "deepseek-coder", "deepseek-reasoner"]:
            # DeepSeekå®˜æ–¹æ¨¡å‹ï¼ˆæ˜ç¡®åˆ—ä¸¾ï¼‰
            api_endpoint = "/api/ai/deepseek"
            provider = "DEEPSEEK"
        elif model_name in ["qwen-max", "qwen-plus", "qwen-turbo", "qwen-max-longcontext", "qwen-turbo-latest"] or "é€šä¹‰åƒé—®" in model_name:
            # Qwenå®˜æ–¹æ¨¡å‹ï¼ˆæ˜ç¡®åˆ—ä¸¾ï¼‰
            api_endpoint = "/api/ai/qwen"
            provider = "QWEN"
        else:
            # é»˜è®¤ä½¿ç”¨ç¡…åŸºæµåŠ¨ï¼ˆæ”¯æŒæœ€å¤šæ¨¡å‹ï¼‰
            api_endpoint = "/api/ai/siliconflow"
            provider = "SILICONFLOW"
        
        # æ„å»ºç³»ç»Ÿæç¤ºè¯
        role_name = get_agent_role(agent_id)
        system_prompt = f"ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„{role_name}ï¼Œéš¶å±äºInvestMindProé¡¶çº§æŠ•ç ”å›¢é˜Ÿã€‚ä½ çš„ç›®æ ‡æ˜¯æä¾›æ·±åº¦ã€çŠ€åˆ©ä¸”ç‹¬åˆ°çš„æŠ•èµ„è§è§£ã€‚"
        system_prompt += "\n\nã€é£æ ¼è¦æ±‚ã€‘\n1. ç›´æ¥åˆ‡å…¥ä¸»é¢˜ï¼Œä¸¥ç¦åºŸè¯ã€‚\n2. ä¸¥ç¦åœ¨å¼€å¤´å¤è¿°è‚¡ç¥¨ä»£ç ã€åç§°ã€å½“å‰ä»·æ ¼ç­‰åŸºç¡€ä¿¡æ¯ï¼ˆé™¤éæ•°æ®å‡ºç°é‡å¤§å¼‚å¸¸ï¼‰ã€‚\n3. åƒåå°”è¡—èµ„æ·±åˆ†æå¸ˆä¸€æ ·è¯´è¯ï¼Œä½¿ç”¨ä¸“ä¸šæœ¯è¯­ä½†é€»è¾‘æ¸…æ™°ã€‚\n4. å¿…é¡»å¼•ç”¨å‰åºåŒäº‹çš„åˆ†æç»“è®ºä½œä¸ºæ”¯æ’‘æˆ–åé©³çš„ä¾æ®ã€‚"

        # æ„å»ºç”¨æˆ·æç¤ºè¯
        user_prompt = ""
        
        # å¦‚æœæœ‰è‡ªå®šä¹‰æŒ‡ä»¤ï¼Œä¼˜å…ˆæ”¾å…¥
        if custom_instruction:
            user_prompt += f"ã€å½“å‰ä»»åŠ¡æŒ‡ä»¤ã€‘\n{custom_instruction}\n\n"
        
        # åŸºç¡€æ•°æ®ä»…ä½œä¸ºå‚è€ƒé™„å½•ï¼Œä¸å¼ºåˆ¶è¦æ±‚åˆ†æ
        user_prompt += f"ã€å‚è€ƒæ•°æ® - {stock_code}ã€‘\n"
        user_prompt += f"ä»·æ ¼: {stock_data.get('nowPri', stock_data.get('price', 'N/A'))} | æ¶¨è·Œ: {stock_data.get('increase', stock_data.get('change', 'N/A'))}%\n"
        user_prompt += f"æˆäº¤: {stock_data.get('traAmount', stock_data.get('volume', 'N/A'))}\n"
        
        # é‡ç‚¹ï¼šå‰åºåˆ†æç»“æœï¼ˆä½¿ç”¨å®Œæ•´å†…å®¹ï¼‰
        summary_text = None
        if previous_outputs and len(previous_outputs) > 0:
            total_prev_len_for_summary = sum(len(str(output)) for output in previous_outputs.values() if output)
            if total_prev_len_for_summary > 3000:
                summary_text = await summarize_previous_outputs(agent_id, previous_outputs, stock_code)

        if previous_outputs and len(previous_outputs) > 0:
            if summary_text:
                user_prompt += "\nã€å›¢é˜Ÿæˆå‘˜å·²å®Œæˆçš„åˆ†ææ‘˜è¦ã€‘(è¯·åŸºäºæ­¤è¿›è¡Œæ·±åŒ–ï¼Œä¸è¦é‡å¤)\n"
                user_prompt += summary_text + "\n\n"
            else:
                user_prompt += "\nã€å›¢é˜Ÿæˆå‘˜å·²å®Œæˆçš„åˆ†æã€‘(è¯·åŸºäºæ­¤è¿›è¡Œæ·±åŒ–ï¼Œä¸è¦é‡å¤)\n"
                for agent_name, output in previous_outputs.items():
                    if output:
                        user_prompt += f">>> {get_agent_role(agent_name)} çš„ç»“è®º:\n{output}\n\n"
        else:
            user_prompt += "\nä½ æ˜¯ç¬¬ä¸€æ‰¹è¿›å…¥åˆ†æçš„ä¸“å®¶ï¼Œè¯·åŸºäºåŸå§‹å¸‚åœºæ•°æ®æ„å»ºåˆå§‹è§‚ç‚¹ã€‚\n"

        # è°ƒç”¨ç›¸åº”çš„AI API
        if provider == "GEMINI":
            req = GeminiRequest(
                prompt=user_prompt,
                systemPrompt=system_prompt,
                model=model_name,
                temperature=temperature
            )
            result = await gemini_api(req)
        elif provider == "DEEPSEEK":
            req = DeepSeekRequest(
                prompt=user_prompt,
                systemPrompt=system_prompt,
                model=model_name,
                temperature=temperature
            )
            result = await deepseek_api(req)
        elif provider == "QWEN":
            req = QwenRequest(
                prompt=user_prompt,
                systemPrompt=system_prompt,
                model=model_name,
                temperature=temperature
            )
            result = await qwen_api(req)
        else:
            # è·å–æ™ºèƒ½ä½“è§’è‰²ï¼ˆç”¨äºé™çº§ç­–ç•¥ï¼‰
            agent_role_map = {
                'news_analyst': 'NEWS',
                'fundamental': 'FUNDAMENTAL',
                'technical': 'TECHNICAL',
                'bull_researcher': 'BULL',
                'bear_researcher': 'BEAR',
                'risk_manager': 'RISK',
                'risk_aggressive': 'RISK',
                'risk_conservative': 'RISK',
                'risk_neutral': 'RISK',
                'research_manager': 'MANAGER',
                'trader': 'TRADER',
                'macro': 'MACRO',
                'industry': 'INDUSTRY',
                'funds': 'FUNDAMENTAL',
                'manager_fundamental': 'MANAGER',
                'manager_momentum': 'MANAGER',
                'risk_system': 'RISK',
                'risk_portfolio': 'RISK',
                'gm': 'MANAGER',
                'china_market': 'NEWS',
                'social_analyst': 'NEWS'
            }
            
            req = SiliconFlowRequest(
                model=model_name,
                systemPrompt=system_prompt,
                prompt=user_prompt,
                temperature=temperature,
                agentRole=agent_role_map.get(request.agent_id, 'UNKNOWN')  # æ·»åŠ æ™ºèƒ½ä½“è§’è‰²
            )
            # æ·»åŠ è¯¦ç»†æ—¥å¿—
            prompt_len = len(system_prompt) + len(user_prompt)
            print(f"[åˆ†æ] {request.agent_id} ç³»ç»Ÿæç¤ºè¯: {len(system_prompt)} å­—ç¬¦")
            print(f"[åˆ†æ] {request.agent_id} ç”¨æˆ·æç¤ºè¯: {len(user_prompt)} å­—ç¬¦")
            print(f"[åˆ†æ] {request.agent_id} æ€»é•¿åº¦: {prompt_len} å­—ç¬¦ (~{prompt_len//2} tokens)")
            print(f"[åˆ†æ] {request.agent_id} é™çº§è§’è‰²: {req.agentRole}")  # æ˜¾ç¤ºé™çº§è§’è‰²
            
            # æ‰“å°å‰åºè¾“å‡ºé•¿åº¦
            if previous_outputs:
                print(f"[åˆ†æ] {request.agent_id} å‰åºè¾“å‡ºæ•°é‡: {len(previous_outputs)}")
                total_prev_len = sum(len(output) for output in previous_outputs.values() if output)
                print(f"[åˆ†æ] {request.agent_id} å‰åºè¾“å‡ºæ€»é•¿åº¦: {total_prev_len} å­—ç¬¦")
                for agent_name, output in list(previous_outputs.items())[:3]:  # åªæ‰“å°å‰3ä¸ª
                    if output:
                        print(f"  - {agent_name}: {len(output)} å­—ç¬¦")
                if len(previous_outputs) > 3:
                    print(f"  ... è¿˜æœ‰ {len(previous_outputs)-3} ä¸ª")
            
            print(f"[åˆ†æ] {request.agent_id} è°ƒç”¨SiliconFlow API: {model_name}")
            result = await siliconflow_api(req)
        
        if result.get("success"):
            print(f"[åˆ†æ] {request.agent_id} åˆ†æå®Œæˆ")
            # å§‹ç»ˆè¿”å› fallback_levelï¼Œé»˜è®¤ä¸º 0ï¼ˆåŸå§‹è¯·æ±‚ï¼‰
            fallback_level = result.get("fallback_level", 0)
            return {
                "success": True,
                "result": result.get("text", ""),
                "fallback_level": fallback_level
            }
        else:
            print(f"[åˆ†æ] {request.agent_id} åˆ†æå¤±è´¥: {result.get('error')}")
            return {"success": False, "error": result.get("error", "åˆ†æå¤±è´¥")}
            
    except Exception as e:
        import traceback
        print(f"[Analyze] {request.agent_id} é”™è¯¯: {str(e)}")
        print(traceback.format_exc())
        return {"success": False, "error": str(e)}


def get_agent_role(agent_id):
    """æ ¹æ®æ™ºèƒ½ä½“IDè·å–è§’è‰²æè¿°"""
    roles = {
        "macro": "å®è§‚ç»æµåˆ†æå¸ˆ",
        "industry": "è¡Œä¸šç ”ç©¶åˆ†æå¸ˆ",
        "technical": "æŠ€æœ¯åˆ†æå¸ˆ",
        "funds": "èµ„é‡‘æµå‘åˆ†æå¸ˆ",
        "fundamental": "åŸºæœ¬é¢åˆ†æå¸ˆ",
        "manager_fundamental": "åŸºæœ¬é¢æŠ•èµ„ç»ç†",
        "manager_momentum": "åŠ¨é‡æŠ•èµ„ç»ç†",
        "risk_system": "ç³»ç»Ÿæ€§é£é™©æ€»ç›‘",
        "risk_portfolio": "ç»„åˆé£é™©æ€»ç›‘",
        "gm": "æŠ•èµ„å†³ç­–æ€»ç»ç†"
    }
    return roles.get(agent_id, "æŠ•èµ„åˆ†æå¸ˆ")

# ==================== è‚¡ç¥¨æ•°æ® API ====================

@app.post("/api/stock/{symbol}")
@app.get("/api/stock/{symbol}")
async def stock_data(symbol: str, request: Optional[StockRequest] = None):
    """è‚¡ç¥¨æ•°æ®API - ä¼˜åŒ–ç‰ˆAKShareä¼˜å…ˆ"""
    try:
        # ä½¿ç”¨ä¼˜åŒ–åçš„é€‚é…å™¨
        from backend.dataflows.stock_data_adapter_optimized import StockDataAdapter
        
        print(f"[Stock API] å¼€å§‹è·å–{symbol}çš„æ•°æ®...")
        print(f"[Stock API] ä½¿ç”¨ä¼˜åŒ–çš„AKShareæ¥å£")
        
        # ä½¿ç”¨é€‚é…å™¨è·å–æ•°æ®
        adapter = StockDataAdapter()
        # è°ƒç”¨å¼‚æ­¥æ–¹æ³•
        result = await adapter.get_stock_data_async(symbol)
        
        print(f"[Stock API] æˆåŠŸä½¿ç”¨æ•°æ®æº: {result.get('data_source')}")
        print(f"[Stock API] ç»“æœ: {result.get('name')} ä»·æ ¼=Â¥{result.get('price')} æ¶¨è·Œå¹…={result.get('change')}%")
        
        return result
    
    except Exception as e:
        import traceback
        print(f"[Stock API] âŒ é”™è¯¯: {str(e)}")
        print(traceback.format_exc())
        return {"success": False, "error": str(e)}

# ==================== é…ç½®ç®¡ç† API ====================

@app.get("/api/config")
async def get_config():
    """è·å–é…ç½®ä¿¡æ¯ï¼ˆè¿”å›å®é™…çš„ API Keysï¼‰"""
    config = {
        "api_keys": {},
        "model_configs": [],
        "backend_status": "running",
        "endpoints": list(API_ENDPOINTS.keys())
    }
    
    # è¿”å›å®é™…çš„ API Keys
    if API_KEYS.get("gemini"):
        config["api_keys"]["gemini"] = API_KEYS["gemini"]
        config["GEMINI_API_KEY"] = API_KEYS["gemini"]
    
    if API_KEYS.get("deepseek"):
        config["api_keys"]["deepseek"] = API_KEYS["deepseek"]
        config["DEEPSEEK_API_KEY"] = API_KEYS["deepseek"]
    
    if API_KEYS.get("qwen"):
        config["api_keys"]["qwen"] = API_KEYS["qwen"]
        config["DASHSCOPE_API_KEY"] = API_KEYS["qwen"]
        
    if API_KEYS.get("siliconflow"):
        config["api_keys"]["siliconflow"] = API_KEYS["siliconflow"]
        config["SILICONFLOW_API_KEY"] = API_KEYS["siliconflow"]
        
    if API_KEYS.get("juhe"):
        config["api_keys"]["juhe"] = API_KEYS["juhe"]
        config["JUHE_API_KEY"] = API_KEYS["juhe"]
    
    # æ·»åŠ æ•°æ®æ¸ é“é…ç½®
    if API_KEYS.get("finnhub"):
        config["api_keys"]["finnhub"] = API_KEYS["finnhub"]
        config["FINNHUB_API_KEY"] = API_KEYS["finnhub"]
    
    if API_KEYS.get("tushare"):
        config["api_keys"]["tushare"] = API_KEYS["tushare"]
        config["TUSHARE_TOKEN"] = API_KEYS["tushare"]

    # æ·»åŠ å·¨æ½®APIé…ç½®ï¼ˆä½¿ç”¨å®˜æ–¹å‘½åï¼šAccess Key, Access Secret, Access Tokenï¼‰
    if API_KEYS.get("cninfo_access_key"):
        config["api_keys"]["cninfo_access_key"] = API_KEYS["cninfo_access_key"]
        config["CNINFO_ACCESS_KEY"] = API_KEYS["cninfo_access_key"]
    if API_KEYS.get("cninfo_access_secret"):
        config["api_keys"]["cninfo_access_secret"] = API_KEYS["cninfo_access_secret"]
        config["CNINFO_ACCESS_SECRET"] = API_KEYS["cninfo_access_secret"]
    if API_KEYS.get("cninfo_access_token"):
        config["api_keys"]["cninfo_access_token"] = API_KEYS["cninfo_access_token"]
        config["CNINFO_ACCESS_TOKEN"] = API_KEYS["cninfo_access_token"]

    # å°è¯•ä»æ–‡ä»¶åŠ è½½æ¨¡å‹é…ç½®
    try:
        config_file = os.path.join(os.path.dirname(__file__), 'agent_configs.json')
        if os.path.exists(config_file):
            with open(config_file, 'r', encoding='utf-8') as f:
                saved_config = json.load(f)
                if 'model_configs' in saved_config:
                    config["model_configs"] = saved_config['model_configs']
        else:
            # ä½¿ç”¨é»˜è®¤æ¨¡å‹é…ç½®
            config["model_configs"] = [
                {"id": "macro", "model_name": "gemini-2.0-flash-exp", "temperature": 0.3},
                {"id": "industry", "model_name": "deepseek-chat", "temperature": 0.3},
                {"id": "technical", "model_name": "qwen-plus", "temperature": 0.2},
                {"id": "funds", "model_name": "Qwen/Qwen2.5-7B-Instruct", "temperature": 0.2},
                {"id": "fundamental", "model_name": "deepseek-chat", "temperature": 0.3}
            ]
    except Exception as e:
        print(f"åŠ è½½æ¨¡å‹é…ç½®å¤±è´¥: {e}")
        
    return config

@app.post("/api/config")
async def save_config(request: Dict[str, Any]):
    """ä¿å­˜ API Keys é…ç½®"""
    try:
        api_keys = request.get('api_keys', {})
        global API_KEYS
        
        # æ›´æ–°å…¨å±€ API_KEYS
        for key, value in api_keys.items():
            if value:  # åªæ›´æ–°éç©ºå€¼
                API_KEYS[key] = value
        
        print(f"[Config] API Keys å·²æ›´æ–°: {list(api_keys.keys())}")
        return {"success": True, "message": "API é…ç½®å·²ä¿å­˜"}
    except Exception as e:
        print(f"[Config] ä¿å­˜å¤±è´¥: {str(e)}")
        return {"success": False, "error": str(e)}

@app.post("/api/config/update")
async def update_config(keys: Dict[str, str]):
    """åŠ¨æ€æ›´æ–° API Keysï¼ˆä»…é™å¼€å‘ç¯å¢ƒï¼‰"""
    global API_KEYS
    for key, value in keys.items():
        if key in API_KEYS and value:
            API_KEYS[key] = value
    return {"success": True, "message": "é…ç½®å·²æ›´æ–°"}

@app.post("/api/config/agents")
async def save_agent_configs(config_data: Dict[str, Any]):
    """ä¿å­˜æ™ºèƒ½ä½“é…ç½®åˆ°æ–‡ä»¶ï¼ˆåŒ…æ‹¬æ¨¡å‹é€‰æ‹©ï¼‰"""
    try:
        config_file = os.path.join(os.path.dirname(__file__), "agent_configs.json")
        with open(config_file, 'w', encoding='utf-8') as f:
            json.dump(config_data, f, ensure_ascii=False, indent=2)
        
        agent_count = len(config_data.get('agents', []))
        model_count = len(config_data.get('selectedModels', []))
        summarizer = config_data.get('summarizerModel', 'N/A')
        print(f"[é…ç½®] å·²ä¿å­˜ {agent_count} ä¸ªæ™ºèƒ½ä½“é…ç½®å’Œ {model_count} ä¸ªæ¨¡å‹é€‰æ‹©")
        print(f"[é…ç½®] æ‘˜è¦å™¨æ¨¡å‹: {summarizer}")
        return {"success": True, "message": "é…ç½®å·²ä¿å­˜"}
    except Exception as e:
        print(f"[é…ç½®] ä¿å­˜å¤±è´¥: {str(e)}")
        return {"success": False, "error": str(e)}

@app.get("/api/config/agents")
async def load_agent_configs():
    """ä»æ–‡ä»¶åŠ è½½æ™ºèƒ½ä½“é…ç½®ï¼ˆåŒ…æ‹¬æ¨¡å‹é€‰æ‹©ï¼‰"""
    try:
        config_file = os.path.join(os.path.dirname(__file__), "agent_configs.json")
        if os.path.exists(config_file):
            with open(config_file, 'r', encoding='utf-8') as f:
                config_data = json.load(f)
            
            # å…¼å®¹æ—§æ ¼å¼ï¼ˆç›´æ¥æ˜¯æ•°ç»„ï¼‰
            if isinstance(config_data, list):
                config_data = {"agents": config_data, "selectedModels": []}
            
            agent_count = len(config_data.get('agents', []))
            model_count = len(config_data.get('selectedModels', []))
            print(f"[é…ç½®] å·²åŠ è½½ {agent_count} ä¸ªæ™ºèƒ½ä½“é…ç½®å’Œ {model_count} ä¸ªæ¨¡å‹é€‰æ‹©")
            return {"success": True, "data": config_data}
        else:
            return {"success": True, "data": {"agents": [], "selectedModels": []}}
    except Exception as e:
        print(f"[é…ç½®] åŠ è½½å¤±è´¥: {str(e)}")
        return {"success": False, "error": str(e)}


async def _check_tushare_points(pro) -> dict:
    """
    æ£€æµ‹ Tushare Token çš„ç§¯åˆ†æƒé™

    é€šè¿‡æµ‹è¯•å„æ¥å£æ¥ä¼°ç®—è´¦æˆ·ç§¯åˆ†ç­‰çº§

    Returns:
        dict: {'summary': 'ç§¯åˆ†æ‘˜è¦', 'details': 'è¯¦ç»†ä¿¡æ¯', 'estimated_points': ç§¯åˆ†æ•°}
    """
    from datetime import datetime, timedelta
    import asyncio

    # å®šä¹‰è¦æ£€æµ‹çš„æ¥å£åŠå…¶æ‰€éœ€ç§¯åˆ†
    interfaces = {
        'stock_basic': {'points': 0, 'desc': 'è‚¡ç¥¨åˆ—è¡¨', 'group': 'åŸºç¡€'},
        'daily': {'points': 0, 'desc': 'æ—¥çº¿è¡Œæƒ…', 'group': 'åŸºç¡€'},
        'daily_basic': {'points': 120, 'desc': 'æ¯æ—¥æŒ‡æ ‡', 'group': 'è¿›é˜¶'},
        'income': {'points': 500, 'desc': 'åˆ©æ¶¦è¡¨', 'group': 'è´¢åŠ¡'},
        'fina_indicator': {'points': 500, 'desc': 'è´¢åŠ¡æŒ‡æ ‡', 'group': 'è´¢åŠ¡'},
        'pledge_detail': {'points': 2000, 'desc': 'è´¨æŠ¼æ˜ç»†', 'group': 'é«˜çº§'},
        'stk_holdertrade': {'points': 2000, 'desc': 'è‚¡ä¸œå¢å‡æŒ', 'group': 'é«˜çº§'},
        'top_inst': {'points': 2000, 'desc': 'æœºæ„é¾™è™æ¦œ', 'group': 'é«˜çº§'},
        'stk_rewards': {'points': 5000, 'desc': 'ç®¡ç†å±‚è–ªé…¬', 'group': 'VIP'},
    }

    results = {}
    today = datetime.now().strftime('%Y%m%d')
    yesterday = (datetime.now() - timedelta(days=1)).strftime('%Y%m%d')

    def test_interface(interface):
        """åŒæ­¥æµ‹è¯•å•ä¸ªæ¥å£"""
        try:
            if interface == 'stock_basic':
                data = pro.stock_basic(list_status='L', limit=1)
            elif interface == 'daily':
                data = pro.daily(ts_code='000001.SZ', start_date=yesterday, end_date=today)
            elif interface == 'daily_basic':
                data = pro.daily_basic(trade_date=yesterday, limit=1)
            elif interface in ['income', 'fina_indicator']:
                data = getattr(pro, interface)(ts_code='000001.SZ', limit=1)
            elif interface == 'pledge_detail':
                data = pro.pledge_detail(ts_code='000001.SZ')
            elif interface == 'stk_holdertrade':
                data = pro.stk_holdertrade(ts_code='000001.SZ', start_date='20240101', end_date=today)
            elif interface == 'top_inst':
                data = pro.top_inst(trade_date=yesterday)
            elif interface == 'stk_rewards':
                data = pro.stk_rewards(ts_code='000001.SZ', end_date='20231231')
            else:
                data = None

            return data is not None and (not hasattr(data, 'empty') or not data.empty)
        except Exception as e:
            error_msg = str(e).lower()
            # ç§¯åˆ†ä¸è¶³æˆ–æƒé™ä¸è¶³
            if 'ç§¯åˆ†' in error_msg or 'point' in error_msg or 'æƒé™' in error_msg:
                return False
            # å…¶ä»–é”™è¯¯ï¼ˆå¦‚ç½‘ç»œé—®é¢˜ï¼‰ä¹Ÿè§†ä¸ºä¸å¯ç”¨
            return False

    # åœ¨çº¿ç¨‹æ± ä¸­å¹¶è¡Œæµ‹è¯•æ¥å£
    import concurrent.futures
    with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:
        future_to_interface = {
            executor.submit(test_interface, interface): interface
            for interface in interfaces.keys()
        }
        for future in concurrent.futures.as_completed(future_to_interface):
            interface = future_to_interface[future]
            try:
                results[interface] = future.result()
            except Exception:
                results[interface] = False

    # ç»Ÿè®¡ç»“æœ
    available_count = sum(1 for v in results.values() if v)
    total_count = len(results)

    # ä¼°ç®—ç§¯åˆ†
    estimated_points = 0
    for interface, available in results.items():
        if available:
            estimated_points = max(estimated_points, interfaces[interface]['points'])

    # ç”Ÿæˆè¯¦ç»†ä¿¡æ¯
    details_lines = []
    groups = {'åŸºç¡€': [], 'è¿›é˜¶': [], 'è´¢åŠ¡': [], 'é«˜çº§': [], 'VIP': []}

    for interface, info in interfaces.items():
        status = 'âœ…' if results.get(interface) else 'âŒ'
        groups[info['group']].append(f"  {status} {info['desc']} ({info['points']}åˆ†)")

    for group_name, items in groups.items():
        if items:
            details_lines.append(f"ã€{group_name}æ¥å£ã€‘")
            details_lines.extend(items)

    # ç”Ÿæˆæ‘˜è¦
    if estimated_points >= 5000:
        level = "VIPä¼šå‘˜ (5000+ç§¯åˆ†)"
        level_emoji = "ğŸ‘‘"
    elif estimated_points >= 2000:
        level = "é«˜çº§ä¼šå‘˜ (2000+ç§¯åˆ†)"
        level_emoji = "â­"
    elif estimated_points >= 500:
        level = "æ ‡å‡†ä¼šå‘˜ (500+ç§¯åˆ†)"
        level_emoji = "ğŸ“Š"
    elif estimated_points >= 120:
        level = "è¿›é˜¶ä¼šå‘˜ (120+ç§¯åˆ†)"
        level_emoji = "ğŸ“ˆ"
    else:
        level = "åŸºç¡€ä¼šå‘˜ (0ç§¯åˆ†)"
        level_emoji = "ğŸ“‹"

    summary = f"{level_emoji} è´¦æˆ·ç­‰çº§: {level}\nğŸ“Š æ¥å£å¯ç”¨: {available_count}/{total_count}"

    return {
        'summary': summary,
        'details': '\n'.join(details_lines),
        'estimated_points': estimated_points,
        'available_count': available_count,
        'total_count': total_count
    }


class TestApiRequest(BaseModel):
    api_key: str

@app.post("/api/test/{provider}")
async def test_api_connection(provider: str, request: TestApiRequest):
    """æµ‹è¯• API è¿æ¥å¹¶è¿”å›çœŸå®å“åº”ç¤ºä¾‹"""
    api_key = request.api_key
    
    # å¤„ç†ç‰¹æ®Šæƒ…å†µ
    if provider == 'akshare':
        # AKShare ä¸éœ€è¦ API Key
        api_key = None
    elif not api_key or api_key.strip() == '':
        return {"success": False, "error": f"è¯·å…ˆè¾“å…¥ {provider} çš„ API Key"}
    
    # æ ¹æ® provider è¿›è¡Œä¸åŒçš„æµ‹è¯•
    try:
        if provider == 'gemini':
            # æµ‹è¯• Gemini API
            try:
                test_url = f"{API_ENDPOINTS['gemini']}/models/gemini-1.5-flash:generateContent?key={api_key}"
                client = http_clients.get('gemini', http_clients['default'])
                response = await client.post(
                    test_url,
                    json={"contents": [{"parts": [{"text": "Hello, this is a test message."}]}]},
                    timeout=15.0
                )
            except Exception as e:
                error_msg = str(e)
                if 'ConnectTimeout' in error_msg or 'timeout' in error_msg.lower():
                    return {"success": False, "error": "è¿æ¥è¶…æ—¶ã€‚Gemini API å¯èƒ½éœ€è¦ä»£ç†è®¿é—®ï¼Œæˆ–ç½‘ç»œä¸ç¨³å®šã€‚"}
                elif 'ConnectError' in error_msg:
                    return {"success": False, "error": "æ— æ³•è¿æ¥åˆ° Gemini æœåŠ¡å™¨ã€‚è¯·æ£€æŸ¥ç½‘ç»œæˆ–ä»£ç†è®¾ç½®ã€‚"}
                else:
                    return {"success": False, "error": f"è¿æ¥é”™è¯¯: {error_msg[:100]}"}
            if response.status_code == 200:
                result = response.json()
                # æå–å“åº”æ–‡æœ¬
                response_text = ""
                if 'candidates' in result and len(result['candidates']) > 0:
                    candidate = result['candidates'][0]
                    if 'content' in candidate and 'parts' in candidate['content']:
                        response_text = candidate['content']['parts'][0].get('text', '')
                return {
                    "success": True, 
                    "message": "Gemini API è¿æ¥æˆåŠŸï¼",
                    "test_response": response_text[:200] if response_text else "æ¨¡å‹å“åº”æˆåŠŸ"
                }
            else:
                return {"success": False, "error": f"HTTP {response.status_code}: {response.text[:200]}"}
                
        elif provider == 'deepseek':
            # æµ‹è¯• DeepSeek API
            client = http_clients.get('deepseek', http_clients['default'])
            response = await client.post(
                f"{API_ENDPOINTS['deepseek']}/chat/completions",
                headers={"Authorization": f"Bearer {api_key}"},
                json={
                    "model": "deepseek-chat",
                    "messages": [{"role": "user", "content": "Say hello in Chinese"}],
                    "max_tokens": 50
                },
                timeout=15.0
            )
            if response.status_code == 200:
                result = response.json()
                response_text = result.get('choices', [{}])[0].get('message', {}).get('content', '')
                return {
                    "success": True, 
                    "message": "DeepSeek API è¿æ¥æˆåŠŸï¼",
                    "test_response": response_text[:200] if response_text else "æ¨¡å‹å“åº”æˆåŠŸ"
                }
            else:
                return {"success": False, "error": f"HTTP {response.status_code}: {response.text[:200]}"}
                
        elif provider == 'qwen':
            # æµ‹è¯•é€šä¹‰åƒé—® API
            client = http_clients.get('qwen', http_clients['default'])
            response = await client.post(
                API_ENDPOINTS['qwen'],
                headers={"Authorization": f"Bearer {api_key}"},
                json={
                    "model": "qwen-turbo",
                    "input": {"messages": [{"role": "user", "content": "ä½ å¥½ï¼Œè¯·ç”¨ä¸­æ–‡é—®å¥½"}]}
                },
                timeout=15.0
            )
            if response.status_code == 200:
                result = response.json()
                response_text = result.get('output', {}).get('text', '')
                return {
                    "success": True, 
                    "message": "é€šä¹‰åƒé—® API è¿æ¥æˆåŠŸï¼",
                    "test_response": response_text[:200] if response_text else "æ¨¡å‹å“åº”æˆåŠŸ"
                }
            else:
                return {"success": False, "error": f"HTTP {response.status_code}: {response.text[:200]}"}
                
        elif provider == 'siliconflow':
            # æµ‹è¯•ç¡…åŸºæµåŠ¨ API - å…ˆè·å–æ¨¡å‹åˆ—è¡¨ï¼Œå†æµ‹è¯•å¯¹è¯
            client = http_clients.get('siliconflow', http_clients['default'])
            # ç¬¬ä¸€æ­¥ï¼šè·å–æ¨¡å‹åˆ—è¡¨
            response = await client.get(
                API_ENDPOINTS['siliconflow_models'],
                headers={"Authorization": f"Bearer {api_key}"},
                timeout=10.0
            )
            if response.status_code != 200:
                return {"success": False, "error": f"HTTP {response.status_code}: {response.text[:200]}"}
            
            models_data = response.json()
            model_count = len(models_data.get('data', []))
            
            # ç¬¬äºŒæ­¥ï¼šæµ‹è¯•å¯¹è¯ API
            chat_response = await client.post(
                API_ENDPOINTS['siliconflow'],
                headers={"Authorization": f"Bearer {api_key}"},
                json={
                    "model": "Qwen/Qwen2.5-7B-Instruct",
                    "messages": [{"role": "user", "content": "ä½ å¥½"}],
                    "max_tokens": 50
                },
                timeout=15.0
            )
            
            if chat_response.status_code == 200:
                result = chat_response.json()
                response_text = result.get('choices', [{}])[0].get('message', {}).get('content', '')
                return {
                    "success": True, 
                    "message": f"ç¡…åŸºæµåŠ¨ API è¿æ¥æˆåŠŸï¼å¯ç”¨æ¨¡å‹: {model_count}ä¸ª",
                    "test_response": response_text[:200] if response_text else "æ¨¡å‹å“åº”æˆåŠŸ"
                }
            else:
                return {"success": False, "error": f"Chat API HTTP {chat_response.status_code}: {chat_response.text[:200]}"}
                
        elif provider == 'juhe':
            # æµ‹è¯•èšåˆæ•°æ® API - è·å–èŒ…å°è‚¡ç¥¨æ•°æ®
            client = http_clients.get('juhe', http_clients['default'])
            response = await client.get(
                f"{API_ENDPOINTS['juhe']}?gid=sh600519&key={api_key}",
                timeout=10.0
            )
            if response.status_code == 200:
                result = response.json()
                if result.get('error_code') == 0:
                    stock_data = result.get('result', [{}])[0]
                    stock_name = stock_data.get('name', '')
                    stock_price = stock_data.get('nowPri', '')
                    return {
                        "success": True, 
                        "message": "èšåˆæ•°æ® API è¿æ¥æˆåŠŸï¼",
                        "test_response": f"è·å–è‚¡ç¥¨æ•°æ®æˆåŠŸ: {stock_name} ç°ä»· {stock_price}"
                    }
                else:
                    return {"success": False, "error": result.get('reason', 'æœªçŸ¥é”™è¯¯')}
            else:
                return {"success": False, "error": f"HTTP {response.status_code}"}
                
        elif provider == 'news':
            # æµ‹è¯•è´¢ç»æ–°é—» API - æ¨¡æ‹Ÿæµ‹è¯•
            return {
                "success": True,
                "message": "è´¢ç»æ–°é—» API é…ç½®å·²ä¿å­˜ï¼",
                "test_response": "æ–°é—»æ•°æ®æºå°†åœ¨åˆ†ææ—¶è‡ªåŠ¨è°ƒç”¨"
            }
            
        elif provider == 'crawler':
            # æµ‹è¯•ç½‘é¡µçˆ¬è™« - æ¨¡æ‹Ÿæµ‹è¯•
            return {
                "success": True,
                "message": "ç½‘é¡µçˆ¬è™«æœåŠ¡é…ç½®å·²ä¿å­˜ï¼",
                "test_response": "çˆ¬è™«æœåŠ¡å°†åœ¨éœ€è¦æ—¶è‡ªåŠ¨å¯åŠ¨"
            }
            
        elif provider == 'finnhub':
            # æµ‹è¯• FinnHub API
            client = http_clients.get('finnhub', http_clients['default'])
            response = await client.get(
                f"https://finnhub.io/api/v1/quote?symbol=AAPL&token={api_key}",
                timeout=10.0
            )
            if response.status_code == 200:
                result = response.json()
                if 'c' in result:  # current price
                    return {
                        "success": True,
                        "message": "FinnHub API è¿æ¥æˆåŠŸï¼",
                        "test_response": f"AAPL å½“å‰ä»·æ ¼: ${result['c']}"
                    }
                else:
                    return {"success": False, "error": "æ— æ•ˆçš„ API å“åº”"}
            else:
                return {"success": False, "error": f"HTTP {response.status_code}: {response.text[:200]}"}
                
        elif provider == 'tushare':
            # æµ‹è¯• Tushare API å¹¶æ£€æµ‹ç§¯åˆ†
            try:
                import tushare as ts
                ts.set_token(api_key)
                pro = ts.pro_api()

                # æµ‹è¯•åŸºç¡€æ¥å£
                df = pro.trade_cal(exchange='SSE', start_date='20240101', end_date='20240110')
                if df is None or len(df) == 0:
                    return {"success": False, "error": "æ— æ³•è·å–æ•°æ®ï¼ŒToken å¯èƒ½æ— æ•ˆ"}

                # æ£€æµ‹å„æ¥å£æƒé™æ¥ä¼°ç®—ç§¯åˆ†
                points_info = await _check_tushare_points(pro)

                return {
                    "success": True,
                    "message": f"Tushare API è¿æ¥æˆåŠŸï¼",
                    "test_response": f"Token æœ‰æ•ˆ âœ…\n\n{points_info['summary']}\n\næ¥å£æƒé™è¯¦æƒ…:\n{points_info['details']}"
                }
            except ImportError:
                return {"success": False, "error": "Tushare æœªå®‰è£…ã€‚è¯·è¿è¡Œ: pip install tushare"}
            except Exception as e:
                error_msg = str(e)
                if 'æƒé™' in error_msg or 'permission' in error_msg.lower():
                    return {"success": False, "error": "Token æƒé™ä¸è¶³ã€‚è¯·è®¿é—® https://tushare.pro è·å–ç§¯åˆ†è§£é”æƒé™ã€‚"}
                elif 'token' in error_msg.lower():
                    return {"success": False, "error": "Token æ— æ•ˆã€‚è¯·æ£€æŸ¥ Tushare Token æ˜¯å¦æ­£ç¡®ã€‚"}
                else:
                    return {"success": False, "error": f"Tushare é”™è¯¯: {error_msg[:100]}"}
                
        elif provider == 'akshare':
            # æµ‹è¯• AKShare - ä¸éœ€è¦ API Keyï¼Œç›´æ¥æ£€æŸ¥æ¨¡å—æ˜¯å¦å¯ç”¨
            try:
                import akshare as ak
                # åªæ£€æŸ¥æ¨¡å—æ˜¯å¦å®‰è£…ï¼Œä¸è¿›è¡Œå®é™…ç½‘ç»œè¯·æ±‚
                # å› ä¸º AKShare çš„æ•°æ®æºæœåŠ¡å™¨ä¸ç¨³å®šï¼Œæµ‹è¯•è¿æ¥å¸¸å¸¸å¤±è´¥
                # ä½†å®é™…ä½¿ç”¨æ—¶ä¼šè‡ªåŠ¨é‡è¯•ï¼Œæ‰€ä»¥åªéœ€ç¡®è®¤æ¨¡å—å­˜åœ¨å³å¯
                if hasattr(ak, 'stock_zh_a_spot_em'):
                    return {
                        "success": True,
                        "message": "AKShare æ¨¡å—å·²å®‰è£…ï¼Œå¯ä»¥ä½¿ç”¨ï¼",
                        "test_response": "AKShare æ˜¯å¼€æºé‡‘èæ•°æ®åº“ï¼Œæ— éœ€ API Keyã€‚å®é™…ä½¿ç”¨æ—¶ä¼šè‡ªåŠ¨è·å–æ•°æ®ã€‚"
                    }
                else:
                    return {"success": False, "error": "AKShare ç‰ˆæœ¬è¿‡æ—§ï¼Œè¯·å‡çº§: pip install --upgrade akshare"}
            except ImportError:
                return {"success": False, "error": "AKShare æœªå®‰è£…ã€‚è¯·è¿è¡Œ: pip install akshare"}
            except Exception as e:
                return {"success": False, "error": f"AKShare æ£€æŸ¥å¤±è´¥: {str(e)[:100]}"}
        else:
            return {"success": False, "error": f"ä¸æ”¯æŒçš„ provider: {provider}"}
            
    except Exception as e:
        import traceback
        print(f"[Test API] {provider} æµ‹è¯•å¤±è´¥: {str(e)}")
        print(traceback.format_exc())
        return {"success": False, "error": str(e)}

# ==================== é™æ€æ–‡ä»¶æœåŠ¡ ====================

# æŒ‚è½½é™æ€æ–‡ä»¶ç›®å½•ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
import os.path
static_dir = os.path.join(os.path.dirname(__file__), "static")
if os.path.exists(static_dir):
    app.mount("/static", StaticFiles(directory=static_dir), name="static")

# ==================== å¥åº·æ£€æŸ¥ ====================

@app.get("/")
async def root():
    """æ ¹è·¯å¾„ - è¿”å› HTML é¡µé¢ï¼ˆå¦‚æœå­˜åœ¨ï¼‰"""
    html_file = os.path.join(static_dir, "index.html")
    if os.path.exists(html_file):
        return FileResponse(html_file)
    else:
        return {
            "status": "running",
            "service": "IcySaint AI Backend",
            "version": "1.0.0",
            "endpoints": [
                "/api/ai/gemini",
                "/api/ai/deepseek",
                "/api/ai/qwen",
                "/api/ai/siliconflow",
                "/api/ai/siliconflow-models",
                "/api/analyze",
                "/api/stock/{symbol}",
                "/api/config"
            ]
        }

@app.get("/health")
@app.get("/api/health")
async def health_check():
    """å¥åº·æ£€æŸ¥ç«¯ç‚¹"""
    return {"status": "healthy"}

# ==================== å¯åŠ¨æœåŠ¡å™¨ ====================

if __name__ == "__main__":
    print("""
    â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
    â•‘         IcySaint AI - Python Backend Server          â•‘
    â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)
    
    # æ£€æŸ¥ API Keys é…ç½®ï¼ˆä½¿ç”¨å…¨å±€çš„API_KEYSï¼Œä¸è¦é‡æ–°èµ‹å€¼ï¼‰
    print("ğŸ“‹ API Keys é…ç½®çŠ¶æ€:")
    for name, key in API_KEYS.items():
        status = "âœ… å·²é…ç½®" if key else "âŒ æœªé…ç½®"
        print(f"  {name.upper()}: {status}")
    
    print("\nğŸš€ å¯åŠ¨æœåŠ¡å™¨...")
    print("ğŸ“ åç«¯API: http://localhost:8000")
    print("ğŸ“ Vueå‰ç«¯: http://localhost:8080")
    print("ğŸ“ APIæ–‡æ¡£: http://localhost:8000/docs")
    print("\nâœ¨ æ¶æ„: FastAPIåç«¯ + Vue3å‰ç«¯")
    print("ğŸ’¡ æç¤º: è¯·ç¡®ä¿Vueå‰ç«¯ä¹Ÿåœ¨è¿è¡Œ (npm run serve)")
    print("ğŸ¯ ä½¿ç”¨ scripts/dev.py å¯ä¸€é”®å¯åŠ¨å‰åç«¯ï¼")
    print("-" * 60)
    
    # å¯åŠ¨æœåŠ¡å™¨
    uvicorn.run(
        app,  # ç›´æ¥ä½¿ç”¨appå¯¹è±¡è€Œä¸æ˜¯å­—ç¬¦ä¸²å¯¼å…¥
        host="0.0.0.0",
        port=8000,
        reload=False,  # å…³é—­è‡ªåŠ¨é‡è½½ä»¥é¿å…CORSé—®é¢˜
        log_level="info"
    )
