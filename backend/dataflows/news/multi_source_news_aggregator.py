"""
å¤šæºæ–°é—»èšåˆå™¨
æ•´åˆTushareã€AKShareã€ä¸œæ–¹è´¢å¯Œç­‰å¤šä¸ªæ•°æ®æºçš„æ–°é—»
æ”¯æŒæ™ºèƒ½å†…å®¹æˆªå–å’Œå…³é”®è¯é«˜äº®
"""

import os
import re
from datetime import datetime, timedelta
from typing import List, Dict, Optional
import pandas as pd

from backend.utils.logging_config import get_logger

logger = get_logger("news.multi_source")


def extract_relevant_content(content: str, stock_code: str, stock_name: str = '', max_length: int = 300) -> str:
    """
    æ™ºèƒ½æå–ä¸è‚¡ç¥¨ç›¸å…³çš„å†…å®¹ç‰‡æ®µ

    Args:
        content: åŸå§‹æ–°é—»å†…å®¹
        stock_code: è‚¡ç¥¨ä»£ç  (å¦‚ 600519)
        stock_name: è‚¡ç¥¨åç§° (å¦‚ è´µå·èŒ…å°)
        max_length: æœ€å¤§è¿”å›é•¿åº¦

    Returns:
        ä¸è‚¡ç¥¨ç›¸å…³çš„å†…å®¹ç‰‡æ®µ
    """
    if not content:
        return ''

    # æ„å»ºå…³é”®è¯åˆ—è¡¨
    keywords = [stock_code]
    if stock_name:
        keywords.append(stock_name)
        # æ·»åŠ ç®€ç§° (å¦‚ "èŒ…å°")
        if len(stock_name) >= 4:
            keywords.append(stock_name[2:])

    # æŒ‰å¥å­åˆ†å‰²
    sentences = re.split(r'[ã€‚ï¼ï¼Ÿ\n]', content)

    # æŸ¥æ‰¾åŒ…å«å…³é”®è¯çš„å¥å­
    relevant_sentences = []
    for sentence in sentences:
        sentence = sentence.strip()
        if not sentence or len(sentence) < 5:
            continue
        if any(kw in sentence for kw in keywords):
            relevant_sentences.append(sentence)

    # å¦‚æœæ‰¾åˆ°ç›¸å…³å¥å­ï¼Œè¿”å›è¿™äº›å¥å­
    if relevant_sentences:
        result = 'ã€‚'.join(relevant_sentences[:3])
        if len(result) > max_length:
            result = result[:max_length] + '...'
        return result + 'ã€‚'

    # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ç›¸å…³å¥å­ï¼Œè¿”å›å¼€å¤´å†…å®¹
    if len(content) > max_length:
        return content[:max_length] + '...'
    return content


class MultiSourceNewsAggregator:
    """å¤šæºæ–°é—»èšåˆå™¨"""
    
    def __init__(self):
        self.tushare_token = os.getenv('TUSHARE_TOKEN', '')
        self.tushare_api = None
        
        # åˆå§‹åŒ–Tushare
        if self.tushare_token:
            try:
                import tushare as ts
                ts.set_token(self.tushare_token)
                self.tushare_api = ts.pro_api()
                logger.info("âœ… Tushareæ–°é—»APIåˆå§‹åŒ–æˆåŠŸ")
            except Exception as e:
                logger.error(f"âŒ Tushareåˆå§‹åŒ–å¤±è´¥: {e}")
        
        logger.info("å¤šæºæ–°é—»èšåˆå™¨åˆå§‹åŒ–å®Œæˆ")
    
    def get_stock_news_tushare(
        self, 
        ts_code: str, 
        start_date: Optional[str] = None,
        end_date: Optional[str] = None,
        limit: int = 20
    ) -> List[Dict]:
        """
        ä»Tushareè·å–è‚¡ç¥¨æ–°é—»
        
        æ³¨æ„: Tushareçš„æ–°é—»æ¥å£éœ€è¦è¾ƒé«˜ç§¯åˆ†(5000+)
        æ¥å£: news (éœ€è¦5000ç§¯åˆ†)
        
        Args:
            ts_code: è‚¡ç¥¨ä»£ç ï¼Œå¦‚600519.SH
            start_date: å¼€å§‹æ—¥æœŸ YYYYMMDD
            end_date: ç»“æŸæ—¥æœŸ YYYYMMDD
            limit: è¿”å›æ•°é‡é™åˆ¶
        """
        if not self.tushare_api:
            logger.warning("âš ï¸ Tushare APIä¸å¯ç”¨")
            return []
        
        try:
            if not end_date:
                end_date = datetime.now().strftime('%Y%m%d')
            if not start_date:
                start_date = (datetime.now() - timedelta(days=30)).strftime('%Y%m%d')
            
            logger.info(f"ğŸ“° è·å–{ts_code}çš„Tushareæ–°é—»...")
            
            # è°ƒç”¨Tushareæ–°é—»æ¥å£
            df = self.tushare_api.news(
                src='sina',  # æ–°æµªè´¢ç»
                start_date=start_date,
                end_date=end_date,
                limit=limit
            )
            
            if df is None or df.empty:
                logger.info("â„¹ï¸ Tushareæœªè¿”å›æ–°é—»æ•°æ®")
                return []
            
            # è¿‡æ»¤ä¸ç›®æ ‡è‚¡ç¥¨ç›¸å…³çš„æ–°é—»
            # Tushareæ–°é—»åŒ…å«contentå­—æ®µï¼Œå¯ä»¥æœç´¢è‚¡ç¥¨ä»£ç 
            stock_code = ts_code.split('.')[0]  # æå–çº¯ä»£ç 
            filtered_df = df[df['content'].str.contains(stock_code, na=False)]
            
            news_list = []
            for _, row in filtered_df.iterrows():
                news_list.append({
                    'title': row.get('title', ''),
                    'content': row.get('content', '')[:200],  # æˆªå–å‰200å­—
                    'pub_time': row.get('datetime', ''),
                    'source': 'Tushare-' + row.get('channels', 'Unknown'),
                    'url': row.get('url', '')
                })
            
            logger.info(f"âœ… Tushareè·å–æ–°é—»: {len(news_list)}æ¡")
            return news_list[:limit]
            
        except Exception as e:
            error_msg = str(e)
            if 'æƒé™' in error_msg or 'permission' in error_msg.lower():
                logger.warning("âš ï¸ Tushareæ–°é—»æ¥å£éœ€è¦5000ç§¯åˆ†")
            else:
                logger.error(f"âŒ Tushareè·å–æ–°é—»å¤±è´¥: {e}")
            return []
    
    def get_stock_news_akshare(
        self,
        symbol: str,
        stock_name: str = '',
        limit: int = 20
    ) -> List[Dict]:
        """
        ä»AKShareè·å–è‚¡ç¥¨æ–°é—»(å¤šæ¥å£é™çº§ç­–ç•¥)

        æ¥å£ä¼˜å…ˆçº§:
        1. stock_news_em - ä¸œæ–¹è´¢å¯Œä¸ªè‚¡æ–°é—»
        2. stock_info_global_em - ä¸œæ–¹è´¢å¯Œå…¨çƒèµ„è®¯(å…³é”®è¯è¿‡æ»¤)
        3. stock_info_global_cls - è´¢è”ç¤¾å…¨çƒèµ„è®¯(å…³é”®è¯è¿‡æ»¤)
        4. news_economic_baidu - ç™¾åº¦è´¢ç»æ–°é—»(å…³é”®è¯è¿‡æ»¤)

        Args:
            symbol: è‚¡ç¥¨ä»£ç (6ä½æ•°å­—)ï¼Œå¦‚603777
            stock_name: è‚¡ç¥¨åç§°ï¼Œç”¨äºå…³é”®è¯è¿‡æ»¤
            limit: è¿”å›æ•°é‡é™åˆ¶
        """
        try:
            import akshare as ak

            # è½¬æ¢è‚¡ç¥¨ä»£ç æ ¼å¼
            if '.' in symbol:
                symbol = symbol.split('.')[0]

            logger.info(f"ğŸ“° è·å–{symbol}çš„AKShareæ–°é—»...")
            news_list = []

            # æ–¹æ³•1: è°ƒç”¨stock_news_emæ¥å£ (ä¸œæ–¹è´¢å¯Œä¸ªè‚¡æ–°é—»)
            try:
                df = ak.stock_news_em(symbol=symbol)
                if df is not None and not df.empty:
                    news_list = self._parse_news_dataframe(df, limit, 'AKShare-ä¸œæ–¹è´¢å¯Œ', symbol, stock_name)
                    if news_list:
                        logger.info(f"âœ… stock_news_emè·å–æ–°é—»: {len(news_list)}æ¡")
                        return news_list
            except Exception as e:
                logger.debug(f"stock_news_emæ¥å£è°ƒç”¨å¤±è´¥: {e}")

            # æ–¹æ³•2: ä½¿ç”¨ä¸œæ–¹è´¢å¯Œå…¨çƒèµ„è®¯ (å…³é”®è¯è¿‡æ»¤)
            try:
                df = ak.stock_info_global_em()
                if df is not None and not df.empty:
                    # è¿‡æ»¤åŒ…å«è‚¡ç¥¨ä»£ç æˆ–åç§°çš„æ–°é—»
                    keywords = [symbol]
                    if stock_name:
                        keywords.append(stock_name)
                        if len(stock_name) >= 4:
                            keywords.append(stock_name[2:])

                    filtered_news = []
                    for _, row in df.iterrows():
                        title = str(row.get('æ ‡é¢˜', ''))
                        summary = str(row.get('æ‘˜è¦', ''))
                        if any(kw in title or kw in summary for kw in keywords):
                            filtered_news.append({
                                'title': title,
                                'content': extract_relevant_content(summary, symbol, stock_name),
                                'pub_time': str(row.get('å‘å¸ƒæ—¶é—´', '')),
                                'source': 'AKShare-ä¸œæ–¹è´¢å¯Œå…¨çƒ',
                                'url': str(row.get('é“¾æ¥', ''))
                            })
                        if len(filtered_news) >= limit:
                            break

                    if filtered_news:
                        logger.info(f"âœ… stock_info_global_emè¿‡æ»¤è·å–: {len(filtered_news)}æ¡")
                        return filtered_news
            except Exception as e:
                logger.debug(f"stock_info_global_emå¤±è´¥: {e}")

            # æ–¹æ³•3: ä½¿ç”¨è´¢è”ç¤¾å…¨çƒèµ„è®¯ (å…³é”®è¯è¿‡æ»¤)
            try:
                df = ak.stock_info_global_cls()
                if df is not None and not df.empty:
                    keywords = [symbol]
                    if stock_name:
                        keywords.append(stock_name)
                        if len(stock_name) >= 4:
                            keywords.append(stock_name[2:])

                    filtered_news = []
                    for _, row in df.iterrows():
                        title = str(row.get('æ ‡é¢˜', ''))
                        content = str(row.get('å†…å®¹', ''))
                        if any(kw in title or kw in content for kw in keywords):
                            filtered_news.append({
                                'title': title,
                                'content': extract_relevant_content(content, symbol, stock_name),
                                'pub_time': str(row.get('å‘å¸ƒæ—¥æœŸ', '')) + ' ' + str(row.get('å‘å¸ƒæ—¶é—´', '')),
                                'source': 'AKShare-è´¢è”ç¤¾',
                                'url': ''
                            })
                        if len(filtered_news) >= limit:
                            break

                    if filtered_news:
                        logger.info(f"âœ… stock_info_global_clsè¿‡æ»¤è·å–: {len(filtered_news)}æ¡")
                        return filtered_news
            except Exception as e:
                logger.debug(f"stock_info_global_clså¤±è´¥: {e}")

            # æ–¹æ³•4: ä½¿ç”¨ç™¾åº¦è´¢ç»æ–°é—» (å…³é”®è¯è¿‡æ»¤)
            try:
                df = ak.news_economic_baidu()
                if df is not None and not df.empty:
                    keywords = [symbol]
                    if stock_name:
                        keywords.append(stock_name)
                        if len(stock_name) >= 4:
                            keywords.append(stock_name[2:])

                    filtered_news = []
                    for _, row in df.iterrows():
                        title = str(row.get('æ ‡é¢˜', ''))
                        content = str(row.get('å†…å®¹', ''))
                        if any(kw in title or kw in content for kw in keywords):
                            filtered_news.append({
                                'title': title,
                                'content': extract_relevant_content(content, symbol, stock_name),
                                'pub_time': str(row.get('å‘å¸ƒæ—¶é—´', '')),
                                'source': 'AKShare-ç™¾åº¦è´¢ç»',
                                'url': str(row.get('é“¾æ¥', ''))
                            })
                        if len(filtered_news) >= limit:
                            break

                    if filtered_news:
                        logger.info(f"âœ… news_economic_baiduè¿‡æ»¤è·å–: {len(filtered_news)}æ¡")
                        return filtered_news
            except Exception as e:
                logger.debug(f"news_economic_baiduå¤±è´¥: {e}")

            # æ–¹æ³•5: ä½¿ç”¨å·²æœ‰çš„realtime_newsä½œä¸ºå¤‡é€‰
            logger.info("å°è¯•ä½¿ç”¨å¤‡ç”¨æ–°é—»æº...")
            return self._get_news_from_realtime(symbol, limit)

        except ImportError:
            logger.error("âŒ AKShareåº“æœªå®‰è£…")
            return []
        except Exception as e:
            logger.error(f"âŒ AKShareè·å–æ–°é—»å¤±è´¥: {e}")
            import traceback
            logger.debug(traceback.format_exc())
            return []

    def _parse_news_dataframe(self, df, limit: int, source: str, stock_code: str = '', stock_name: str = '') -> List[Dict]:
        """è§£ææ–°é—»DataFrameä¸ºåˆ—è¡¨ï¼Œæ”¯æŒæ™ºèƒ½å†…å®¹æˆªå–"""
        news_list = []
        for _, row in df.head(limit).iterrows():
            try:
                # ç¡®ä¿æ‰€æœ‰å­—æ®µéƒ½è½¬æ¢ä¸ºå­—ç¬¦ä¸²
                title = str(row.get('æ–°é—»æ ‡é¢˜', row.get('æ ‡é¢˜', '')) or '')
                content = str(row.get('æ–°é—»å†…å®¹', row.get('å†…å®¹', '')) or '')
                pub_time = str(row.get('å‘å¸ƒæ—¶é—´', row.get('æ—¶é—´', '')) or '')
                url = str(row.get('æ–°é—»é“¾æ¥', row.get('é“¾æ¥', '')) or '')

                if not title:  # è·³è¿‡ç©ºæ ‡é¢˜
                    continue

                # ä½¿ç”¨æ™ºèƒ½å†…å®¹æˆªå–
                if stock_code or stock_name:
                    processed_content = extract_relevant_content(content, stock_code, stock_name)
                else:
                    processed_content = content[:300] + '...' if len(content) > 300 else content

                news_list.append({
                    'title': title,
                    'content': processed_content,
                    'pub_time': pub_time,
                    'source': source,
                    'url': url
                })
            except Exception as e:
                logger.debug(f"è·³è¿‡ä¸€æ¡æ–°é—»: {e}")
                continue
        return news_list
    
    def _get_news_from_realtime(self, symbol: str, limit: int = 10) -> List[Dict]:
        """
        ä½¿ç”¨å·²æœ‰çš„realtime_newsä½œä¸ºå¤‡ç”¨æ–°é—»æº
        å°è¯•è§£æå‡ºç‹¬ç«‹çš„æ–°é—»æ¡ç›®
        """
        try:
            from backend.dataflows.news.realtime_news import get_realtime_stock_news
            from datetime import datetime
            import re

            logger.info("ğŸ”„ ä½¿ç”¨å¤‡ç”¨æ–°é—»æº(realtime_news)")

            # è°ƒç”¨å·²æœ‰çš„realtime_newsæ¥å£
            news_report = get_realtime_stock_news(
                ticker=symbol,
                curr_date=datetime.now().strftime('%Y-%m-%d'),
                hours_back=24
            )

            if not news_report or not isinstance(news_report, str):
                return []

            news_list = []

            # è§£ææ–°é—»æŠ¥å‘Šæ ¼å¼ï¼š
            # ### æ–°é—»æ ‡é¢˜
            # ğŸ“… å‘å¸ƒæ—¶é—´
            # ğŸ”— æ–°é—»é“¾æ¥
            # æ–°é—»å†…å®¹

            lines = news_report.split('\n')
            current_news = None

            for line in lines:
                line = line.strip()
                if not line:
                    continue

                # æ£€æµ‹æ–°é—»æ ‡é¢˜è¡Œï¼ˆä»¥ ### å¼€å¤´ï¼‰
                if line.startswith('### '):
                    # ä¿å­˜ä¹‹å‰çš„æ–°é—»
                    if current_news and current_news.get('title'):
                        news_list.append(current_news)
                        if len(news_list) >= limit:
                            break

                    # å¼€å§‹æ–°çš„æ–°é—»æ¡ç›®
                    title = line[4:].strip()
                    # è·³è¿‡æ— æ•ˆæ ‡é¢˜ï¼ˆæ—¶é—´ã€URLç­‰ï¼‰
                    if title and not title.startswith('http') and not re.match(r'^\d{4}-\d{2}-\d{2}', title):
                        current_news = {
                            'title': title,
                            'content': '',
                            'pub_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                            'source': 'RealTime-ä¸œæ–¹è´¢å¯Œ',
                            'url': ''
                        }
                    else:
                        current_news = None
                    continue

                # å¦‚æœå½“å‰æ²¡æœ‰æœ‰æ•ˆçš„æ–°é—»æ¡ç›®ï¼Œè·³è¿‡
                if not current_news:
                    continue

                # è§£æå‘å¸ƒæ—¶é—´
                if line.startswith('ğŸ“… '):
                    current_news['pub_time'] = line[2:].strip()
                    continue

                # è§£ææ–°é—»é“¾æ¥
                if line.startswith('ğŸ”— '):
                    current_news['url'] = line[2:].strip()
                    continue

                # è·³è¿‡æ ‡é¢˜è¡Œã€åˆ†éš”çº¿å’Œå…ƒæ•°æ®è¡Œ
                if line.startswith('#') or line.startswith('=') or line.startswith('-'):
                    continue
                if line.startswith('ğŸ“…') or line.startswith('ğŸ“Š') or line.startswith('ğŸ•’'):
                    continue

                # å…¶ä»–å†…å®¹ä½œä¸ºæ–°é—»å†…å®¹
                if current_news and len(line) > 5:
                    if current_news['content']:
                        current_news['content'] += ' ' + line
                    else:
                        current_news['content'] = line

            # ä¿å­˜æœ€åä¸€æ¡æ–°é—»
            if current_news and current_news.get('title'):
                news_list.append(current_news)

            # å¦‚æœæ²¡æœ‰è§£æå‡ºç‹¬ç«‹æ¡ç›®ï¼Œè¿”å›æ•´ä½“æ‘˜è¦
            if not news_list and news_report:
                # å°è¯•æå–ç¬¬ä¸€ä¸ªæœ‰æ•ˆæ ‡é¢˜
                title_match = re.search(r'### (.+?)(?:\n|$)', news_report)
                title = title_match.group(1).strip() if title_match else f'{symbol} ä»Šæ—¥æ–°é—»åŠ¨æ€'

                # è·³è¿‡æ— æ•ˆæ ‡é¢˜
                if title.startswith('http') or re.match(r'^\d{4}-\d{2}-\d{2}', title):
                    title = f'{symbol} ä»Šæ—¥æ–°é—»åŠ¨æ€'

                news_list = [{
                    'title': title,
                    'content': news_report[:500],
                    'pub_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                    'source': 'RealTime-ä¸œæ–¹è´¢å¯Œ',
                    'url': f'https://so.eastmoney.com/news/s?keyword={symbol}'
                }]

            logger.info(f"âœ… å¤‡ç”¨æºè·å–æˆåŠŸ: {len(news_list)}æ¡")
            return news_list

        except Exception as e:
            logger.debug(f"å¤‡ç”¨æºä¹Ÿå¤±è´¥: {e}")
            return []
    
    def get_market_news_akshare(self, limit: int = 20) -> List[Dict]:
        """
        ä»AKShareè·å–å¸‚åœºè¦é—»
        ä½¿ç”¨å¤šä¸ªæ¥å£ä½œä¸ºå¤‡é€‰ï¼Œç¡®ä¿è‡³å°‘æœ‰ä¸€ä¸ªå¯ç”¨

        æ¥å£ä¼˜å…ˆçº§:
        1. stock_info_global_em - ä¸œæ–¹è´¢å¯Œå…¨çƒèµ„è®¯ (æœ€ç¨³å®š)
        2. stock_info_global_cls - è´¢è”ç¤¾å…¨çƒèµ„è®¯
        3. news_cctv - å¤®è§†æ–°é—»
        4. news_economic_baidu - ç™¾åº¦è´¢ç»æ–°é—»
        """
        try:
            import akshare as ak
            from datetime import datetime

            logger.info("ğŸ“° è·å–å¸‚åœºè¦é—»...")
            news_list = []

            # æ–¹æ³•1: ä¸œæ–¹è´¢å¯Œå…¨çƒèµ„è®¯ (æœ€ç¨³å®š)
            try:
                df = ak.stock_info_global_em()
                if df is not None and not df.empty:
                    for _, row in df.head(limit).iterrows():
                        title = str(row.get('æ ‡é¢˜', ''))
                        if title:
                            news_list.append({
                                'title': title,
                                'content': str(row.get('æ‘˜è¦', ''))[:300],
                                'pub_time': str(row.get('å‘å¸ƒæ—¶é—´', '')),
                                'source': 'AKShare-ä¸œæ–¹è´¢å¯Œ',
                                'url': str(row.get('é“¾æ¥', ''))
                            })
                    if news_list:
                        logger.info(f"âœ… stock_info_global_emè·å–: {len(news_list)}æ¡")
                        return news_list
            except Exception as e:
                logger.debug(f"stock_info_global_emå¤±è´¥: {e}")

            # æ–¹æ³•2: è´¢è”ç¤¾å…¨çƒèµ„è®¯
            try:
                df = ak.stock_info_global_cls()
                if df is not None and not df.empty:
                    for _, row in df.head(limit).iterrows():
                        title = str(row.get('æ ‡é¢˜', ''))
                        if title:
                            news_list.append({
                                'title': title,
                                'content': str(row.get('å†…å®¹', ''))[:300],
                                'pub_time': str(row.get('å‘å¸ƒæ—¥æœŸ', '')) + ' ' + str(row.get('å‘å¸ƒæ—¶é—´', '')),
                                'source': 'AKShare-è´¢è”ç¤¾',
                                'url': ''
                            })
                    if news_list:
                        logger.info(f"âœ… stock_info_global_clsè·å–: {len(news_list)}æ¡")
                        return news_list
            except Exception as e:
                logger.debug(f"stock_info_global_clså¤±è´¥: {e}")

            # æ–¹æ³•3: news_cctv (å¤®è§†æ–°é—»)
            try:
                today = datetime.now().strftime('%Y%m%d')
                df = ak.news_cctv(date=today)
                if df is not None and not df.empty:
                    for _, row in df.head(limit).iterrows():
                        title = str(row.get('title', ''))
                        if title:
                            news_list.append({
                                'title': title,
                                'content': str(row.get('content', ''))[:300],
                                'pub_time': str(row.get('date', today)),
                                'source': 'AKShare-å¤®è§†æ–°é—»',
                                'url': ''
                            })
                    if news_list:
                        logger.info(f"âœ… news_cctvè·å–: {len(news_list)}æ¡")
                        return news_list
            except Exception as e:
                logger.debug(f"news_cctvå¤±è´¥: {e}")

            # æ–¹æ³•4: ç™¾åº¦è´¢ç»æ–°é—»
            try:
                df = ak.news_economic_baidu()
                if df is not None and not df.empty:
                    for _, row in df.head(limit).iterrows():
                        title = str(row.get('æ ‡é¢˜', ''))
                        if title:
                            news_list.append({
                                'title': title,
                                'content': str(row.get('å†…å®¹', ''))[:300],
                                'pub_time': str(row.get('å‘å¸ƒæ—¶é—´', '')),
                                'source': 'AKShare-ç™¾åº¦è´¢ç»',
                                'url': str(row.get('é“¾æ¥', ''))
                            })
                    if news_list:
                        logger.info(f"âœ… news_economic_baiduè·å–: {len(news_list)}æ¡")
                        return news_list
            except Exception as e:
                logger.debug(f"news_economic_baiduå¤±è´¥: {e}")

            # å¦‚æœæ‰€æœ‰æ¥å£éƒ½å¤±è´¥ï¼Œè¿”å›ç©ºåˆ—è¡¨
            if not news_list:
                logger.warning("âš ï¸ æ‰€æœ‰å¸‚åœºæ–°é—»æ¥å£æš‚ä¸å¯ç”¨")

            return news_list

        except Exception as e:
            logger.error(f"âŒ è·å–å¸‚åœºè¦é—»å¤±è´¥: {e}")
            return []
    
    def aggregate_news(
        self, 
        ts_code: str,
        include_tushare: bool = True,
        include_akshare: bool = True,
        include_market_news: bool = False,
        limit_per_source: int = 10
    ) -> Dict:
        """
        èšåˆå¤šä¸ªæ•°æ®æºçš„æ–°é—»
        
        Args:
            ts_code: è‚¡ç¥¨ä»£ç 
            include_tushare: æ˜¯å¦åŒ…å«Tushareæ–°é—»
            include_akshare: æ˜¯å¦åŒ…å«AKShareæ–°é—»
            include_market_news: æ˜¯å¦åŒ…å«å¸‚åœºè¦é—»
            limit_per_source: æ¯ä¸ªæ•°æ®æºçš„æ•°é‡é™åˆ¶
            
        Returns:
            {
                'ts_code': str,
                'total_count': int,
                'sources': {
                    'tushare': [...],
                    'akshare': [...],
                    'market': [...]
                },
                'merged_news': [...],  # åˆå¹¶åçš„æ–°é—»åˆ—è¡¨
                'timestamp': str
            }
        """
        logger.info(f"ğŸ” å¼€å§‹èšåˆ{ts_code}çš„æ–°é—»...")
        
        result = {
            'ts_code': ts_code,
            'total_count': 0,
            'sources': {},
            'merged_news': [],
            'timestamp': datetime.now().isoformat()
        }
        
        # 1. Tushareæ–°é—»
        if include_tushare:
            try:
                tushare_news = self.get_stock_news_tushare(
                    ts_code, 
                    limit=limit_per_source
                )
                result['sources']['tushare'] = tushare_news
                result['merged_news'].extend(tushare_news)
                logger.info(f"âœ… Tushareæ–°é—»: {len(tushare_news)}æ¡")
            except Exception as e:
                logger.error(f"âŒ Tushareæ–°é—»è·å–å¤±è´¥: {e}")
                result['sources']['tushare'] = []
        
        # 2. AKShareä¸ªè‚¡æ–°é—»
        if include_akshare:
            try:
                symbol = ts_code.split('.')[0]
                akshare_news = self.get_stock_news_akshare(
                    symbol, 
                    limit=limit_per_source
                )
                result['sources']['akshare'] = akshare_news
                result['merged_news'].extend(akshare_news)
                logger.info(f"âœ… AKShareæ–°é—»: {len(akshare_news)}æ¡")
            except Exception as e:
                logger.error(f"âŒ AKShareæ–°é—»è·å–å¤±è´¥: {e}")
                result['sources']['akshare'] = []
        
        # 3. å¸‚åœºè¦é—»ï¼ˆå¯é€‰ï¼‰
        if include_market_news:
            try:
                market_news = self.get_market_news_akshare(limit=limit_per_source)
                result['sources']['market'] = market_news
                result['merged_news'].extend(market_news)
                logger.info(f"âœ… å¸‚åœºè¦é—»: {len(market_news)}æ¡")
            except Exception as e:
                logger.error(f"âŒ å¸‚åœºè¦é—»è·å–å¤±è´¥: {e}")
                result['sources']['market'] = []
        
        # ç»Ÿè®¡æ€»æ•°
        result['total_count'] = len(result['merged_news'])
        
        # æŒ‰æ—¶é—´æ’åº
        result['merged_news'] = self._sort_news_by_time(result['merged_news'])
        
        logger.info(f"âœ… æ–°é—»èšåˆå®Œæˆ: å…±{result['total_count']}æ¡")
        
        return result
    
    def _sort_news_by_time(self, news_list: List[Dict]) -> List[Dict]:
        """æŒ‰å‘å¸ƒæ—¶é—´æ’åºæ–°é—»"""
        try:
            # å°è¯•è§£ææ—¶é—´å¹¶æ’åº
            def parse_time(news):
                try:
                    time_str = news.get('pub_time', '')
                    if time_str:
                        # å°è¯•å¤šç§æ—¶é—´æ ¼å¼
                        for fmt in ['%Y-%m-%d %H:%M:%S', '%Y%m%d %H:%M:%S', '%Y-%m-%d']:
                            try:
                                return datetime.strptime(str(time_str), fmt)
                            except:
                                continue
                except:
                    pass
                return datetime.min
            
            sorted_news = sorted(
                news_list, 
                key=parse_time, 
                reverse=True  # æœ€æ–°çš„åœ¨å‰
            )
            return sorted_news
        except Exception as e:
            logger.warning(f"æ’åºå¤±è´¥ï¼Œè¿”å›åŸåˆ—è¡¨: {e}")
            return news_list
    
    def format_news_summary(self, news_data: Dict) -> str:
        """
        æ ¼å¼åŒ–æ–°é—»æ‘˜è¦ä¸ºæ–‡æœ¬æŠ¥å‘Š
        
        Args:
            news_data: aggregate_newsè¿”å›çš„æ•°æ®
            
        Returns:
            æ ¼å¼åŒ–çš„æ–‡æœ¬æŠ¥å‘Š
        """
        ts_code = news_data.get('ts_code', 'Unknown')
        total = news_data.get('total_count', 0)
        
        report = f"ğŸ“° {ts_code} æ–°é—»æ±‡æ€»\n"
        report += f"=" * 60 + "\n"
        report += f"æ€»è®¡: {total}æ¡æ–°é—»\n"
        report += f"æ—¶é—´: {news_data.get('timestamp', '')}\n\n"
        
        # æŒ‰æ•°æ®æºç»Ÿè®¡
        sources = news_data.get('sources', {})
        if sources:
            report += "ğŸ“Š æ•°æ®æºç»Ÿè®¡:\n"
            for source_name, news_list in sources.items():
                report += f"  - {source_name}: {len(news_list)}æ¡\n"
            report += "\n"
        
        # æ˜¾ç¤ºæœ€æ–°çš„æ–°é—»
        merged_news = news_data.get('merged_news', [])
        if merged_news:
            report += "ğŸ“‹ æœ€æ–°æ–°é—»:\n"
            report += "-" * 60 + "\n"
            
            for i, news in enumerate(merged_news[:10], 1):
                report += f"\n[{i}] {news.get('title', 'Unknown')}\n"
                report += f"    æ¥æº: {news.get('source', 'Unknown')}\n"
                report += f"    æ—¶é—´: {news.get('pub_time', 'Unknown')}\n"
                if news.get('content'):
                    content = news['content'][:100]
                    report += f"    å†…å®¹: {content}...\n"
        
        return report


# å…¨å±€å®ä¾‹
_news_aggregator = None


def get_news_aggregator() -> MultiSourceNewsAggregator:
    """è·å–å…¨å±€æ–°é—»èšåˆå™¨å®ä¾‹"""
    global _news_aggregator
    if _news_aggregator is None:
        _news_aggregator = MultiSourceNewsAggregator()
    return _news_aggregator


# ä¾¿æ·å‡½æ•°
def get_stock_news(
    ts_code: str, 
    include_tushare: bool = True,
    include_akshare: bool = True,
    limit_per_source: int = 10
) -> Dict:
    """è·å–è‚¡ç¥¨æ–°é—»"""
    aggregator = get_news_aggregator()
    return aggregator.aggregate_news(
        ts_code, 
        include_tushare=include_tushare,
        include_akshare=include_akshare,
        limit_per_source=limit_per_source
    )


def get_news_summary(ts_code: str) -> str:
    """è·å–è‚¡ç¥¨æ–°é—»æ‘˜è¦æ–‡æœ¬"""
    aggregator = get_news_aggregator()
    news_data = aggregator.aggregate_news(ts_code)
    return aggregator.format_news_summary(news_data)
