"""
å¤šæºæ–°é—»èšåˆå™¨
æ•´åˆTushareã€AKShareã€ä¸œæ–¹è´¢å¯Œç­‰å¤šä¸ªæ•°æ®æºçš„æ–°é—»
"""

import os
from datetime import datetime, timedelta
from typing import List, Dict, Optional
import pandas as pd

from backend.utils.logging_config import get_logger

logger = get_logger("news.multi_source")


class MultiSourceNewsAggregator:
    """å¤šæºæ–°é—»èšåˆå™¨"""
    
    def __init__(self):
        self.tushare_token = os.getenv('TUSHARE_TOKEN', '')
        self.tushare_api = None
        
        # åˆå§‹åŒ–Tushare
        if self.tushare_token:
            try:
                import tushare as ts
                ts.set_token(self.tushare_token)
                self.tushare_api = ts.pro_api()
                logger.info("âœ… Tushareæ–°é—»APIåˆå§‹åŒ–æˆåŠŸ")
            except Exception as e:
                logger.error(f"âŒ Tushareåˆå§‹åŒ–å¤±è´¥: {e}")
        
        logger.info("å¤šæºæ–°é—»èšåˆå™¨åˆå§‹åŒ–å®Œæˆ")
    
    def get_stock_news_tushare(
        self, 
        ts_code: str, 
        start_date: Optional[str] = None,
        end_date: Optional[str] = None,
        limit: int = 20
    ) -> List[Dict]:
        """
        ä»Tushareè·å–è‚¡ç¥¨æ–°é—»
        
        æ³¨æ„: Tushareçš„æ–°é—»æ¥å£éœ€è¦è¾ƒé«˜ç§¯åˆ†(5000+)
        æ¥å£: news (éœ€è¦5000ç§¯åˆ†)
        
        Args:
            ts_code: è‚¡ç¥¨ä»£ç ï¼Œå¦‚600519.SH
            start_date: å¼€å§‹æ—¥æœŸ YYYYMMDD
            end_date: ç»“æŸæ—¥æœŸ YYYYMMDD
            limit: è¿”å›æ•°é‡é™åˆ¶
        """
        if not self.tushare_api:
            logger.warning("âš ï¸ Tushare APIä¸å¯ç”¨")
            return []
        
        try:
            if not end_date:
                end_date = datetime.now().strftime('%Y%m%d')
            if not start_date:
                start_date = (datetime.now() - timedelta(days=30)).strftime('%Y%m%d')
            
            logger.info(f"ğŸ“° è·å–{ts_code}çš„Tushareæ–°é—»...")
            
            # è°ƒç”¨Tushareæ–°é—»æ¥å£
            df = self.tushare_api.news(
                src='sina',  # æ–°æµªè´¢ç»
                start_date=start_date,
                end_date=end_date,
                limit=limit
            )
            
            if df is None or df.empty:
                logger.info("â„¹ï¸ Tushareæœªè¿”å›æ–°é—»æ•°æ®")
                return []
            
            # è¿‡æ»¤ä¸ç›®æ ‡è‚¡ç¥¨ç›¸å…³çš„æ–°é—»
            # Tushareæ–°é—»åŒ…å«contentå­—æ®µï¼Œå¯ä»¥æœç´¢è‚¡ç¥¨ä»£ç 
            stock_code = ts_code.split('.')[0]  # æå–çº¯ä»£ç 
            filtered_df = df[df['content'].str.contains(stock_code, na=False)]
            
            news_list = []
            for _, row in filtered_df.iterrows():
                news_list.append({
                    'title': row.get('title', ''),
                    'content': row.get('content', '')[:200],  # æˆªå–å‰200å­—
                    'pub_time': row.get('datetime', ''),
                    'source': 'Tushare-' + row.get('channels', 'Unknown'),
                    'url': row.get('url', '')
                })
            
            logger.info(f"âœ… Tushareè·å–æ–°é—»: {len(news_list)}æ¡")
            return news_list[:limit]
            
        except Exception as e:
            error_msg = str(e)
            if 'æƒé™' in error_msg or 'permission' in error_msg.lower():
                logger.warning("âš ï¸ Tushareæ–°é—»æ¥å£éœ€è¦5000ç§¯åˆ†")
            else:
                logger.error(f"âŒ Tushareè·å–æ–°é—»å¤±è´¥: {e}")
            return []
    
    def get_stock_news_akshare(
        self, 
        symbol: str, 
        limit: int = 20
    ) -> List[Dict]:
        """
        ä»AKShareè·å–è‚¡ç¥¨æ–°é—»(ä¸œæ–¹è´¢å¯Œ)
        
        æ¥å£: stock_news_em
        
        Args:
            symbol: è‚¡ç¥¨ä»£ç (6ä½æ•°å­—)ï¼Œå¦‚603777
            limit: è¿”å›æ•°é‡é™åˆ¶
        """
        try:
            import akshare as ak
            
            # è½¬æ¢è‚¡ç¥¨ä»£ç æ ¼å¼
            if '.' in symbol:
                symbol = symbol.split('.')[0]
            
            logger.info(f"ğŸ“° è·å–{symbol}çš„AKShareæ–°é—»...")
            
            # è°ƒç”¨AKShareæ¥å£ - ä½¿ç”¨æ›´ç¨³å®šçš„æ–¹å¼
            try:
                df = ak.stock_news_em(symbol=symbol)
            except Exception as e:
                logger.warning(f"âš ï¸ stock_news_emæ¥å£è°ƒç”¨å¤±è´¥: {e}")
                # å°è¯•ä½¿ç”¨å·²æœ‰çš„realtime_newsä½œä¸ºå¤‡é€‰
                logger.info("å°è¯•ä½¿ç”¨å¤‡ç”¨æ–°é—»æº...")
                return self._get_news_from_realtime(symbol, limit)
            
            if df is None or df.empty:
                logger.info("â„¹ï¸ AKShareæœªè¿”å›æ–°é—»æ•°æ®ï¼Œå°è¯•å¤‡ç”¨æº")
                return self._get_news_from_realtime(symbol, limit)
            
            news_list = []
            for _, row in df.head(limit).iterrows():
                try:
                    # ç¡®ä¿æ‰€æœ‰å­—æ®µéƒ½è½¬æ¢ä¸ºå­—ç¬¦ä¸²
                    title = str(row.get('æ–°é—»æ ‡é¢˜', '') or '')
                    content = str(row.get('æ–°é—»å†…å®¹', '') or '')
                    pub_time = str(row.get('å‘å¸ƒæ—¶é—´', '') or '')
                    url = str(row.get('æ–°é—»é“¾æ¥', '') or '')
                    
                    if not title:  # è·³è¿‡ç©ºæ ‡é¢˜
                        continue
                    
                    news_list.append({
                        'title': title,
                        'content': content[:200] if content else '',
                        'pub_time': pub_time,
                        'source': 'AKShare-ä¸œæ–¹è´¢å¯Œ',
                        'url': url
                    })
                except Exception as e:
                    logger.debug(f"è·³è¿‡ä¸€æ¡æ–°é—»: {e}")
                    continue
            
            logger.info(f"âœ… AKShareè·å–æ–°é—»: {len(news_list)}æ¡")
            return news_list
            
        except ImportError:
            logger.error("âŒ AKShareåº“æœªå®‰è£…")
            return []
        except Exception as e:
            logger.error(f"âŒ AKShareè·å–æ–°é—»å¤±è´¥: {e}")
            import traceback
            logger.debug(traceback.format_exc())
            return []
    
    def _get_news_from_realtime(self, symbol: str, limit: int = 10) -> List[Dict]:
        """
        ä½¿ç”¨å·²æœ‰çš„realtime_newsä½œä¸ºå¤‡ç”¨æ–°é—»æº
        """
        try:
            from backend.dataflows.news.realtime_news import get_realtime_stock_news
            from datetime import datetime
            
            logger.info("ğŸ”„ ä½¿ç”¨å¤‡ç”¨æ–°é—»æº(realtime_news)")
            
            # è°ƒç”¨å·²æœ‰çš„realtime_newsæ¥å£
            news_report = get_realtime_stock_news(
                ticker=symbol,
                curr_date=datetime.now().strftime('%Y-%m-%d'),
                hours_back=24
            )
            
            # è§£ææ–‡æœ¬æŠ¥å‘Šä¸ºç»“æ„åŒ–æ•°æ®(ç®€åŒ–å¤„ç†)
            if news_report and isinstance(news_report, str):
                # å¦‚æœæœ‰æ•°æ®ï¼Œè¿”å›ä¸€ä¸ªæ‘˜è¦
                news_list = [{
                    'title': f'{symbol}æ–°é—»æ±‡æ€»',
                    'content': news_report[:500],
                    'pub_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                    'source': 'RealTime-ä¸œæ–¹è´¢å¯Œ',
                    'url': ''
                }]
                logger.info(f"âœ… å¤‡ç”¨æºè·å–æˆåŠŸ: 1æ¡æ±‡æ€»")
                return news_list
            
            return []
            
        except Exception as e:
            logger.debug(f"å¤‡ç”¨æºä¹Ÿå¤±è´¥: {e}")
            return []
    
    def get_market_news_akshare(self, limit: int = 20) -> List[Dict]:
        """
        ä»AKShareè·å–å¸‚åœºè¦é—»
        
        æ¥å£: stock_news_main_cx
        """
        try:
            import akshare as ak
            
            logger.info("ğŸ“° è·å–å¸‚åœºè¦é—»...")
            
            df = ak.stock_news_main_cx()
            
            if df is None or df.empty:
                logger.info("â„¹ï¸ æœªè·å–åˆ°å¸‚åœºè¦é—»")
                return []
            
            news_list = []
            for _, row in df.head(limit).iterrows():
                news_list.append({
                    'title': row.get('æ ‡é¢˜', ''),
                    'content': '',
                    'pub_time': str(row.get('æ—¶é—´', '')),
                    'source': 'AKShare-å¸‚åœºè¦é—»',
                    'url': row.get('é“¾æ¥', '')
                })
            
            logger.info(f"âœ… è·å–å¸‚åœºè¦é—»: {len(news_list)}æ¡")
            return news_list
            
        except Exception as e:
            logger.error(f"âŒ è·å–å¸‚åœºè¦é—»å¤±è´¥: {e}")
            return []
    
    def aggregate_news(
        self, 
        ts_code: str,
        include_tushare: bool = True,
        include_akshare: bool = True,
        include_market_news: bool = False,
        limit_per_source: int = 10
    ) -> Dict:
        """
        èšåˆå¤šä¸ªæ•°æ®æºçš„æ–°é—»
        
        Args:
            ts_code: è‚¡ç¥¨ä»£ç 
            include_tushare: æ˜¯å¦åŒ…å«Tushareæ–°é—»
            include_akshare: æ˜¯å¦åŒ…å«AKShareæ–°é—»
            include_market_news: æ˜¯å¦åŒ…å«å¸‚åœºè¦é—»
            limit_per_source: æ¯ä¸ªæ•°æ®æºçš„æ•°é‡é™åˆ¶
            
        Returns:
            {
                'ts_code': str,
                'total_count': int,
                'sources': {
                    'tushare': [...],
                    'akshare': [...],
                    'market': [...]
                },
                'merged_news': [...],  # åˆå¹¶åçš„æ–°é—»åˆ—è¡¨
                'timestamp': str
            }
        """
        logger.info(f"ğŸ” å¼€å§‹èšåˆ{ts_code}çš„æ–°é—»...")
        
        result = {
            'ts_code': ts_code,
            'total_count': 0,
            'sources': {},
            'merged_news': [],
            'timestamp': datetime.now().isoformat()
        }
        
        # 1. Tushareæ–°é—»
        if include_tushare:
            try:
                tushare_news = self.get_stock_news_tushare(
                    ts_code, 
                    limit=limit_per_source
                )
                result['sources']['tushare'] = tushare_news
                result['merged_news'].extend(tushare_news)
                logger.info(f"âœ… Tushareæ–°é—»: {len(tushare_news)}æ¡")
            except Exception as e:
                logger.error(f"âŒ Tushareæ–°é—»è·å–å¤±è´¥: {e}")
                result['sources']['tushare'] = []
        
        # 2. AKShareä¸ªè‚¡æ–°é—»
        if include_akshare:
            try:
                symbol = ts_code.split('.')[0]
                akshare_news = self.get_stock_news_akshare(
                    symbol, 
                    limit=limit_per_source
                )
                result['sources']['akshare'] = akshare_news
                result['merged_news'].extend(akshare_news)
                logger.info(f"âœ… AKShareæ–°é—»: {len(akshare_news)}æ¡")
            except Exception as e:
                logger.error(f"âŒ AKShareæ–°é—»è·å–å¤±è´¥: {e}")
                result['sources']['akshare'] = []
        
        # 3. å¸‚åœºè¦é—»ï¼ˆå¯é€‰ï¼‰
        if include_market_news:
            try:
                market_news = self.get_market_news_akshare(limit=limit_per_source)
                result['sources']['market'] = market_news
                result['merged_news'].extend(market_news)
                logger.info(f"âœ… å¸‚åœºè¦é—»: {len(market_news)}æ¡")
            except Exception as e:
                logger.error(f"âŒ å¸‚åœºè¦é—»è·å–å¤±è´¥: {e}")
                result['sources']['market'] = []
        
        # ç»Ÿè®¡æ€»æ•°
        result['total_count'] = len(result['merged_news'])
        
        # æŒ‰æ—¶é—´æ’åº
        result['merged_news'] = self._sort_news_by_time(result['merged_news'])
        
        logger.info(f"âœ… æ–°é—»èšåˆå®Œæˆ: å…±{result['total_count']}æ¡")
        
        return result
    
    def _sort_news_by_time(self, news_list: List[Dict]) -> List[Dict]:
        """æŒ‰å‘å¸ƒæ—¶é—´æ’åºæ–°é—»"""
        try:
            # å°è¯•è§£ææ—¶é—´å¹¶æ’åº
            def parse_time(news):
                try:
                    time_str = news.get('pub_time', '')
                    if time_str:
                        # å°è¯•å¤šç§æ—¶é—´æ ¼å¼
                        for fmt in ['%Y-%m-%d %H:%M:%S', '%Y%m%d %H:%M:%S', '%Y-%m-%d']:
                            try:
                                return datetime.strptime(str(time_str), fmt)
                            except:
                                continue
                except:
                    pass
                return datetime.min
            
            sorted_news = sorted(
                news_list, 
                key=parse_time, 
                reverse=True  # æœ€æ–°çš„åœ¨å‰
            )
            return sorted_news
        except Exception as e:
            logger.warning(f"æ’åºå¤±è´¥ï¼Œè¿”å›åŸåˆ—è¡¨: {e}")
            return news_list
    
    def format_news_summary(self, news_data: Dict) -> str:
        """
        æ ¼å¼åŒ–æ–°é—»æ‘˜è¦ä¸ºæ–‡æœ¬æŠ¥å‘Š
        
        Args:
            news_data: aggregate_newsè¿”å›çš„æ•°æ®
            
        Returns:
            æ ¼å¼åŒ–çš„æ–‡æœ¬æŠ¥å‘Š
        """
        ts_code = news_data.get('ts_code', 'Unknown')
        total = news_data.get('total_count', 0)
        
        report = f"ğŸ“° {ts_code} æ–°é—»æ±‡æ€»\n"
        report += f"=" * 60 + "\n"
        report += f"æ€»è®¡: {total}æ¡æ–°é—»\n"
        report += f"æ—¶é—´: {news_data.get('timestamp', '')}\n\n"
        
        # æŒ‰æ•°æ®æºç»Ÿè®¡
        sources = news_data.get('sources', {})
        if sources:
            report += "ğŸ“Š æ•°æ®æºç»Ÿè®¡:\n"
            for source_name, news_list in sources.items():
                report += f"  - {source_name}: {len(news_list)}æ¡\n"
            report += "\n"
        
        # æ˜¾ç¤ºæœ€æ–°çš„æ–°é—»
        merged_news = news_data.get('merged_news', [])
        if merged_news:
            report += "ğŸ“‹ æœ€æ–°æ–°é—»:\n"
            report += "-" * 60 + "\n"
            
            for i, news in enumerate(merged_news[:10], 1):
                report += f"\n[{i}] {news.get('title', 'Unknown')}\n"
                report += f"    æ¥æº: {news.get('source', 'Unknown')}\n"
                report += f"    æ—¶é—´: {news.get('pub_time', 'Unknown')}\n"
                if news.get('content'):
                    content = news['content'][:100]
                    report += f"    å†…å®¹: {content}...\n"
        
        return report


# å…¨å±€å®ä¾‹
_news_aggregator = None


def get_news_aggregator() -> MultiSourceNewsAggregator:
    """è·å–å…¨å±€æ–°é—»èšåˆå™¨å®ä¾‹"""
    global _news_aggregator
    if _news_aggregator is None:
        _news_aggregator = MultiSourceNewsAggregator()
    return _news_aggregator


# ä¾¿æ·å‡½æ•°
def get_stock_news(
    ts_code: str, 
    include_tushare: bool = True,
    include_akshare: bool = True,
    limit_per_source: int = 10
) -> Dict:
    """è·å–è‚¡ç¥¨æ–°é—»"""
    aggregator = get_news_aggregator()
    return aggregator.aggregate_news(
        ts_code, 
        include_tushare=include_tushare,
        include_akshare=include_akshare,
        limit_per_source=limit_per_source
    )


def get_news_summary(ts_code: str) -> str:
    """è·å–è‚¡ç¥¨æ–°é—»æ‘˜è¦æ–‡æœ¬"""
    aggregator = get_news_aggregator()
    news_data = aggregator.aggregate_news(ts_code)
    return aggregator.format_news_summary(news_data)
