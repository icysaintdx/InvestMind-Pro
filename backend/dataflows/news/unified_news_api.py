#!/usr/bin/env python3
"""
ç»Ÿä¸€æ–°é—»APIæ¥å£
æ•´åˆç°æœ‰è‡ªåˆ¶æ¥å£å’ŒAKShareæ¥å£
"""

from typing import List, Dict, Any
from datetime import datetime
import time
import threading

from backend.dataflows.news.realtime_news import get_realtime_stock_news
from backend.dataflows.news.akshare_news_api import get_akshare_news_api
from backend.dataflows.news.improved_sentiment_analysis import get_sentiment_analyzer
from backend.utils.logging_config import get_logger

logger = get_logger("unified_news")


# ==================== æ–°é—»ç¼“å­˜ç³»ç»Ÿ ====================

class NewsCache:
    """
    æ–°é—»ç¼“å­˜ç±»
    ç”¨äºé¿å…çŸ­æ—¶é—´å†…é‡å¤è¯·æ±‚åŒä¸€è‚¡ç¥¨çš„æ–°é—»æ•°æ®
    """
    
    def __init__(self, ttl_seconds: int = 300):
        """
        åˆå§‹åŒ–ç¼“å­˜
        
        Args:
            ttl_seconds: ç¼“å­˜æœ‰æ•ˆæœŸï¼ˆç§’ï¼‰ï¼Œé»˜è®¤5åˆ†é’Ÿ
        """
        self._cache: Dict[str, Dict[str, Any]] = {}
        self._ttl = ttl_seconds
        self._lock = threading.Lock()
        logger.info(f"ğŸ“¦ æ–°é—»ç¼“å­˜åˆå§‹åŒ–å®Œæˆï¼ŒTTL={ttl_seconds}ç§’")
    
    def _get_cache_key(self, ticker: str) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        return f"news_{ticker}"
    
    def get(self, ticker: str) -> Dict[str, Any] | None:
        """
        è·å–ç¼“å­˜æ•°æ®
        
        Args:
            ticker: è‚¡ç¥¨ä»£ç 
            
        Returns:
            ç¼“å­˜çš„æ•°æ®ï¼Œå¦‚æœä¸å­˜åœ¨æˆ–å·²è¿‡æœŸåˆ™è¿”å›None
        """
        cache_key = self._get_cache_key(ticker)
        
        with self._lock:
            if cache_key not in self._cache:
                return None
            
            cache_entry = self._cache[cache_key]
            cached_time = cache_entry.get('timestamp', 0)
            current_time = time.time()
            
            # æ£€æŸ¥æ˜¯å¦è¿‡æœŸ
            if (current_time - cached_time) > self._ttl:
                # ç¼“å­˜å·²è¿‡æœŸï¼Œåˆ é™¤å¹¶è¿”å›None
                del self._cache[cache_key]
                logger.info(f"â° ç¼“å­˜å·²è¿‡æœŸ: {ticker}")
                return None
            
            # ç¼“å­˜æœ‰æ•ˆ
            remaining_ttl = self._ttl - (current_time - cached_time)
            logger.info(f"âœ… å‘½ä¸­ç¼“å­˜: {ticker} (å‰©ä½™{remaining_ttl:.1f}ç§’)")
            return cache_entry.get('data')
    
    def set(self, ticker: str, data: Dict[str, Any]) -> None:
        """
        è®¾ç½®ç¼“å­˜æ•°æ®
        
        Args:
            ticker: è‚¡ç¥¨ä»£ç 
            data: è¦ç¼“å­˜çš„æ•°æ®
        """
        cache_key = self._get_cache_key(ticker)
        
        with self._lock:
            self._cache[cache_key] = {
                'data': data,
                'timestamp': time.time()
            }
            logger.info(f"ğŸ’¾ å·²ç¼“å­˜: {ticker} (TTL={self._ttl}ç§’)")
    
    def clear(self, ticker: str = None) -> None:
        """
        æ¸…é™¤ç¼“å­˜
        
        Args:
            ticker: è‚¡ç¥¨ä»£ç ï¼Œå¦‚æœä¸ºNoneåˆ™æ¸…é™¤æ‰€æœ‰ç¼“å­˜
        """
        with self._lock:
            if ticker:
                cache_key = self._get_cache_key(ticker)
                if cache_key in self._cache:
                    del self._cache[cache_key]
                    logger.info(f"ğŸ—‘ï¸ å·²æ¸…é™¤ç¼“å­˜: {ticker}")
            else:
                self._cache.clear()
                logger.info(f"ğŸ—‘ï¸ å·²æ¸…é™¤æ‰€æœ‰ç¼“å­˜")
    
    def get_stats(self) -> Dict[str, Any]:
        """
        è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯
        
        Returns:
            ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯
        """
        with self._lock:
            current_time = time.time()
            valid_count = 0
            expired_count = 0
            
            for cache_key, entry in list(self._cache.items()):
                if (current_time - entry.get('timestamp', 0)) <= self._ttl:
                    valid_count += 1
                else:
                    expired_count += 1
            
            return {
                'total_entries': len(self._cache),
                'valid_entries': valid_count,
                'expired_entries': expired_count,
                'ttl_seconds': self._ttl
            }


# å…¨å±€ç¼“å­˜å®ä¾‹
_news_cache = NewsCache(ttl_seconds=300)  # 5åˆ†é’Ÿç¼“å­˜


class UnifiedNewsAPI:
    """ç»Ÿä¸€æ–°é—»API"""
    
    def __init__(self):
        """åˆå§‹åŒ–"""
        self.akshare_api = get_akshare_news_api()
        self.sentiment_analyzer = get_sentiment_analyzer()
        self.cache = _news_cache  # ä½¿ç”¨å…¨å±€ç¼“å­˜
        logger.info("ç»Ÿä¸€æ–°é—»APIåˆå§‹åŒ–å®Œæˆ")
    
    def get_stock_news_comprehensive(self, ticker: str, use_cache: bool = True) -> Dict[str, Any]:
        """
        è·å–è‚¡ç¥¨çš„ç»¼åˆæ–°é—»æ•°æ®
        æ•´åˆå¤šä¸ªæ•°æ®æº
        
        Args:
            ticker: è‚¡ç¥¨ä»£ç 
            use_cache: æ˜¯å¦ä½¿ç”¨ç¼“å­˜ï¼Œé»˜è®¤True
            
        Returns:
            ç»¼åˆæ–°é—»æ•°æ®
        """
        # ==================== ç¼“å­˜æ£€æŸ¥ ====================
        if use_cache:
            cached_data = self.cache.get(ticker)
            if cached_data:
                # æ›´æ–°æ—¶é—´æˆ³ä¸ºå½“å‰æ—¶é—´ï¼ˆè¡¨ç¤ºè¿™æ˜¯ç¼“å­˜æ•°æ®ï¼‰
                cached_data['from_cache'] = True
                cached_data['cache_timestamp'] = cached_data.get('timestamp')
                cached_data['timestamp'] = datetime.now().isoformat()
                return cached_data
        
        logger.info(f"å¼€å§‹è·å–{ticker}çš„ç»¼åˆæ–°é—»æ•°æ®...")
        
        result = {
            'ticker': ticker,
            'timestamp': datetime.now().isoformat(),
            'sources': {},
            'summary': {},
            'from_cache': False
        }
        
        # æ•°æ®æº1: å®æ—¶æ–°é—»èšåˆå™¨ï¼ˆå·²éªŒè¯å¯ç”¨ï¼‰
        try:
            logger.info(f"[æ•°æ®æº1] å®æ—¶æ–°é—»èšåˆå™¨...")
            realtime_news = get_realtime_stock_news(
                ticker=ticker,
                curr_date=datetime.now().strftime('%Y-%m-%d'),
                hours_back=6
            )
            
            if realtime_news:
                # è§£ææ–°é—»æ•°é‡ï¼ˆä»æŠ¥å‘Šä¸­æå–ï¼‰
                news_count = 0
                if isinstance(realtime_news, str):
                    # ä»æŠ¥å‘Šä¸­æå–æ–°é—»æ•°é‡
                    import re
                    match = re.search(r'(è·å–åˆ°|\d+)æ¡', realtime_news)
                    if match:
                        try:
                            news_count = int(re.search(r'\d+', match.group(0)).group())
                        except:
                            news_count = 10  # é»˜è®¤å€¼
                
                result['sources']['realtime_news'] = {
                    'status': 'success',
                    'data': realtime_news,
                    'count': news_count,
                    'source': 'å®æ—¶æ–°é—»èšåˆå™¨ï¼ˆä¸œæ–¹è´¢å¯Œï¼‰'
                }
                logger.info(f"âœ… å®æ—¶æ–°é—»èšåˆå™¨æˆåŠŸ: {news_count}æ¡")
            else:
                result['sources']['realtime_news'] = {
                    'status': 'no_data',
                    'message': 'æœªè·å–åˆ°æ•°æ®'
                }
                logger.warning(f"âš ï¸ å®æ—¶æ–°é—»èšåˆå™¨æ— æ•°æ®")
                
        except Exception as e:
            logger.error(f"âŒ å®æ—¶æ–°é—»èšåˆå™¨å¤±è´¥: {e}")
            result['sources']['realtime_news'] = {
                'status': 'error',
                'message': str(e)
            }
        
        # æ•°æ®æº2: AKShareä¸ªè‚¡æ–°é—»ï¼ˆå·²éªŒè¯å¯ç”¨ï¼‰
        try:
            logger.info(f"[æ•°æ®æº2] AKShareä¸ªè‚¡æ–°é—»...")
            akshare_news = self.akshare_api.get_stock_news(ticker, limit=20)
            
            if akshare_news:
                result['sources']['akshare_stock_news'] = {
                    'status': 'success',
                    'data': akshare_news,
                    'count': len(akshare_news),
                    'source': 'AKShareï¼ˆä¸œæ–¹è´¢å¯Œï¼‰'
                }
                logger.info(f"âœ… AKShareä¸ªè‚¡æ–°é—»æˆåŠŸ: {len(akshare_news)}æ¡")
            else:
                result['sources']['akshare_stock_news'] = {
                    'status': 'no_data',
                    'message': 'æœªè·å–åˆ°æ•°æ®'
                }
                logger.warning(f"âš ï¸ AKShareä¸ªè‚¡æ–°é—»æ— æ•°æ®")
                
        except Exception as e:
            logger.error(f"âŒ AKShareä¸ªè‚¡æ–°é—»å¤±è´¥: {e}")
            result['sources']['akshare_stock_news'] = {
                'status': 'error',
                'message': str(e)
            }
        
        # æ•°æ®æº3: è´¢è”ç¤¾å¿«è®¯ï¼ˆå·²éªŒè¯å¯ç”¨ï¼‰
        try:
            logger.info(f"[æ•°æ®æº3] è´¢è”ç¤¾å¿«è®¯...")
            cls_news = self.akshare_api.get_cls_telegraph(limit=10)
            
            if cls_news:
                result['sources']['cls_telegraph'] = {
                    'status': 'success',
                    'data': cls_news,
                    'count': len(cls_news),
                    'source': 'è´¢è”ç¤¾'
                }
                logger.info(f"âœ… è´¢è”ç¤¾å¿«è®¯æˆåŠŸ: {len(cls_news)}æ¡")
            else:
                result['sources']['cls_telegraph'] = {
                    'status': 'no_data',
                    'message': 'æœªè·å–åˆ°æ•°æ®'
                }
                logger.warning(f"âš ï¸ è´¢è”ç¤¾å¿«è®¯æ— æ•°æ®")
                
        except Exception as e:
            logger.error(f"âŒ è´¢è”ç¤¾å¿«è®¯å¤±è´¥: {e}")
            result['sources']['cls_telegraph'] = {
                'status': 'error',
                'message': str(e)
            }
        
        # æ•°æ®æº4: å¾®åšçƒ­è®®ï¼ˆå·²éªŒè¯å¯ç”¨ï¼‰
        try:
            logger.info(f"[æ•°æ®æº4] å¾®åšçƒ­è®®...")
            weibo_hot = self.akshare_api.get_weibo_stock_hot()
            
            if weibo_hot:
                result['sources']['weibo_hot'] = {
                    'status': 'success',
                    'data': weibo_hot,
                    'count': len(weibo_hot),
                    'source': 'å¾®åšçƒ­è®®'
                }
                logger.info(f"âœ… å¾®åšçƒ­è®®æˆåŠŸ: {len(weibo_hot)}æ¡")
            else:
                result['sources']['weibo_hot'] = {
                    'status': 'no_data',
                    'message': 'æœªè·å–åˆ°æ•°æ®'
                }
                logger.warning(f"âš ï¸ å¾®åšçƒ­è®®æ— æ•°æ®")
                
        except Exception as e:
            logger.error(f"âŒ å¾®åšçƒ­è®®å¤±è´¥: {e}")
            result['sources']['weibo_hot'] = {
                'status': 'error',
                'message': str(e)
            }
        
        # æ•°æ®æº5: è´¢ç»æ—©é¤ï¼ˆä¸œæ–¹è´¢å¯Œï¼‰
        try:
            logger.info(f"[æ•°æ®æº5] è´¢ç»æ—©é¤...")
            morning_news = self.akshare_api.get_morning_news()
            
            if morning_news:
                result['sources']['morning_news'] = {
                    'status': 'success',
                    'data': morning_news[:10],
                    'count': len(morning_news),
                    'source': 'ä¸œæ–¹è´¢å¯Œè´¢ç»æ—©é¤'
                }
                logger.info(f"âœ… è´¢ç»æ—©é¤æˆåŠŸ: {len(morning_news)}æ¡")
            else:
                result['sources']['morning_news'] = {
                    'status': 'no_data',
                    'message': 'æœªè·å–åˆ°æ•°æ®'
                }
                logger.warning(f"âš ï¸ è´¢ç»æ—©é¤æ— æ•°æ®")
        except Exception as e:
            logger.error(f"âŒ è´¢ç»æ—©é¤å¤±è´¥: {e}")
            result['sources']['morning_news'] = {'status': 'error', 'message': str(e)}
        
        # æ•°æ®æº6: å…¨çƒè´¢ç»æ–°é—»ï¼ˆä¸œæ–¹è´¢å¯Œï¼‰
        try:
            logger.info(f"[æ•°æ®æº6] å…¨çƒè´¢ç»æ–°é—»...")
            global_news = self.akshare_api.get_global_news_em(limit=10)
            
            if global_news:
                result['sources']['global_news_em'] = {
                    'status': 'success',
                    'data': global_news,
                    'count': len(global_news),
                    'source': 'ä¸œæ–¹è´¢å¯Œå…¨çƒæ–°é—»'
                }
                logger.info(f"âœ… å…¨çƒè´¢ç»æ–°é—»æˆåŠŸ: {len(global_news)}æ¡")
            else:
                result['sources']['global_news_em'] = {'status': 'no_data', 'message': 'æœªè·å–åˆ°æ•°æ®'}
        except Exception as e:
            logger.error(f"âŒ å…¨çƒè´¢ç»æ–°é—»å¤±è´¥: {e}")
            result['sources']['global_news_em'] = {'status': 'error', 'message': str(e)}
        
        # æ•°æ®æº7: æ–°æµªè´¢ç»å…¨çƒæ–°é—»
        try:
            logger.info(f"[æ•°æ®æº7] æ–°æµªè´¢ç»å…¨çƒæ–°é—»...")
            sina_news = self.akshare_api.get_global_news_sina(limit=10)
            
            if sina_news:
                result['sources']['global_news_sina'] = {
                    'status': 'success',
                    'data': sina_news,
                    'count': len(sina_news),
                    'source': 'æ–°æµªè´¢ç»'
                }
                logger.info(f"âœ… æ–°æµªè´¢ç»æˆåŠŸ: {len(sina_news)}æ¡")
            else:
                result['sources']['global_news_sina'] = {'status': 'no_data', 'message': 'æœªè·å–åˆ°æ•°æ®'}
        except Exception as e:
            logger.error(f"âŒ æ–°æµªè´¢ç»å¤±è´¥: {e}")
            result['sources']['global_news_sina'] = {'status': 'error', 'message': str(e)}
        
        # æ•°æ®æº8: å¯Œé€”ç‰›ç‰›å…¨çƒè´¢ç»
        try:
            logger.info(f"[æ•°æ®æº8] å¯Œé€”ç‰›ç‰›å…¨çƒè´¢ç»...")
            futu_news = self.akshare_api.get_futu_global_news(limit=10)
            
            if futu_news:
                result['sources']['futu_news'] = {
                    'status': 'success',
                    'data': futu_news,
                    'count': len(futu_news),
                    'source': 'å¯Œé€”ç‰›ç‰›'
                }
                logger.info(f"âœ… å¯Œé€”ç‰›ç‰›æˆåŠŸ: {len(futu_news)}æ¡")
            else:
                result['sources']['futu_news'] = {'status': 'no_data', 'message': 'æœªè·å–åˆ°æ•°æ®'}
        except Exception as e:
            logger.error(f"âŒ å¯Œé€”ç‰›ç‰›å¤±è´¥: {e}")
            result['sources']['futu_news'] = {'status': 'error', 'message': str(e)}
        
        # æ•°æ®æº9: åŒèŠ±é¡ºå…¨çƒè´¢ç»
        try:
            logger.info(f"[æ•°æ®æº9] åŒèŠ±é¡ºå…¨çƒè´¢ç»...")
            ths_news = self.akshare_api.get_ths_global_news(limit=10)
            
            if ths_news:
                result['sources']['ths_news'] = {
                    'status': 'success',
                    'data': ths_news,
                    'count': len(ths_news),
                    'source': 'åŒèŠ±é¡º'
                }
                logger.info(f"âœ… åŒèŠ±é¡ºæˆåŠŸ: {len(ths_news)}æ¡")
            else:
                result['sources']['ths_news'] = {'status': 'no_data', 'message': 'æœªè·å–åˆ°æ•°æ®'}
        except Exception as e:
            logger.error(f"âŒ åŒèŠ±é¡ºå¤±è´¥: {e}")
            result['sources']['ths_news'] = {'status': 'error', 'message': str(e)}
        
        # æƒ…ç»ªåˆ†æ
        try:
            logger.info(f"[æƒ…ç»ªåˆ†æ] å¼€å§‹åˆ†æ...")
            
            # æ”¶é›†æ‰€æœ‰æ–°é—»ç”¨äºæƒ…ç»ªåˆ†æ
            all_news = []
            
            # ä»AKShareä¸ªè‚¡æ–°é—»æå–
            if result['sources'].get('akshare_stock_news', {}).get('status') == 'success':
                all_news.extend(result['sources']['akshare_stock_news']['data'])
            
            # è¿›è¡Œæƒ…ç»ªåˆ†æ
            if all_news:
                sentiment = self.sentiment_analyzer.analyze_news_sentiment(all_news)
                result['summary']['sentiment'] = sentiment
                logger.info(f"âœ… æƒ…ç»ªåˆ†æå®Œæˆ: {sentiment.get('sentiment_label')}")
                
                # æ™ºèƒ½è¿‡æ»¤ï¼šä¼˜å…ˆæ˜¾ç¤ºéä¸­æ€§æ–°é—»ï¼Œä½†ä¸å°‘äº30ç¯‡
                filtered_news = self._filter_news_by_sentiment(all_news)
                result['summary']['filtered_news'] = filtered_news
                result['summary']['filter_info'] = {
                    'total_count': len(all_news),
                    'filtered_count': len(filtered_news['news']),
                    'positive_count': filtered_news['positive_count'],
                    'negative_count': filtered_news['negative_count'],
                    'neutral_count': filtered_news['neutral_count'],
                    'filter_strategy': filtered_news['strategy']
                }
                logger.info(f"âœ… æ–°é—»è¿‡æ»¤å®Œæˆ: {len(all_news)}æ¡ -> {len(filtered_news['news'])}æ¡ ({filtered_news['strategy']})")
            else:
                result['summary']['sentiment'] = {
                    'sentiment_score': 0.0,
                    'sentiment_label': 'æ— æ•°æ®',
                    'confidence': 0.0
                }
                logger.warning(f"âš ï¸ æƒ…ç»ªåˆ†ææ— æ•°æ®")
                
        except Exception as e:
            logger.error(f"âŒ æƒ…ç»ªåˆ†æå¤±è´¥: {e}")
            result['summary']['sentiment'] = {
                'error': str(e)
            }
        
        # ç»Ÿè®¡æ€»ç»“
        success_count = sum(1 for s in result['sources'].values() if s.get('status') == 'success')
        total_count = len(result['sources'])
        
        result['summary']['data_sources'] = {
            'total': total_count,
            'success': success_count,
            'success_rate': f"{success_count/total_count*100:.1f}%"
        }
        
        logger.info(f"âœ… ç»¼åˆæ–°é—»æ•°æ®è·å–å®Œæˆ: {success_count}/{total_count} ä¸ªæ•°æ®æºæˆåŠŸ")
        
        # ==================== å­˜å…¥ç¼“å­˜ ====================
        if use_cache:
            self.cache.set(ticker, result)
        
        return result
    
    def _filter_news_by_sentiment(self, news_list: List[Dict]) -> Dict:
        """
        æ™ºèƒ½è¿‡æ»¤æ–°é—»ï¼šä¼˜å…ˆæ˜¾ç¤ºéä¸­æ€§æ–°é—»ï¼Œä½†ä¸å°‘äº30ç¯‡
        
        è§„åˆ™ï¼š
        1. å¦‚æœéä¸­æ€§æ–°é—» >= 30ç¯‡ï¼Œåªæ˜¾ç¤ºéä¸­æ€§
        2. å¦‚æœéä¸­æ€§æ–°é—» < 30ç¯‡ï¼Œæ˜¾ç¤ºæ‰€æœ‰éä¸­æ€§ + éƒ¨åˆ†ä¸­æ€§ï¼Œæ€»æ•°è¾¾åˆ°30ç¯‡
        3. å¦‚æœæ€»æ•° < 30ç¯‡ï¼Œæ˜¾ç¤ºå…¨éƒ¨
        
        Args:
            news_list: æ–°é—»åˆ—è¡¨
            
        Returns:
            è¿‡æ»¤åçš„æ–°é—»å’Œç»Ÿè®¡ä¿¡æ¯
        """
        if not news_list:
            return {
                'news': [],
                'positive_count': 0,
                'negative_count': 0,
                'neutral_count': 0,
                'strategy': 'æ— æ•°æ®'
            }
        
        # åˆ†ç±»æ–°é—»
        positive_news = []
        negative_news = []
        neutral_news = []
        
        for news in news_list:
            title = news.get('title', '')
            content = news.get('content', '')
            text = f"{title} {content}"
            
            # åˆ†ææƒ…ç»ª
            score = self.sentiment_analyzer.analyze_text_sentiment(text)
            news['sentiment_score'] = score
            
            if score > 0.2:
                positive_news.append(news)
            elif score < -0.2:
                negative_news.append(news)
            else:
                neutral_news.append(news)
        
        non_neutral_count = len(positive_news) + len(negative_news)
        total_count = len(news_list)
        min_count = 30
        
        # å†³ç­–é€»è¾‘
        if non_neutral_count >= min_count:
            # æƒ…å†µ1ï¼šéä¸­æ€§æ–°é—»è¶³å¤Ÿï¼Œåªæ˜¾ç¤ºéä¸­æ€§
            filtered = positive_news + negative_news
            strategy = f'åªæ˜¾ç¤ºéä¸­æ€§æ–°é—» ({non_neutral_count}ç¯‡)'
        elif total_count <= min_count:
            # æƒ…å†µ2ï¼šæ€»æ•°ä¸è¶³ï¼Œæ˜¾ç¤ºå…¨éƒ¨
            filtered = news_list
            strategy = f'æ€»æ•°ä¸è¶³ï¼Œæ˜¾ç¤ºå…¨éƒ¨ ({total_count}ç¯‡)'
        else:
            # æƒ…å†µ3ï¼šéä¸­æ€§ä¸è¶³ï¼Œè¡¥å……ä¸­æ€§æ–°é—»
            need_neutral = min_count - non_neutral_count
            filtered = positive_news + negative_news + neutral_news[:need_neutral]
            strategy = f'éä¸­æ€§{non_neutral_count}ç¯‡ + ä¸­æ€§{need_neutral}ç¯‡ = {len(filtered)}ç¯‡'
        
        # æŒ‰æ—¶é—´æ’åºï¼ˆæœ€æ–°çš„åœ¨å‰ï¼‰
        filtered.sort(key=lambda x: x.get('publish_time', ''), reverse=True)
        
        return {
            'news': filtered,
            'positive_count': len(positive_news),
            'negative_count': len(negative_news),
            'neutral_count': len(neutral_news),
            'strategy': strategy
        }
    
    def get_market_news(self) -> Dict[str, Any]:
        """
        è·å–å¸‚åœºæ–°é—»
        
        Returns:
            å¸‚åœºæ–°é—»æ•°æ®
        """
        logger.info("å¼€å§‹è·å–å¸‚åœºæ–°é—»...")
        
        result = {
            'timestamp': datetime.now().isoformat(),
            'sources': {}
        }
        
        # è´¢ç»æ—©é¤
        try:
            morning_news = self.akshare_api.get_morning_news()
            if morning_news:
                result['sources']['morning_news'] = {
                    'status': 'success',
                    'data': morning_news[:10],  # åªå–å‰10æ¡
                    'count': len(morning_news),
                    'source': 'ä¸œæ–¹è´¢å¯Œè´¢ç»æ—©é¤'
                }
                logger.info(f"âœ… è´¢ç»æ—©é¤æˆåŠŸ: {len(morning_news)}æ¡")
        except Exception as e:
            logger.error(f"âŒ è´¢ç»æ—©é¤å¤±è´¥: {e}")
            result['sources']['morning_news'] = {'status': 'error', 'message': str(e)}
        
        # å…¨çƒè´¢ç»æ–°é—»
        try:
            global_news = self.akshare_api.get_global_news_em(limit=10)
            if global_news:
                result['sources']['global_news'] = {
                    'status': 'success',
                    'data': global_news,
                    'count': len(global_news),
                    'source': 'ä¸œæ–¹è´¢å¯Œå…¨çƒæ–°é—»'
                }
                logger.info(f"âœ… å…¨çƒæ–°é—»æˆåŠŸ: {len(global_news)}æ¡")
        except Exception as e:
            logger.error(f"âŒ å…¨çƒæ–°é—»å¤±è´¥: {e}")
            result['sources']['global_news'] = {'status': 'error', 'message': str(e)}
        
        return result
    
    def clear_cache(self, ticker: str = None) -> Dict[str, Any]:
        """
        æ¸…é™¤ç¼“å­˜
        
        Args:
            ticker: è‚¡ç¥¨ä»£ç ï¼Œå¦‚æœä¸ºNoneåˆ™æ¸…é™¤æ‰€æœ‰ç¼“å­˜
            
        Returns:
            æ“ä½œç»“æœ
        """
        self.cache.clear(ticker)
        return {
            'success': True,
            'message': f'å·²æ¸…é™¤ç¼“å­˜: {ticker if ticker else "å…¨éƒ¨"}'
        }
    
    def get_cache_stats(self) -> Dict[str, Any]:
        """
        è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯
        
        Returns:
            ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯
        """
        return self.cache.get_stats()


# å…¨å±€å®ä¾‹
_unified_news_api = None

def get_unified_news_api():
    """è·å–ç»Ÿä¸€æ–°é—»APIå®ä¾‹ï¼ˆå•ä¾‹ï¼‰"""
    global _unified_news_api
    if _unified_news_api is None:
        _unified_news_api = UnifiedNewsAPI()
    return _unified_news_api
