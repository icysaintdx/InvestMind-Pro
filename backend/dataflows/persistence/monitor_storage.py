"""
ç›‘æ§æ•°æ®æŒä¹…åŒ–å­˜å‚¨
ä½¿ç”¨SQLiteæ•°æ®åº“å­˜å‚¨ç›‘æ§é…ç½®å’Œå†å²æ•°æ®
"""

import json
import os
import sqlite3
from datetime import datetime, timedelta
from typing import Dict, List, Optional
from pathlib import Path

from backend.utils.logging_config import get_logger

logger = get_logger("persistence.monitor")


class MonitorStorage:
    """ç›‘æ§æ•°æ®å­˜å‚¨"""

    def __init__(self, storage_dir: str = None):
        """
        åˆå§‹åŒ–å­˜å‚¨

        Args:
            storage_dir: å­˜å‚¨ç›®å½•ï¼ˆé»˜è®¤ä¸ºé¡¹ç›®æ ¹ç›®å½•ä¸‹çš„ data/monitorï¼‰
        """
        if storage_dir is None:
            # Docker ç¯å¢ƒä½¿ç”¨ /app/data ç›®å½•
            if os.path.exists('/app/data'):
                storage_dir = Path('/app/data/monitor')
            else:
                # æœ¬åœ°å¼€å‘ä½¿ç”¨é¡¹ç›®æ ¹ç›®å½•çš„ç»å¯¹è·¯å¾„
                project_root = Path(__file__).parent.parent.parent.parent  # backend/dataflows/persistence -> é¡¹ç›®æ ¹ç›®å½•
                storage_dir = project_root / "data" / "monitor"

        self.storage_dir = Path(storage_dir)
        self.storage_dir.mkdir(parents=True, exist_ok=True)

        self.config_file = self.storage_dir / "monitor_config.json"
        self.history_dir = self.storage_dir / "history"
        self.history_dir.mkdir(exist_ok=True)

        logger.info(f"âœ… ç›‘æ§å­˜å‚¨åˆå§‹åŒ–å®Œæˆ: {self.storage_dir.absolute()}")
    
    def save_monitor_config(self, config: Dict):
        """
        ä¿å­˜ç›‘æ§é…ç½®
        
        Args:
            config: ç›‘æ§é…ç½®å­—å…¸
                {
                    'stocks': {
                        '600519.SH': {
                            'name': 'è´µå·èŒ…å°',
                            'frequency': '1h',
                            'items': {...},
                            'created_at': '2024-12-17T...',
                            'updated_at': '2024-12-17T...'
                        }
                    }
                }
        """
        try:
            with open(self.config_file, 'w', encoding='utf-8') as f:
                json.dump(config, f, ensure_ascii=False, indent=2)
            
            logger.info(f"âœ… ä¿å­˜ç›‘æ§é…ç½®æˆåŠŸ: {len(config.get('stocks', {}))}åªè‚¡ç¥¨")
            
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜ç›‘æ§é…ç½®å¤±è´¥: {e}")
            raise
    
    def load_monitor_config(self) -> Dict:
        """
        åŠ è½½ç›‘æ§é…ç½®
        
        Returns:
            ç›‘æ§é…ç½®å­—å…¸
        """
        try:
            if not self.config_file.exists():
                logger.info("â„¹ï¸ ç›‘æ§é…ç½®æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè¿”å›ç©ºé…ç½®")
                return {'stocks': {}}
            
            with open(self.config_file, 'r', encoding='utf-8') as f:
                config = json.load(f)
            
            logger.info(f"âœ… åŠ è½½ç›‘æ§é…ç½®æˆåŠŸ: {len(config.get('stocks', {}))}åªè‚¡ç¥¨")
            return config
            
        except Exception as e:
            logger.error(f"âŒ åŠ è½½ç›‘æ§é…ç½®å¤±è´¥: {e}")
            return {'stocks': {}}
    
    def add_monitored_stock(
        self, 
        ts_code: str, 
        name: str,
        frequency: str = '1h',
        items: Dict = None
    ):
        """
        æ·»åŠ ç›‘æ§è‚¡ç¥¨
        
        Args:
            ts_code: è‚¡ç¥¨ä»£ç 
            name: è‚¡ç¥¨åç§°
            frequency: æ›´æ–°é¢‘ç‡
            items: ç›‘æ§é¡¹ç›®
        """
        if items is None:
            items = {
                'news': True,
                'risk': True,
                'sentiment': True,
                'suspend': False
            }
        
        config = self.load_monitor_config()
        
        if 'stocks' not in config:
            config['stocks'] = {}
        
        config['stocks'][ts_code] = {
            'name': name,
            'frequency': frequency,
            'items': items,
            'created_at': datetime.now().isoformat(),
            'updated_at': datetime.now().isoformat()
        }
        
        self.save_monitor_config(config)
        logger.info(f"â• æ·»åŠ ç›‘æ§è‚¡ç¥¨: {name}({ts_code})")
    
    def remove_monitored_stock(self, ts_code: str):
        """ç§»é™¤ç›‘æ§è‚¡ç¥¨"""
        config = self.load_monitor_config()
        
        if ts_code in config.get('stocks', {}):
            stock_name = config['stocks'][ts_code].get('name', ts_code)
            del config['stocks'][ts_code]
            self.save_monitor_config(config)
            logger.info(f"â– ç§»é™¤ç›‘æ§è‚¡ç¥¨: {stock_name}({ts_code})")
        else:
            logger.warning(f"âš ï¸ è‚¡ç¥¨ä¸åœ¨ç›‘æ§åˆ—è¡¨: {ts_code}")
    
    def get_monitored_stocks(self) -> Dict:
        """è·å–æ‰€æœ‰ç›‘æ§è‚¡ç¥¨"""
        config = self.load_monitor_config()
        return config.get('stocks', {})
    
    def save_stock_history(
        self, 
        ts_code: str, 
        data: Dict
    ):
        """
        ä¿å­˜è‚¡ç¥¨å†å²æ•°æ®
        
        Args:
            ts_code: è‚¡ç¥¨ä»£ç 
            data: è‚¡ç¥¨æ•°æ®ï¼ˆåŒ…æ‹¬é£é™©åˆ†æã€æ–°é—»ã€æƒ…ç»ªç­‰ï¼‰
        """
        try:
            # æŒ‰æ—¥æœŸç»„ç»‡å†å²æ•°æ®
            today = datetime.now().strftime('%Y-%m-%d')
            history_file = self.history_dir / f"{ts_code}_{today}.json"
            
            # åŠ è½½å½“å¤©å·²æœ‰æ•°æ®
            if history_file.exists():
                with open(history_file, 'r', encoding='utf-8') as f:
                    history = json.load(f)
            else:
                history = {'ts_code': ts_code, 'date': today, 'records': []}
            
            # æ·»åŠ æ–°è®°å½•
            record = {
                'timestamp': datetime.now().isoformat(),
                'data': data
            }
            history['records'].append(record)
            
            # ä¿å­˜
            with open(history_file, 'w', encoding='utf-8') as f:
                json.dump(history, f, ensure_ascii=False, indent=2)
            
            logger.debug(f"âœ… ä¿å­˜{ts_code}å†å²æ•°æ®æˆåŠŸ")
            
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜å†å²æ•°æ®å¤±è´¥ {ts_code}: {e}")
    
    def load_stock_history(
        self, 
        ts_code: str, 
        date: Optional[str] = None
    ) -> List[Dict]:
        """
        åŠ è½½è‚¡ç¥¨å†å²æ•°æ®
        
        Args:
            ts_code: è‚¡ç¥¨ä»£ç 
            date: æ—¥æœŸï¼ˆYYYY-MM-DDï¼‰ï¼Œé»˜è®¤ä»Šå¤©
            
        Returns:
            å†å²è®°å½•åˆ—è¡¨
        """
        try:
            if date is None:
                date = datetime.now().strftime('%Y-%m-%d')
            
            history_file = self.history_dir / f"{ts_code}_{date}.json"
            
            if not history_file.exists():
                logger.debug(f"â„¹ï¸ å†å²æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {history_file}")
                return []
            
            with open(history_file, 'r', encoding='utf-8') as f:
                history = json.load(f)
            
            return history.get('records', [])
            
        except Exception as e:
            logger.error(f"âŒ åŠ è½½å†å²æ•°æ®å¤±è´¥ {ts_code}: {e}")
            return []
    
    def cleanup_old_history(self, days: int = 30):
        """
        æ¸…ç†æ—§çš„å†å²æ•°æ®

        Args:
            days: ä¿ç•™å¤©æ•°
        """
        try:
            from datetime import timedelta
            cutoff_date = datetime.now() - timedelta(days=days)
            deleted_count = 0

            for history_file in self.history_dir.glob("*.json"):
                # ä»æ–‡ä»¶åæå–æ—¥æœŸ
                try:
                    date_str = history_file.stem.split('_')[-1]
                    file_date = datetime.strptime(date_str, '%Y-%m-%d')

                    if file_date < cutoff_date:
                        history_file.unlink()
                        deleted_count += 1

                except Exception:
                    continue

            logger.info(f"ğŸ—‘ï¸ æ¸…ç†å†å²æ•°æ®: åˆ é™¤{deleted_count}ä¸ªæ–‡ä»¶")

        except Exception as e:
            logger.error(f"âŒ æ¸…ç†å†å²æ•°æ®å¤±è´¥: {e}")

    # ==================== æ¯æ—¥ç»Ÿè®¡æ•°æ® ====================

    def get_daily_stats_file(self, date: str = None) -> Path:
        """è·å–æ¯æ—¥ç»Ÿè®¡æ–‡ä»¶è·¯å¾„"""
        if date is None:
            date = datetime.now().strftime('%Y-%m-%d')
        return self.storage_dir / f"daily_stats_{date}.json"

    def load_daily_stats(self, date: str = None) -> Dict:
        """
        åŠ è½½æ¯æ—¥ç»Ÿè®¡æ•°æ®

        Returns:
            {
                'date': '2024-12-19',
                'news_count': 0,
                'risk_alerts': 0,
                'analysis_tasks': 0,
                'api_calls': {
                    'tushare': 0,
                    'akshare': 0,
                    'eastmoney': 0,
                    'juhe': 0
                },
                'last_updated': '...'
            }
        """
        try:
            stats_file = self.get_daily_stats_file(date)

            if not stats_file.exists():
                # è¿”å›é»˜è®¤ç»Ÿè®¡
                today = date or datetime.now().strftime('%Y-%m-%d')
                return {
                    'date': today,
                    'news_count': 0,
                    'risk_alerts': 0,
                    'analysis_tasks': 0,
                    'api_calls': {
                        'tushare': 0,
                        'akshare': 0,
                        'eastmoney': 0,
                        'juhe': 0
                    },
                    'last_updated': datetime.now().isoformat()
                }

            with open(stats_file, 'r', encoding='utf-8') as f:
                return json.load(f)

        except Exception as e:
            logger.error(f"âŒ åŠ è½½æ¯æ—¥ç»Ÿè®¡å¤±è´¥: {e}")
            return {
                'date': date or datetime.now().strftime('%Y-%m-%d'),
                'news_count': 0,
                'risk_alerts': 0,
                'analysis_tasks': 0,
                'api_calls': {'tushare': 0, 'akshare': 0, 'eastmoney': 0, 'juhe': 0},
                'last_updated': datetime.now().isoformat()
            }

    def save_daily_stats(self, stats: Dict):
        """ä¿å­˜æ¯æ—¥ç»Ÿè®¡æ•°æ®"""
        try:
            date = stats.get('date', datetime.now().strftime('%Y-%m-%d'))
            stats_file = self.get_daily_stats_file(date)
            stats['last_updated'] = datetime.now().isoformat()

            with open(stats_file, 'w', encoding='utf-8') as f:
                json.dump(stats, f, ensure_ascii=False, indent=2)

            logger.debug(f"âœ… ä¿å­˜æ¯æ—¥ç»Ÿè®¡æˆåŠŸ")

        except Exception as e:
            logger.error(f"âŒ ä¿å­˜æ¯æ—¥ç»Ÿè®¡å¤±è´¥: {e}")

    def increment_stat(self, stat_name: str, increment: int = 1):
        """å¢åŠ ç»Ÿè®¡è®¡æ•°"""
        stats = self.load_daily_stats()
        if stat_name in stats:
            stats[stat_name] = stats.get(stat_name, 0) + increment
        self.save_daily_stats(stats)

    def increment_api_call(self, source: str, increment: int = 1):
        """å¢åŠ APIè°ƒç”¨è®¡æ•°"""
        stats = self.load_daily_stats()
        if 'api_calls' not in stats:
            stats['api_calls'] = {'tushare': 0, 'akshare': 0, 'eastmoney': 0, 'juhe': 0}
        stats['api_calls'][source] = stats['api_calls'].get(source, 0) + increment
        self.save_daily_stats(stats)

    # ==================== æ–°é—»åˆ—è¡¨æŒä¹…åŒ– ====================

    def save_news_list(self, news_list: List[Dict]):
        """ä¿å­˜æ–°é—»åˆ—è¡¨"""
        try:
            news_file = self.storage_dir / "news_cache.json"
            data = {
                'news': news_list[-100:],  # åªä¿ç•™æœ€è¿‘100æ¡
                'last_updated': datetime.now().isoformat()
            }
            with open(news_file, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
            logger.debug(f"âœ… ä¿å­˜æ–°é—»åˆ—è¡¨: {len(news_list)}æ¡")
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜æ–°é—»åˆ—è¡¨å¤±è´¥: {e}")

    def load_news_list(self) -> List[Dict]:
        """åŠ è½½æ–°é—»åˆ—è¡¨"""
        try:
            news_file = self.storage_dir / "news_cache.json"
            if not news_file.exists():
                return []
            with open(news_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            return data.get('news', [])
        except Exception as e:
            logger.error(f"âŒ åŠ è½½æ–°é—»åˆ—è¡¨å¤±è´¥: {e}")
            return []


# å…¨å±€å­˜å‚¨å®ä¾‹
_monitor_storage: Optional[MonitorStorage] = None


def get_monitor_storage() -> MonitorStorage:
    """è·å–å…¨å±€ç›‘æ§å­˜å‚¨å®ä¾‹"""
    global _monitor_storage
    if _monitor_storage is None:
        _monitor_storage = MonitorStorage()
    return _monitor_storage


# ä¾¿æ·å‡½æ•°
def save_config(config: Dict):
    """ä¿å­˜ç›‘æ§é…ç½®"""
    storage = get_monitor_storage()
    storage.save_monitor_config(config)


def load_config() -> Dict:
    """åŠ è½½ç›‘æ§é…ç½®"""
    storage = get_monitor_storage()
    return storage.load_monitor_config()


def add_stock(ts_code: str, name: str, frequency: str = '1h', items: Dict = None):
    """æ·»åŠ ç›‘æ§è‚¡ç¥¨"""
    storage = get_monitor_storage()
    storage.add_monitored_stock(ts_code, name, frequency, items)


def remove_stock(ts_code: str):
    """ç§»é™¤ç›‘æ§è‚¡ç¥¨"""
    storage = get_monitor_storage()
    storage.remove_monitored_stock(ts_code)


# ==================== ç»¼åˆæ•°æ®ç¼“å­˜æŒä¹…åŒ– ====================

def save_comprehensive_cache(ts_code: str, data: Dict):
    """
    ä¿å­˜è‚¡ç¥¨ç»¼åˆæ•°æ®ç¼“å­˜åˆ°æ–‡ä»¶

    Args:
        ts_code: è‚¡ç¥¨ä»£ç 
        data: ç»¼åˆæ•°æ®
    """
    storage = get_monitor_storage()
    try:
        cache_dir = storage.storage_dir / "comprehensive_cache"
        cache_dir.mkdir(exist_ok=True)

        cache_file = cache_dir / f"{ts_code.replace('.', '_')}.json"
        cache_data = {
            'ts_code': ts_code,
            'data': data,
            'cached_at': datetime.now().isoformat()
        }

        with open(cache_file, 'w', encoding='utf-8') as f:
            json.dump(cache_data, f, ensure_ascii=False, indent=2, default=str)

        logger.debug(f"âœ… ä¿å­˜ç»¼åˆæ•°æ®ç¼“å­˜: {ts_code}")

    except Exception as e:
        logger.error(f"âŒ ä¿å­˜ç»¼åˆæ•°æ®ç¼“å­˜å¤±è´¥ {ts_code}: {e}")


def load_comprehensive_cache(ts_code: str) -> Optional[Dict]:
    """
    ä»æ–‡ä»¶åŠ è½½è‚¡ç¥¨ç»¼åˆæ•°æ®ç¼“å­˜

    Args:
        ts_code: è‚¡ç¥¨ä»£ç 

    Returns:
        ç¼“å­˜æ•°æ®å­—å…¸ï¼ŒåŒ…å« 'data' å’Œ 'cached_at'ï¼Œå¦‚æœä¸å­˜åœ¨è¿”å› None
    """
    storage = get_monitor_storage()
    try:
        cache_dir = storage.storage_dir / "comprehensive_cache"
        cache_file = cache_dir / f"{ts_code.replace('.', '_')}.json"

        if not cache_file.exists():
            return None

        with open(cache_file, 'r', encoding='utf-8') as f:
            cache_data = json.load(f)

        logger.debug(f"âœ… åŠ è½½ç»¼åˆæ•°æ®ç¼“å­˜: {ts_code}")
        return cache_data

    except Exception as e:
        logger.error(f"âŒ åŠ è½½ç»¼åˆæ•°æ®ç¼“å­˜å¤±è´¥ {ts_code}: {e}")
        return None


def load_all_comprehensive_cache() -> Dict[str, Dict]:
    """
    åŠ è½½æ‰€æœ‰è‚¡ç¥¨çš„ç»¼åˆæ•°æ®ç¼“å­˜

    Returns:
        {ts_code: {'data': ..., 'cached_at': ...}, ...}
    """
    storage = get_monitor_storage()
    result = {}
    try:
        cache_dir = storage.storage_dir / "comprehensive_cache"
        if not cache_dir.exists():
            return result

        for cache_file in cache_dir.glob("*.json"):
            try:
                with open(cache_file, 'r', encoding='utf-8') as f:
                    cache_data = json.load(f)
                ts_code = cache_data.get('ts_code')
                if ts_code:
                    result[f"comprehensive_{ts_code}"] = cache_data
            except Exception as e:
                logger.warning(f"âš ï¸ åŠ è½½ç¼“å­˜æ–‡ä»¶å¤±è´¥ {cache_file}: {e}")

        logger.info(f"âœ… åŠ è½½æ‰€æœ‰ç»¼åˆæ•°æ®ç¼“å­˜: {len(result)}ä¸ª")
        return result

    except Exception as e:
        logger.error(f"âŒ åŠ è½½æ‰€æœ‰ç»¼åˆæ•°æ®ç¼“å­˜å¤±è´¥: {e}")
        return result
