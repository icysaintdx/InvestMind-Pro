#!/usr/bin/env python3
"""
æµ‹è¯•å·¨æ½®èµ„è®¯ç½‘çˆ¬è™«
"""

import sys
import os
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from backend.dataflows.announcement.cninfo_crawler import get_cninfo_crawler
from backend.utils.logging_config import get_logger

logger = get_logger("test_cninfo")


def test_get_announcements():
    """æµ‹è¯•è·å–å…¬å‘Š"""
    logger.info("=" * 60)
    logger.info("æµ‹è¯•1: è·å–å…¬å¸å…¬å‘Š")
    logger.info("=" * 60)
    
    crawler = get_cninfo_crawler()
    
    # æµ‹è¯•å¤šä¸ªè‚¡ç¥¨
    test_stocks = [
        ("600519.SH", "è´µå·èŒ…å°"),
        ("000001.SZ", "å¹³å®‰é“¶è¡Œ"),
        ("300750.SZ", "å®å¾·æ—¶ä»£"),
    ]
    
    for stock_code, company_name in test_stocks:
        logger.info(f"\n{'='*50}")
        logger.info(f"æµ‹è¯•è‚¡ç¥¨: {company_name} ({stock_code})")
        logger.info(f"{'='*50}")
        
        try:
            # è·å–æœ€è¿‘7å¤©çš„å…¬å‘Š
            announcements = crawler.get_company_announcements(
                stock_code=stock_code,
                days=7
            )
            
            if announcements:
                logger.info(f"âœ… æˆåŠŸè·å– {len(announcements)} æ¡å…¬å‘Š")
                
                # æ˜¾ç¤ºå‰3æ¡
                for i, ann in enumerate(announcements[:3], 1):
                    logger.info(f"\nå…¬å‘Š {i}:")
                    logger.info(f"  æ ‡é¢˜: {ann.get('title', 'N/A')}")
                    logger.info(f"  ç±»å‹: {ann.get('type', 'N/A')}")
                    logger.info(f"  æ—¥æœŸ: {ann.get('publish_date', 'N/A')}")
                    logger.info(f"  é‡è¦æ€§: {ann.get('importance', 'N/A')}")
                    logger.info(f"  URL: {ann.get('url', 'N/A')[:80]}...")
            else:
                logger.warning(f"âš ï¸ æœªè·å–åˆ°å…¬å‘Šæ•°æ®")
                
        except Exception as e:
            logger.error(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()


def test_filter_important():
    """æµ‹è¯•è¿‡æ»¤é‡è¦å…¬å‘Š"""
    logger.info("\n" + "=" * 60)
    logger.info("æµ‹è¯•2: è¿‡æ»¤é‡è¦å…¬å‘Š")
    logger.info("=" * 60)
    
    crawler = get_cninfo_crawler()
    
    # è·å–å…¬å‘Š
    announcements = crawler.get_company_announcements(
        stock_code="600519.SH",
        days=30
    )
    
    if announcements:
        # è¿‡æ»¤é‡è¦å…¬å‘Š
        important = crawler.filter_important_announcements(announcements)
        
        logger.info(f"\næ€»å…¬å‘Šæ•°: {len(announcements)}")
        logger.info(f"é‡è¦å…¬å‘Šæ•°: {len(important)}")
        
        if important:
            logger.info("\né‡è¦å…¬å‘Šåˆ—è¡¨:")
            for i, ann in enumerate(important[:5], 1):
                logger.info(f"\n{i}. {ann.get('title', 'N/A')}")
                logger.info(f"   ç±»å‹: {ann.get('type', 'N/A')}")
                logger.info(f"   æ—¥æœŸ: {ann.get('publish_date', 'N/A')}")
    else:
        logger.warning("âš ï¸ æ— å…¬å‘Šæ•°æ®å¯ä¾›è¿‡æ»¤")


def test_analyze_announcements():
    """æµ‹è¯•å…¬å‘Šåˆ†æ"""
    logger.info("\n" + "=" * 60)
    logger.info("æµ‹è¯•3: å…¬å‘Šåˆ†æ")
    logger.info("=" * 60)
    
    crawler = get_cninfo_crawler()
    
    # è·å–å…¬å‘Š
    announcements = crawler.get_company_announcements(
        stock_code="600519.SH",
        days=30
    )
    
    if announcements:
        # åˆ†æå…¬å‘Š
        analysis = crawler.analyze_announcements(announcements)
        
        logger.info(f"\nåˆ†æç»“æœ:")
        logger.info(f"  æ€»å…¬å‘Šæ•°: {analysis.get('total', 0)}")
        logger.info(f"  é‡è¦å…¬å‘Šæ•°: {analysis.get('important_count', 0)}")
        logger.info(f"  æ‘˜è¦: {analysis.get('summary', 'N/A')}")
        
        logger.info(f"\nå…¬å‘Šç±»å‹åˆ†å¸ƒ:")
        for ann_type, count in analysis.get('types', {}).items():
            logger.info(f"  {ann_type}: {count}æ¡")
    else:
        logger.warning("âš ï¸ æ— å…¬å‘Šæ•°æ®å¯ä¾›åˆ†æ")


def test_announcement_types():
    """æµ‹è¯•ä¸åŒå…¬å‘Šç±»å‹"""
    logger.info("\n" + "=" * 60)
    logger.info("æµ‹è¯•4: ä¸åŒå…¬å‘Šç±»å‹")
    logger.info("=" * 60)
    
    crawler = get_cninfo_crawler()
    
    # æµ‹è¯•ä¸åŒç±»å‹
    types_to_test = [
        None,  # å…¨éƒ¨
        # å¯ä»¥æ·»åŠ å…·ä½“ç±»å‹ä»£ç 
    ]
    
    for ann_type in types_to_test:
        type_name = ann_type if ann_type else "å…¨éƒ¨ç±»å‹"
        logger.info(f"\næµ‹è¯•ç±»å‹: {type_name}")
        
        announcements = crawler.get_company_announcements(
            stock_code="600519.SH",
            days=30,
            announcement_type=ann_type
        )
        
        logger.info(f"è·å–åˆ° {len(announcements)} æ¡å…¬å‘Š")


def main():
    """ä¸»å‡½æ•°"""
    logger.info("ğŸš€ å¼€å§‹æµ‹è¯•å·¨æ½®èµ„è®¯ç½‘çˆ¬è™«")
    logger.info("=" * 60)
    
    try:
        # è¿è¡Œæ‰€æœ‰æµ‹è¯•
        test_get_announcements()
        test_filter_important()
        test_analyze_announcements()
        test_announcement_types()
        
        logger.info("\n" + "=" * 60)
        logger.info("âœ… æ‰€æœ‰æµ‹è¯•å®Œæˆ")
        logger.info("=" * 60)
        
    except Exception as e:
        logger.error(f"âŒ æµ‹è¯•è¿‡ç¨‹å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
        return 1
    
    return 0


if __name__ == "__main__":
    exit(main())
